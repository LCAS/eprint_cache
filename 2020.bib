@inproceedings{lincoln43364,
       booktitle = {Brein Informatics},
           month = {December},
           title = {Recall Performance Improvement in a Bio-Inspired Model of the Mammalian Hippocampus},
          author = {Nikolas Andreakos and Shigang Yue and Vassilis Cutsuridis},
            year = {2020},
           pages = {319--328},
             doi = {10.1007/978-3-030-59277-6\_29},
        keywords = {ARRAY(0x55988cf180b0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/43364/},
        abstract = {Mammalian hippocampus is involved in short-term formation of declarative memories. We employed a
bio-inspired neural model of hippocampal CA1 region consisting of a zoo of excitatory and inhibitory
cells. Cells? firing was timed to a theta oscillation paced by two distinct neuronal populations exhibiting
highly regular bursting activity, one tightly coupled to the trough and the other to the peak of theta. To
systematically evaluate the model?s recall performance against number of stored patterns, overlaps and
?active cells per pattern?, its cells were driven by a non-specific excitatory input to their dendrites. This
excitatory input to model excitatory cells provided context and timing information for retrieval of
previously stored memory patterns. Inhibition to excitatory cells? dendrites acted as a non-specific global
threshold machine that removed spurious activity during recall. Out of the three models tested, ?model 1?
recall quality was excellent across all conditions. ?Model 2? recall was the worst. The number of ?active
cells per pattern? had a massive effect on network recall quality regardless of how many patterns were
stored in it. As ?active cells per pattern? decreased, network?s memory capacity increased, interference
effects between stored patterns decreased, and recall quality improved. Key finding was that increased
firing rate of an inhibitory cell inhibiting a network of excitatory cells has a better success at removing
spurious activity at the network level and improving recall quality than increasing the synaptic strength of
the same inhibitory cell inhibiting the same network of excitatory cells, while keeping its firing rate fixed.}
}

@article{lincoln46137,
          volume = {27},
          number = {4},
           month = {December},
          author = {Jiaqi Liu and Saverio Iacoponi and Cecilia Laschi and Li Wen and Marcello Calisti},
           title = {Underwater Mobile Manipulation: A Soft Arm on a Benthic Legged Robot},
            year = {2020},
         journal = {IEEE Robotics \& Automation Magazine},
             doi = {10.1109/MRA.2020.3024001},
           pages = {12--26},
        keywords = {ARRAY(0x55988cf15ed0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/46137/},
        abstract = {Robotic systems that can explore the sea floor, collect marine samples, gather shallow water refuse, and perform other underwater tasks are interesting and important in several fields, from biology and ecology to off-shore industry. In this article, we present a robotic platform that is, to our knowledge, the first to combine benthic legged locomotion and soft continuum manipulation to perform real-world underwater mission-like experiments. We experimentally exploit inverse kinematics for spatial manipulation in a laboratory environment and then examine the robot's workspace extensibility, force, energy consumption, and grasping ability in different undersea scenarios.}
}

@inproceedings{lincoln42134,
       booktitle = {The IEEE International Conference on Advanced Robotics and Mechatronics (ARM)},
           month = {December},
           title = {Complementary Visual Neuronal Systems Model for Collision Sensing},
          author = {Qinbing Fu and Shigang Yue},
            year = {2020},
             doi = {10.1109/ICARM49381.2020.9195303},
        keywords = {ARRAY(0x55988cf15eb8)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42134/},
        abstract = {Inspired by insects? visual brains, this paper presents original modelling of a complementary visual neuronal systems model for real-time and robust collision sensing. Two categories of wide-?eld motion sensitive neurons, i.e., the lobula giant movement detectors (LGMDs) in locusts and the lobula plate tangential cells (LPTCs) in ?ies, have been studied, intensively. The LGMDs have speci?c selectivity to approaching objects in depth that threaten collision; whilst the LPTCs are only sensitive to translating objects in horizontal and vertical directions. Though each has been modelled and applied in various visual scenes including robot scenarios, little has been done on investigating their complementary functionality and selectivity when functioning together. To ?ll this vacancy, we introduce a hybrid model combining two LGMDs (LGMD-1 and LGMD2) with horizontally (rightward and leftward) sensitive LPTCs (LPTC-R and LPTC-L) specialising in fast collision perception. With coordination and competition between different activated neurons, the proximity feature by frontal approaching stimuli can be largely sharpened up by suppressing translating and receding motions. The proposed method has been implemented ingroundmicro-mobile robots as embedded systems. The multi-robot experiments have demonstrated the effectiveness and robustness of the proposed model for frontal collision sensing, which outperforms previous single-type neuron computation methods against translating interference.}
}

@inproceedings{lincoln42338,
       booktitle = {4th IEEE International Conference on Image Processing, Applications and Systems (IPAS)},
           month = {December},
           title = {Real-time Object Detection using Deep Learning for helping People with Visual Impairments},
          author = {Matteo Terreran and Andrea Tramontano and Jacobus Lock and Stefano Ghidoni and Nicola Bellotto},
       publisher = {IEEE},
            year = {2020},
             doi = {10.1109/IPAS50080.2020.9334933},
        keywords = {ARRAY(0x55988cf18140)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42338/},
        abstract = {Object detection plays a crucial role in the development of Electronic Travel Aids (ETAs), capable to guide a person with visual impairments towards a target object in an unknown indoor environment. In such a scenario, the object detector runs on a mobile device (e.g. smartphone) and needs to be fast, accurate, and, most importantly, lightweight. Nowadays, Deep Neural Networks (DNN) have become the state-of-the-art solution for object detection tasks, with many works improving speed and accuracy by proposing new architectures or extending existing ones. A common strategy is to use deeper networks to get higher performance, but that leads to a higher computational cost which makes it impractical to integrate them on mobile devices with limited computational power. In this work we compare different object detectors to find a suitable candidate to be implemented on ETAs, focusing on lightweight models capable of working in real-time on mobile devices with a good accuracy. In particular, we select two models: SSD Lite with Mobilenet V2 and Tiny-DSOD. Both models have been tested on the popular OpenImage dataset and a new dataset, called Office dataset, collected to further test models? performance and robustness in a real scenario inspired by the actual perception challenges of a user with visual impairments.}
}

@inproceedings{lincoln49496,
       booktitle = {21st Towards Autonomous Robotic Systems Conference},
           month = {December},
           title = {Modelling and Control of an End-Over-End Walking Robot},
          author = {Manu H. Nair and Mini Saaj and Amir G. Esfahani},
       publisher = {Springer},
            year = {2020},
             doi = {10.1007/978-3-030-63486-5\_15},
        keywords = {ARRAY(0x55988cf18128)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/49496/},
        abstract = {Over the last few decades, Space robots have found their applications in various in-orbit operations. The Canadarm2 and the European Robotic Arm (ERA), onboard the International Space Station (ISS), are exceptional examples of supervised robotic manipulators (RMs) used for station assembly and mainte?nance. However, in the case of in-space assembly of structures, like Large-Aperture Space Telescope (LAT) with an aperture larger than the Hubble Space Telescope (HST) and James Webb Space Telescope (JWST), missions are still in their infancy; this is heavily attributed to the limitations of current state-of-the-art Robotics, Automation and Autonomous Systems (RAAS) for the extreme space environ?ment. To address this challenge, this paper introduces the modelling and control of a candidate robotic architecture, inspired by Canadarm2 and ERA, for in-situ assembly of LAT. The kinematic and dynamic models of a five degrees-of-freedom (DoF) End-Over-End Walking robot's (E-Walker's) first phase of motion is pre?sented. A closed-loop feedback system validates the system's accurate gait pat?tern. The simulation results presented show that a Proportional-Integral-Derivative (PID) controller is able to track the desired joint angles without exceeding the joint torque limits; this ensures precise motion along the desired trajectory for one full cycle comprising of Phase-1 and Phase-2 respectively. The gait pattern of the E-Walker for the next phases is also briefly discussed.}
}

@inproceedings{lincoln40186,
       booktitle = {21st Towards Autonomous Robotic Systems Conference},
           month = {December},
           title = {Towards Safer Robot Motion: Using a Qualitative Motion Model to Classify Human-Robot Spatial Interaction},
          author = {Laurence Roberts-Elliott and Manuel Fernandez-Carmona and Marc Hanheide},
       publisher = {Springer},
            year = {2020},
        keywords = {ARRAY(0x55988cf18188)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/40186/},
        abstract = {For adoption of Autonomous Mobile Robots (AMR) across a breadth of industries, they must navigate around humans in a way which is safe and which humans perceive as safe, but without greatly compromising efficiency. This work aims to classify the Human-Robot Spatial Interaction (HRSI) situation of an interacting human and robot, to be applied in Human-Aware Navigation (HAN) to account for situational context. We develop qualitative probabilistic models of relative human and robot movements in various HRSI situations to classify situations, and explain our plan to develop per-situation probabilistic models of socially legible HRSI to predict human and robot movement. In future work we aim to use these predictions to generate qualitative constraints in the form of metric cost-maps for local robot motion planners, enforcing more efficient and socially legible trajectories which are both physically safe and perceived as safe.}
}

@article{lincoln46135,
          volume = {12},
          number = {6},
           month = {December},
          author = {Saverio Iacoponi and Marcello Calisti and Cecilia Laschi},
           title = {Simulation and Analysis of Microspines Interlocking Behavior on Rocky Surfaces: An In-Depth Study of the Isolated Spine},
         journal = {Journal of Mechanisms and Robotics},
             doi = {10.1115/1.4047725},
            year = {2020},
        keywords = {ARRAY(0x55988cf208f0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/46135/},
        abstract = {Microspine grippers address a large variety of possible applications, especially in field robotics and manipulation in extreme environments. Predicting and modeling the gripper behavior remains a major challenge to this day. One of the most complex aspects of these predictions is how to model the spine to rock interaction of the spine tip with the local asperity. This paper proposes a single spine model, in order to fill the gap of knowledge in this specific field. A new model for the anchoring resistance of a single spine is proposed and discussed. The model is then applied to a simulation campaign. With the aid of simulations and analytic functions, we correlated performance characteristics of a spine with a set of quantitative, macroscopic variables related to the spine, the substrate and its usage. Eventually, this paper presents some experimental comparison tests and discusses traversal phenomena observed during the tests.}
}

@article{lincoln43255,
          volume = {39},
           month = {November},
          author = {Gerard Canal and Rita Borgo and Andrew Coles and Archie Drake and Dong Huynh and Perry Keller and Senka Krivi{\'c} and Paul Luff and Quratul-ain Mahesar and Luc Moreau and Simon Parsons and Menisha Patel and Elizabeth Sklar},
           title = {Building Trust in Human-Machine Partnerships},
         journal = {Computer Law \& Security Review},
             doi = {10.1016/j.clsr.2020.105489},
           pages = {105489},
            year = {2020},
        keywords = {ARRAY(0x55988cf180c8)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/43255/},
        abstract = {Artificial Intelligence (AI) is bringing radical change to our lives. Fostering trust in this technology requires the technology to be transparent, and one route to transparency is to make the decisions that are reached by AIs explainable to the humans that interact with them. This paper lays out an exploratory approach to developing explainability and trust, describing the specific technologies that we are adopting, the social and organizational context in which we are working, and some of the challenges that we are addressing.}
}

@article{lincoln46141,
          volume = {20},
          number = {22},
           month = {November},
          author = {Giacomo Picardi and Clara Borrelli and Augusto Sarti and Giovanni Chimienti and Marcello Calisti},
           title = {A Minimal Metric for the Characterization of Acoustic Noise Emitted by Underwater Vehicles},
            year = {2020},
         journal = {Sensors},
             doi = {10.3390/s20226644},
           pages = {6644},
        keywords = {ARRAY(0x55988cf180e0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/46141/},
        abstract = {Underwater robots emit sound during operations which can deteriorate the quality of acoustic data recorded by on-board sensors or disturb marine fauna during in vivo observations. Notwithstanding this, there have only been a few attempts at characterizing the acoustic emissions of underwater robots in the literature, and the datasheets of commercially available devices do not report information on this topic. This work has a twofold goal. First, we identified a setup consisting of a camera directly mounted on the robot structure to acquire the acoustic data and two indicators (i.e., spectral roll-off point and noise introduced to the environment) to provide a simple and intuitive characterization of the acoustic emissions of underwater robots carrying out specific maneuvers in specific environments. Second, we performed the proposed analysis on three underwater robots belonging to the classes of remotely operated vehicles and underwater legged robots. Our results showed how the legged device produced a clearly different signature compared to remotely operated vehicles which can be an advantage in operations that require low acoustic disturbance. Finally, we argue that the proposed indicators, obtained through a standardized procedure, may be a useful addition to datasheets of existing underwater robots}
}

@article{lincoln42179,
          volume = {198},
           month = {October},
          author = {Petra Bosilj and Iain Gould and Tom Duckett and Grzegorz Cielniak},
           title = {Estimating soil aggregate size distribution from images using pattern spectra},
       publisher = {Elsevier},
            year = {2020},
         journal = {Biosystems Engineering},
             doi = {10.1016/j.biosystemseng.2020.07.012},
           pages = {63--77},
        keywords = {ARRAY(0x55988cf20920)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42179/},
        abstract = {A method for quantifying aggregate size distribution from the images of soil samples is introduced. Knowledge of soil aggregate size distribution can help to inform soil management practices for the sustainable growth of crops. While current in-field approaches are mostly subjective, obtaining quantifiable results in a laboratory is labour- and time-intensive. Our goal is to develop an imaging technique for quantitative analysis of soil aggregate size distribution, which could provide the basis of a tool for rapid assessment of soil structure. The prediction accuracy of pattern spectra descriptors based on hierarchical representations from attribute morphology are analysed, as well as the impact of using images of different quality and scales. The method is able to handle greater sample complexity than the previous approaches, while working with smaller samples sizes that are easier to handle. The results show promise for size analysis of soils with larger structures, and minimal sample preparation, as typical of soil assessment in agriculture.}
}

@article{lincoln46870,
          volume = {114},
           month = {October},
          author = {Qinbing Fu and Shigang Yue},
           title = {Modelling Drosophila motion vision pathways for decoding the direction of translating objects against cluttered moving backgrounds},
       publisher = {Springer},
            year = {2020},
         journal = {Biological Cybernetics},
             doi = {10.1007/s00422-020-00841-x},
           pages = {443--460},
        keywords = {ARRAY(0x55988cf20938)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/46870/},
        abstract = {Decoding the direction of translating objects in front of cluttered moving backgrounds, accurately and efficiently, is still a challenging problem. In nature, lightweight and low-powered flying insects apply motion vision to detect a moving target in highly variable environments during flight, which are excellent paradigms to learn motion perception strategies. This paper investigates the fruit fly Drosophila motion vision pathways and presents computational modelling based on cutting-edge physiological researches. The proposed visual system model features bio-plausible ON and OFF pathways, wide-field horizontal-sensitive (HS) and vertical-sensitive (VS) systems. The main contributions of this research are on two aspects: 1) the proposed model articulates the forming of both direction-selective (DS) and direction-opponent (DO) responses revealed as principal features of motion perception neural circuits, in a feed-forward manner; 2) it also shows robust direction selectivity to translating objects in front of cluttered moving backgrounds, via the modelling of spatiotemporal dynamics including combination of motion pre-filtering mechanisms and ensembles of local correlators inside both the ON and OFF pathways, which works effectively to suppress irrelevant background motion or distractors, and to improve the dynamic response. Accordingly, the direction of translating objects is decoded as global responses of both the HS and VS systems with positive or negative output indicating preferred-direction (PD) or null-direction (ND) translation. The experiments have verified the effectiveness of the proposed neural system model, and demonstrated its responsive preference to faster-moving, higher-contrast and larger-size targets embedded in cluttered moving backgrounds.}
}

@inproceedings{lincoln44709,
       booktitle = {2020 The 4th International Conference on Advances in Artificial Intelligence},
           month = {October},
           title = {Learning Symbolic Action Definitions from Unlabelled Image Pairs},
          author = {Helen Harman and Pieter Simoens},
            year = {2020},
           pages = {72--78},
             doi = {10.1145/3441417.3441419},
        keywords = {ARRAY(0x55988cf20968)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/44709/},
        abstract = {Task planners and goal recognisers often require symbolic models of an agent?s behaviour. These models are usually manually developed, which can be a time consuming and error prone process. Therefore, our work transforms unlabelled pairs of images, showing the state before and after an action has been executed, into reusable action definitions. Each action definition consist of a set of parameters, effects and preconditions. To evaluate these action definitions, states were generated and a task planner invoked. Problems with large state spaces were solved using the action definitions learnt from smaller state spaces. On average, the task plans contained 5.46 actions and planning took 0.06 seconds. Moreover, when 20 \% of transitions were missing, our approach generated the correct number of objects, action definitions and plans 70 \% of the time.}
}

@incollection{lincoln42872,
           month = {October},
          author = {Fanta Camara and Serhan Cosar and Nicola Bellotto and Natasha Merat and Charles Fox},
          series = {River Publishers Series in Transport Technology},
       booktitle = {Human Factors in Intelligent Vehicles},
          editor = {Cristina Olaverri-Monreal and Fernando Garc{\'i}a-Fern{\'a}ndez and Rosaldo J. F. Rossetti},
           title = {Continuous Game Theory Pedestrian Modelling Method for Autonomous Vehicles},
       publisher = {River Publishers},
            year = {2020},
        keywords = {ARRAY(0x55988cf20980)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42872/},
        abstract = {Autonomous Vehicles (AVs) must interact with other road users. They must understand and adapt to complex pedestrian behaviour, especially during crossings where priority is not clearly defined. This includes feedback effects
such as modelling a pedestrian?s likely behaviours resulting from changes in the AVs behaviour. For example, whether a pedestrian will yield if the AV accelerates, and vice versa. To enable such automated interactions, it is necessary for the AV to possess a statistical model of the pedestrian?s responses to its own actions. A previous work demonstrated a proof-of- concept method to fit parameters to a simplified model based on data from a highly artificial discrete laboratory task with human subjects. The method was based on LIDAR-based person tracking, game theory, and Gaussian process analysis. The present study extends this method to enable analysis of more realistic continuous human experimental data. It shows for the first time how game-theoretic predictive parameters can be fit into pedestrians natural and continuous motion during road-crossings, and how predictions can be made about their interactions with AV controllers in similar real-world settings.}
}

@inproceedings{lincoln42806,
       booktitle = {5th International Workshop on Non-Intrusive Load Monitoring},
           month = {October},
           title = {Lightweight Non-Intrusive Load Monitoring Employing Pruned Sequence-to-Point Learning},
          author = {Jack Barber and Heriberto Cuayahuitl and Mingjun Zhong and Wempen Luan},
       publisher = {ACM Conference Proceedings},
            year = {2020},
             doi = {10.1145/1122445.1122456},
        keywords = {ARRAY(0x55988cf209b0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42806/},
        abstract = {Non-intrusive load monitoring (NILM) is the process in which a household?s total power consumption is used to determine the power consumption of household appliances. 
Previous work has shown that sequence-to-point (seq2point) learning is one of the most promising methods for tackling NILM. This process uses a sequence of aggregate power data to map a target appliance's power consumption at the midpoint of that window of power data.
However, models produced using this method contain upwards of thirty million weights, meaning that the models require large volumes of resources to perform disaggregation. This paper addresses this problem by pruning the weights learned by such a model, which results in a lightweight NILM algorithm for the purpose of being deployed on mobile devices such as smart meters. The pruned seq2point learning algorithm was applied to the REFIT data, experimentally showing that the performance was retained comparing to the original seq2point learning whilst the number of weights was reduced by 87{$\backslash$}\%. Code:https://github.com/JackBarber98/pruned-nilm}
}

@inproceedings{lincoln42419,
       booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
           month = {October},
           title = {Incorporating Spatial Constraints into a Bayesian Tracking Framework for Improved Localisation in Agricultural Environments},
          author = {Waqas Khan and Gautham Das and Marc Hanheide and Grzegorz Cielniak},
       publisher = {IEEE},
            year = {2020},
        keywords = {ARRAY(0x55988cf209e0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42419/},
        abstract = {Global navigation satellite system (GNSS) has been considered as a panacea for positioning and tracking since the last decade. However, it suffers from severe limitations in terms of accuracy, particularly in highly cluttered and indoor environments. Though real-time kinematics (RTK) supported GNSS promises extremely accurate localisation, employing such services are expensive, fail in occluded environments and are unavailable in areas where cellular base stations are not accessible. It is, therefore, necessary that the GNSS data is to be filtered if high accuracy is required. Thus, this article presents a GNSS-based particle filter that exploits the spatial constraints imposed by the environment. In the proposed setup, the state prediction of the sample set follows a restricted motion according to the topological map of the environment. This results in the transition of the samples getting confined between specific discrete points, called the topological nodes, defined by a topological map. This is followed by a refinement stage where the full set of predicted samples goes through weighting and resampling, where the weight is proportional to the predicted particle?s proximity with the GNSS measurement. Thus, a discrete space continuous-time Bayesian filter is proposed, called the Topological Particle Filter (TPF). 
The proposed TPF is put to test by localising and tracking fruit pickers inside polytunnels. Fruit pickers inside polytunnels can only follow specific paths according to the topology of the tunnel. These paths are defined in the topological map of the polytunnels and are fed to TPF to tracks fruit pickers. Extensive datasets are collected to demonstrate the improved discrete tracking of strawberry pickers inside polytunnels thanks to the exploitation of the environmental constraints.}
}

@inproceedings{lincoln48338,
       booktitle = {Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
           month = {October},
           title = {On Robotic In-Orbit Assembly of Large Aperture Space Telescopes},
          author = {Manu H. Nair and Chakravarthini M. Saaj and Amir G. Esfahani},
       publisher = {IEEE},
            year = {2020},
        keywords = {ARRAY(0x55988cf20a10)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/48338/},
        abstract = {Space has found itself amidst numerous missions  benefitting the life on Earth and for mankind to explore further.
The space community has been in the move of launching various on-orbit missions, tackling the extremities of the space environment, with the use of robots, for performing tasks like assembly, maintenance, repairs, etc. The urge to explore further in the universe for scientific benefits has found the rise of modular Large-Space Telescopes (LASTs). With respect to the challenges of the in-space assembly of LAST, a five Degrees-of Freedom (DoF) End-Over-End Walking Robot (E-Walker) is presented in this paper. The Dynamical Model and Gait Pattern of the E-Walker is discussed with reference to the different phases of its motion. For the initial verification of the E-Walker model, a PID controller was used to make the E-Walker follow the desired trajectory. A mission concept discussing a potential strategy of assembling a 25m LAST with 342 Primary Mirror Units (PMUs) is briefly discussed. Simulation results show the 
precise tracking of the E-Walker along a desired trajectory is achieved without exceeding the joint torques.}
}

@inproceedings{lincoln49489,
       booktitle = {15th International Symposium on Artificial Intelligence, Robotics and Automation in Space (i-SAIRAS 2020)},
           month = {October},
           title = {Advances in Robotic In-Orbit Assembly of Large Aperture Space Telescopes},
          author = {Manu H. Nair and Mini Saaj and Sam Adlen and Amir G, Esfahani and Steve Eckersley},
       publisher = {European Space Agency},
            year = {2020},
        keywords = {ARRAY(0x55988cf20a40)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/49489/},
        abstract = {Modular Large Aperture Space Telescopes (LAST) hold the key to future astronomical missions in search of the origin of the cosmos. Robotics and Autonomous Systems technology would be required to meet the challenges associated with the assembly of such high value infrastructure in orbit. In this paper an End-Over-End walking robot is selected to assemble a 25m LAST. The dynamical model, control architecture and gait pattern of the E-Walker are discussed. The key mission requirements are stated along with the strategies for scheduling the assembly process. A mission concept of operations (ConOps) is proposed for assembling the 25m LAST. Simulation results show the precise trajectory tracking of the EWalker for the chosen mission scenario.}
}

@inproceedings{lincoln42213,
           month = {October},
          author = {Nikos Mavrakis and Rustam Stolkin and Amir Ghalamzan Esfahani},
       booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
           title = {Estimating An Object?s Inertial Parameters By Robotic Pushing: A Data-Driven Approach},
         journal = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2020)},
             doi = {10.1109/IROS45743.2020.9341112},
           pages = {9537--9544},
            year = {2020},
        keywords = {ARRAY(0x55988cf20a70)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42213/},
        abstract = {Estimating the inertial properties of an object can make robotic manipulations more efficient, especially in extreme environments. This paper presents a novel method of estimating the 2D inertial parameters of an object, by having a robot applying a push on it. We draw inspiration from previous analyses on quasi-static pushing mechanics, and introduce a data-driven model that can accurately represent these mechan- ics and provide a prediction for the object?s inertial parameters. We evaluate the model with two datasets. For the first dataset, we set up a V-REP simulation of seven robots pushing objects with large range of inertial parameters, acquiring 48000 pushes in total. For the second dataset, we use the object pushes from the MIT M-Cube lab pushing dataset. We extract features from force, moment and velocity measurements of the pushes, and train a Multi-Output Regression Random Forest. The experimental results show that we can accurately predict the 2D inertial parameters from a single push, and that our method retains this robust performance under various surface types.}
}

@inproceedings{lincoln49491,
       booktitle = {71st International Astronautical Congress},
           month = {October},
           title = {In-Space Robotic Assembly and Servicing of High-Value Infrastructure},
          author = {Manu H. Nair and Chakravarthini Mini Saaj and Amir G. Esfahani and Angadh Nanjangud and Steve Eckersley and Paolo Bianco},
       publisher = {International Astronautical Federation},
            year = {2020},
        keywords = {ARRAY(0x55988cf20aa0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/49491/},
        abstract = {With the advances in Robotics, Automation and Autonomous Systems (RAAS), the horizon of space
exploration has grown, and there is a need to develop space-qualified intelligent robots for future missions. Building
upon the heritage of successful surface exploration rover and lander missions to the Moon, Mars and Asteroids, the
space community worldwide is now pushing the frontiers of in-orbit robotics. Doubtlessly, RAAS will facilitate a
range of manufacturing, assembly, servicing and active debris removal missions. Resources published by space
agencies and major companies worldwide clearly indicate that mankind will start witnessing in-orbit robotic
missions within the next decade. This includes but not limited to building modular Large-Aperture Space Telescopes
(LAST), synthetic aperture radar, radiofrequency antennas, in-space power generation stations, mobile servicing
stations for repairing and maintenance of satellites and possibly large-scale infrastructure for space tourism. Out of
the many potential missions RAAS could support, in-orbit robotic assembly and construction of LAST is gaining
more popularity with the intent to understand the rate of growth of the cosmos and also for Earth Observation (EO).
However, there are numerous technical challenges associated with assembling LAST in space, including its
manufacturing and stowing into current and planned launch vehicles. To address these issues, a segmented design
approach for LAST is considered in this paper; the modular mirror units will be robotically assembled in orbit. The
in-space assembly of a modular 25m LAST mission concept is presented using a five Degrees-of-Freedom End-
Over-End Walking Robot (E-Walker). The design and gait pattern of the E-Walker is introduced first. The key
mission requirements including the requirements for the Robotic System, Space Telescope and Assembly are
discussed along with the strategies for scheduling the assembly process. Four main mission scenarios, subcategorised
into eleven mission scenarios are discussed in detail with a maximum of four E-Walkers. A trade-off
analysis was conducted to identify feasible mission scenarios and inferences are drawn accordingly to the best
mission concept of operations (ConOps) to realise the assembly of the 25m LAST. The results are based on the
overall mission mass-power budget, cost, control and motion planning complexity for the different mission scenarios
addressed in this paper. This study is a stepping stone towards proving the feasibility of the E-Walker for assembling
LAST; such advancements in orbital robotics will prove advantageous for building and servicing other high-value
infrastructure in space.}
}

@article{lincoln43697,
          volume = {12},
          number = {19},
           month = {October},
          author = {Dionysis Bochtis and Lefteris Benos and Maria Lampridi and Vasso Marinoudi and Simon Pearson and Claus G. S{\o}rensen},
           title = {Agricultural Workforce Crisis in Light of the COVID-19 Pandemic},
            year = {2020},
         journal = {Sustainability},
             doi = {10.3390/su12198212},
           pages = {8212},
        keywords = {ARRAY(0x55988cf20ad0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/43697/},
        abstract = {COVID-19 and the restrictive measures towards containing the spread of its infections have seriously affected the agricultural workforce and jeopardized food security. The present study aims at assessing the COVID-19 pandemic impacts on agricultural labor and suggesting strategies to mitigate them. To this end, after an introduction to the pandemic background, the negative consequences on agriculture and the existing mitigation policies, risks to the agricultural workers were benchmarked across the United States? Standard Occupational Classification system. The individual tasks associated with each occupation in agricultural production were evaluated on the basis of potential COVID-19 infection risk. As criteria, the most prevalent virus transmission mechanisms were considered, namely the possibility of touching contaminated surfaces and the close proximity of workers. The higher risk occupations within the sector were identified, which facilitates the allocation of worker protection resources to the occupations where they are most needed. In particular, the results demonstrated that 50\% of the agricultural workforce and 54\% of the workers? annual income are at moderate to high risk. As a consequence, a series of control measures need to be adopted so as to enhance the resilience and sustainability of the sector as well as protect farmers including physical distancing, hygiene practices, and personal protection equipment.}
}

@article{lincoln42612,
          volume = {35},
          number = {6},
           month = {September},
          author = {Gary Bosworth and Liz Price and Martin Collison and Charles Fox},
           title = {Unequal Futures of Rural Mobility:�Challenges for a ?Smart Countryside?},
       publisher = {Sage},
            year = {2020},
         journal = {Local Economy},
             doi = {10.1177/0269094220968231},
           pages = {586--608},
        keywords = {ARRAY(0x55988cf20b00)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42612/},
        abstract = {Current transport strategy in the UK is strongly urban-focused, with assumptions that technological advances in mobility will simply trickle down into rural areas. This paper challenges such a view and instead draws on rural development thinking aligned to a ?Smart Countryside? which emphasises the need for place-based approaches. Survey and interview methods are employed to develop a framework of rural needs associated with older people, younger people and businesses. This framework is employed to assess a range of mobility innovations that could most effectively address these needs in different rural contexts. In presenting visions of future rural mobility, the paper also identifies key infrastructure as well as institutional and financial changes that are required to facilitate the roll-out of new technologies across rural areas.}
}

@inproceedings{lincoln49494,
       booktitle = {Model Based Space Systems and Software Engineering MBSE2020},
           month = {September},
           title = {Cloud SF ? A continuous Integration Framework for the Design and Validation of Cyber-Physical Systems},
          author = {Gianmaria Bullegas and Anurag Kapur and Mini Saaj and Manu H. Nair and Adrian Pop and Peter Fritzson},
       publisher = {European Space Agency},
            year = {2020},
        keywords = {ARRAY(0x55988cf20b30)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/49494/},
        abstract = {The extensive use of virtual prototyping methods has become an indispensable tool in the context of
Model-Based Design of complex space missions. Modelling the behaviour of such missions often requires
considering systems that are composed of physical subsystems (usually from different physical domains)
together with computing and networking. Perpetual Labs is developing a new software platform for collaborative design of CPS called Cloud System Factory (CloudSF). It enables all the stakeholders of a complex engineering system to exchange system data and engineering artefacts independently of the specific tools that they are using. The proposed benefits of the CloudSF platform will be demonstrated and measured through the application
of said platform to the model-based design and verification of a robotic system for on-orbit assembly of
telescopic structures using an End-Over-End Walking robot, called the E-Walker.}
}

@article{lincoln42446,
          volume = {10},
          number = {1},
           month = {September},
          author = {Michelle T. Fountain and Amir Badiee and Sebastian Hemer and Alvaro Delgado and Michael Mangan and Colin Dowding and Frederick Davis and Simon Pearson},
           title = {The use of light spectrum blocking films to reduce populations of Drosophila suzukii Matsumura in fruit crops},
       publisher = {Nature Publishing Group},
            year = {2020},
         journal = {Scientific Reports},
             doi = {10.1038/s41598-020-72074-8},
        keywords = {ARRAY(0x55988cf20b60)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42446/},
        abstract = {Spotted wing drosophila, Drosophila suzukii, is a serious invasive pest impacting the production of
multiple fruit crops, including soft and stone fruits such as strawberries, raspberries and cherries.
Effective control is challenging and reliant on integrated pest management which includes the use
of an ever decreasing number of approved insecticides. New means to reduce the impact of this pest
that can be integrated into control strategies are urgently required. In many production regions,
including the UK, soft fruit are typically grown inside tunnels clad with polyethylene based materials.
These can be modified to filter specific wavebands of light. We investigated whether targeted spectral
modifications to cladding materials that disrupt insect vision could reduce the incidence of D. suzukii.
We present a novel approach that starts from a neuroscientific investigation of insect sensory systems
and ends with infield testing of new cladding materials inspired by the biological data. We show D.
suzukii are predominantly sensitive to wavelengths below 405 nm (ultraviolet) and above 565 nm
(orange \& red) and that targeted blocking of lower wavebands (up to 430 nm) using light restricting
materials reduces pest populations up to 73\% in field trials.}
}

@article{lincoln42433,
          volume = {7},
          number = {116},
           month = {September},
          author = {Francesco Del Duchetto and Paul Baxter and Marc Hanheide},
           title = {Are You Still With Me? Continuous Engagement Assessment From a Robot's Point of View},
       publisher = {Frontiers Media S.A.},
            year = {2020},
         journal = {Frontiers in Robotics and AI},
             doi = {10.3389/frobt.2020.00116},
        keywords = {ARRAY(0x55988cf20b90)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42433/},
        abstract = {Continuously measuring the engagement of users with a robot in a Human-Robot Interaction (HRI) setting paves the way toward in-situ reinforcement learning, improve metrics of interaction quality, and can guide interaction design and behavior optimization. However, engagement is often considered very multi-faceted and difficult to capture in a workable and generic computational model that can serve as an overall measure of engagement. Building upon the intuitive ways humans successfully can assess situation for a degree of engagement when they see it, we propose a novel regression model (utilizing CNN and LSTM networks) enabling robots to compute a single scalar engagement during interactions with humans from standard video streams, obtained from the point of view of an interacting robot. The model is based on a long-term dataset from an autonomous tour guide robot deployed in a public museum, with continuous annotation of a numeric engagement assessment by three independent coders. We show that this model not only can predict engagement very well in our own application domain but show its successful transfer to an entirely different dataset (with different tasks, environment, camera, robot and people). The trained model and the software is available to the HRI community, at https://github.com/LCAS/engagement\_detector, as a tool to measure engagement in a variety of settings.}
}

@inproceedings{lincoln34791,
          volume = {48},
           month = {September},
          author = {Sreedevi Kottayil and Panagiotis Tsoleridis and Kacper Rossa and Richard Connors and Charles Fox},
       booktitle = {15th World Conference on Transport Research},
           title = {Investigation of Driver Route Choice Behaviour using Bluetooth Data},
       publisher = {Elsevier},
            year = {2020},
         journal = {Transportation Research Procedia},
             doi = {10.1016/j.trpro.2020.08.065},
           pages = {632--645},
        keywords = {ARRAY(0x55988cf20bc0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/34791/},
        abstract = {Many local authorities use small-scale transport models to manage their transportation networks. These may assume drivers? behaviour to be rational in choosing the fastest route, and thus that all drivers behave the same given an origin and destination, leading to simplified aggregate flow models, fitted to anonymous traffic flow measurements. Recent price falls in traffic sensors, data storage, and compute power now enable Data Science to empirically test such assumptions, by using per-driver data to infer route selection from sensor observations and compare with optimal route selection. A methodology is presented using per-driver data to analyse driver route choice behaviour in transportation networks. Traffic flows on multiple measurable routes for origin destination pairs are compared based on the length of each route. A driver rationality index is defined by considering the shortest physical route between an origin-destination pair. The proposed method is intended to aid calibration of parameters used in traffic assignment models e.g. weights in generalized cost formulations or dispersion within stochastic user equilibrium models. The method is demonstrated using raw sensor datasets collected through Bluetooth sensors in the area of Chesterfield, Derbyshire, UK. The results for this region show that routes with a significant difference in lengths of their paths have the majority (71\%) of drivers using the optimal path but as the difference in length decreases, the probability of suboptimal route choice decreases (27\%). The methodology can be used for extended research considering the impact on route choice of other factors including travel time and road specific conditions.}
}

@inproceedings{lincoln43687,
           month = {September},
          author = {Hamid Isakhani and Shigang Yue and Caihua Xiong and Wenbin Chen and Xuelong Sun and Tian liu},
       booktitle = {5th International Conference on Advanced Robotics and Mechatronics (ICARM)},
           title = {Fabrication and Mechanical Analysis of Bioinspired Gliding-optimized Wing Prototypes for Micro Aerial Vehicles},
       publisher = {IEEE},
            year = {2020},
         journal = {2020 5th International Conference on Advanced Robotics and Mechatronics (ICARM)},
             doi = {10.1109/ICARM49381.2020.9195392},
           pages = {602--608},
        keywords = {ARRAY(0x55988cf20bf0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/43687/},
        abstract = {Gliding is the most efficient flight mode that is explicitly appreciated by natural fliers. This is achieved by high-performance structures developed over millions of years of evolution. One such prehistoric insect, locust (Schistocerca gregaria) is a perfect example of a natural glider capable of endured transatlantic flights, which could potentially inspire numerous solutions to the problems in aerospace engineering. However, biomimicry of such aerodynamic properties is hindered by the limitations of conventional as well as modern fabrication technologies in terms of precision and availability, respectively. Therefore, we explore and propose novel combinations of economical manufacturing methods to develop various locust-inspired tandem wing prototypes (i.e. fore and hindwings), for further wind tunnel based aerodynamic studies. Additionally, we determine the flexural stiffness and maximum deformation rate of our prototypes and compare it to their counterparts in nature and literature, recommending the most suitable artificial bioinspired wing for gliding micro aerial vehicle applications.}
}

@inproceedings{lincoln43680,
           month = {September},
          author = {Tian Liu and Xuelong Sun and Cheng Hu and Qinbing Fu and Hamid Isakhani and Shigang Yue},
       booktitle = {2020 5th International Conference on Advanced Robotics and Mechatronics (ICARM)},
           title = {Investigating Multiple Pheromones in Swarm Robots - A Case Study of Multi-Robot Deployment},
       publisher = {IEEE},
             doi = {10.1109/ICARM49381.2020.9195311},
           pages = {595--601},
            year = {2020},
        keywords = {ARRAY(0x55988cf20c20)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/43680/},
        abstract = {Social insects are known as the experts in handling complex task in a collective smart way although their small brains contain only limited computation resources and sensory information. It is believed that pheromones play a vital role in shaping social insects' collective behaviours. One of the key points underlying the stigmergy is the combination of different pheromones in a specific task. In the swarm intelligence field, pheromone inspired studies usually focus one single pheromone at a time, so it is not clear how effectively multiple pheromones could be employed for a collective strategy in the real physical world. In this study, we investigate multiple pheromone-based deployment strategy for swarm robots inspired by social insects. The proposed deployment strategy uses two kinds of artificial pheromones; the attractive and the repellent pheromone that enables micro robots to be distributed in desired positions with high efficiency. The strategy is assessed systematically by both simulation and real robot experiments using a novel artificial pheromone platform ColCOS{\ensuremath{\Phi}}. Results from the simulation and real robot experiments both demonstrate the effectiveness of the proposed strategy and reveal the role of multiple pheromones. The feasibility of the ColCOS{\ensuremath{\Phi}} platform, and its potential for further robotic research on multiple pheromones are also verified. Our study of using different pheromones for one collective swarm robotics task may help or inspire biologists in real insects' research.}
}

@article{lincoln43658,
          volume = {8},
           month = {September},
          author = {Cheng Hu and Caihua Xiong and Jigen Peng and Shigang Yue},
           title = {Coping With Multiple Visual Motion Cues Under Extremely Constrained Computation Power of Micro Autonomous Robots},
       publisher = {IEEE},
            year = {2020},
         journal = {IEEE Access},
             doi = {10.1109/ACCESS.2020.3016893},
           pages = {159050--159066},
        keywords = {ARRAY(0x55988cf20c50)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/43658/},
        abstract = {The perception of different visual motion cues is crucial for autonomous mobile robots to react to or interact with the dynamic visual world. It is still a great challenge for a micro mobile robot to cope with dynamic environments due to the restricted computational resources and the limited functionalities of its visual systems. In this study, we propose a compound visual neural system to automatically extract and fuse different visual motion cues in real-time using the extremely constrained computation power of micro mobile robots. The proposed visual system contains multiple bio-inspired visual motion perceptive neurons each with a unique role, for example to extract collision visual cues, darker collision cue and directional motion cues. In the embedded system, these multiple visual neurons share a similar presynaptic network to minimise the consumption of computation resources. In the postsynaptic part of the system, visual cues pass results to corresponding action neurons using lateral inhibition mechanism. The translational motion cues, which are identified by comparing pairs of directional cues, are given the highest priority, followed by the darker colliding cues and approaching cues. Systematic experiments with both virtual visual stimuli and real-world scenarios have been carried out to validate the system's functionality and reliability. The proposed methods have demonstrated that (1) with extremely limited computation power, it is still possible for a micro mobile robot to extract multiple visual motion cues robustly in a complex dynamic environment; (2) the cues extracted can be fused with a lateral inhibited postsynaptic network, thus enabling the micro robots to respond effectively with different actions, accordingly to different states, in real-time. The proposed embedded visual system has been modularised and can be easily implemented in other autonomous mobile platforms for real-time applications. The system could also be used by neurophysiologists to test new hypotheses pertaining to biological visual neural systems.}
}

@inproceedings{lincoln43365,
       booktitle = {9th International Conference, Living Machines 2020},
           month = {September},
           title = {Improving Recall in an Associative Neural Network Model of the Hippocampus},
          author = {Nikolas Andreakos and Shigang Yue and Vassilis Cutsuridis},
       publisher = {Springer Nature},
            year = {2020},
        keywords = {ARRAY(0x55988cf20c80)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/43365/},
        abstract = {The mammalian hippocampus is involved in auto-association and hetero-association of declarative
memories. We employed a bio-inspired neural model of hippocampal CA1 region to systematically
evaluate its mean recall quality against different number of stored patterns, overlaps and active cells per
pattern. Model consisted of excitatory (pyramidal cells) and four types of inhibitory cells: axo-axonic,
basket, bistratified, and oriens lacunosum-moleculare cells. Cells were simplified compartmental models
with complex ion channel dynamics. Cells? firing was timed to a theta oscillation paced by two distinct
neuronal populations exhibiting highly regular bursting activity, one tightly coupled to the trough and the
other to the peak of theta. During recall excitatory input to network excitatory cells provided context and
timing information for retrieval of previously stored memory patterns. Dendritic inhibition acted as a nonspecific
global threshold machine that removed spurious activity during recall. Simulations showed recall
quality improved when the network?s memory capacity increased as the number of active cells per pattern
decreased. Furthermore, increased firing rate of a presynaptic inhibitory threshold machine inhibiting a
network of postsynaptic excitatory cells has a better success at removing spurious activity at the network
level and improving recall quality than increased synaptic efficacy of the same threshold machine on the
same network of excitatory cells, while keeping its firing rate fixed.}
}

@inproceedings{lincoln41283,
           month = {August},
          author = {Soran Parsa and Disha Kamale and Sariah Mghames and Kiyanoush Nazari and Tommaso Pardi and Aravinda Srinivasan and Gerhard Neumann and Marc Hanheide and Amir Ghalamzan Esfahani},
       booktitle = {CASE 2020- International Conference on Automation Science and Engineering},
           title = {Haptic-guided shared control grasping: collision-free manipulation},
       publisher = {IEEE},
         journal = {International Conference on Automation Science and Engineering (CASE)},
             doi = {10.1109/CASE48305.2020.9216789},
            year = {2020},
        keywords = {ARRAY(0x55988cf20cb0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/41283/},
        abstract = {We propose a haptic-guided shared control system that provides an operator with force cues during reach-to-grasp phase of tele-manipulation. The force cues inform the operator of grasping configuration which allows collision-free autonomous post-grasp movements. Previous studies showed haptic guided shared control significantly reduces the complexities of the teleoperation. We propose two architectures of shared control in which the operator is informed about (1) the local gradient of the collision cost, and (2) the grasping configuration suitable for collision-free movements of an aimed pick-and-place task. We demonstrate the efficiency of our proposed shared control systems by a series of experiments with Franka Emika robot. Our experimental results illustrate our shared control systems successfully inform the operator of predicted collisions between the robot and an obstacle in the robot?s workspace. We learned that informing the operator of the global information about the grasping configuration associated with minimum collision cost of post-grasp movements results in a reach-to-grasp time much shorter than the case in which the operator is informed about the local-gradient information of the collision cost.}
}

@techreport{lincoln42273,
           month = {August},
            type = {Project Report},
           title = {The Future of Rural Mobility Study (FoRMS)},
          author = {Gary Bosworth and Charles Fox and Liz Price and Martin Collison},
       publisher = {Midlands Connect},
            year = {2020},
     institution = {Midlands Connect},
        keywords = {ARRAY(0x55988cf20ce0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42273/},
        abstract = {Recognising the urban-focus of many national and regional transport strategies, the purpose of this project is to explore how emerging technologies could support rural economies across the Midlands. Fundamentally, the rationale for the study is to begin with an assessment of rural needs and then exploring a range of mobility innovations, including social innovations as well as technologies, that can provide place-based solutions designed for more rural areas. This avoids the National Transport Strategy assumption that new mobility innovations will inevitably occur in urban areas and then be rolled out across more rural places. While economic realities mean that many private sector transport innovations can start out in urban centres, their rural impacts may be quite different and require alternative responses from rural planners and policy-makers.}
}

@article{lincoln39037,
          volume = {12},
          number = {3},
           month = {July},
          author = {Serhan Cosar and Manuel Fernandez-Carmona and Roxana Agrigoroaie and Jordi Pages and Francois Ferland and Feng Zhao and Shigang Yue and Nicola Bellotto and Adriana Tapus},
           title = {ENRICHME: Perception and Interaction of an Assistive Robot for the Elderly at Home},
       publisher = {Springer},
            year = {2020},
         journal = {International Journal of Social Robotics},
             doi = {10.1007/s12369-019-00614-y},
           pages = {779--805},
        keywords = {ARRAY(0x55988cf20d10)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/39037/},
        abstract = {Recent technological advances enabled modern robots to become part of our daily life. In particular, assistive robotics emerged as an exciting research topic that can provide solutions to improve the quality of life of elderly and vulnerable people. This paper introduces the robotic platform developed in the ENRICHME project, with particular focus on its innovative perception and interaction capabilities. The project?s main goal is to enrich the day-to-day experience of elderly people at home with technologies that enable health monitoring, complementary care, and social support. The paper presents several modules created to provide cognitive stimulation services for elderly users with mild cognitive impairments. The ENRICHME robot was tested in three pilot sites around Europe (Poland, Greece, and UK) and proven to be an effective assistant for the elderly at home.}
}

@article{lincoln40049,
          volume = {31},
          number = {12},
           month = {July},
          author = {Iain J Gould and Isobel Wright and Martin Collison and Eric Ruto and Gary Bosworth and Simon Pearson},
           title = {The impact of coastal flooding on agriculture: a case study of Lincolnshire, United Kingdom},
       publisher = {Wiley},
            year = {2020},
         journal = {Land Degradation \& Development},
             doi = {10.1002/ldr.3551},
           pages = {1545--1559},
        keywords = {ARRAY(0x55988cf20d40)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/40049/},
        abstract = {Under future climate predictions the incidence of coastal flooding is set to rise. Many coastal regions at risk, such as those surrounding the North Sea, comprise large areas of low-lying and productive agricultural land. Flood risk assessments typically emphasise the economic consequences of coastal  flooding on urban areas and national infrastructure. Impacts on agricultural land have seen less attention, and considerations tend to omit the long term effects of soil salinity. The aim of this study is to develop a universal framework to evaluate the economic impact of coastal flooding to agriculture. We incorporated existing flood models, satellite acquired crop data, soil salinity and crop sensitivity to give a novel and detailed assessment of salt damage to agricultural productivity over time. We focussed our case study on low-lying, highly productive agricultural land with a history of flooding in Lincolnshire, UK. The potential impact of agricultural flood damage varied across our study region.Assuming typical cropping does not change post-flood, financial losses range from {\pounds}1,366/ha to {\pounds}5,526/ha per inundation; these losses would be reduced by between 35\% up to 85\% in the likely event  that an alternative, more salt-tolerant, cropping, regime is implemented post-flood. These losses are substantially higher than loses calculated on the same areas using established flood risk assessment framework conventionally used for freshwater flood assessments, with differences attributed to our longer term salt damage projections impacting over several years. This suggests flood protection policy needs to consider local and long terms impacts of flooding on agricultural land.}
}

@article{lincoln41706,
           month = {July},
           title = {Pedestrian Models for Autonomous Driving Part II: High-Level Models of Human Behavior},
          author = {Fanta Camara and Nicola Bellotto and Serhan Cosar and Florian Weber and Dimitris Nathanael and Matthias Althoff and Jingyuan Wu and Johannes Ruenz and Andre Dietrich and Gustav Markkula and Anna Schieben and Fabio Tango and Natasha Merat and Charles Fox},
       publisher = {IEEE},
            year = {2020},
             doi = {10.1109/TITS.2020.3006767},
         journal = {IEEE Transactions on Intelligent Transport Systems},
        keywords = {ARRAY(0x55988cf20d70)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/41706/},
        abstract = {Abstract{--}Autonomous vehicles (AVs) must share space with pedestrians, both in carriageway cases such as cars at pedestrian crossings and off-carriageway cases such as delivery vehicles navigating through crowds on pedestrianized high-streets. Unlike static obstacles, pedestrians are active agents with complex, inter- active motions. Planning AV actions in the presence of pedestrians thus requires modelling of their probable future behaviour as well as detecting and tracking them. This narrative review article is Part II of a pair, together surveying the current technology stack involved in this process, organising recent research into a hierarchical taxonomy ranging from low-level image detection to high-level psychological models, from the perspective of an AV designer. This self-contained Part II covers the higher levels of this stack, consisting of models of pedestrian behaviour, from prediction of individual pedestrians? likely destinations and paths, to game-theoretic models of interactions between pedestrians and autonomous vehicles. This survey clearly shows that, although there are good models for optimal walking behaviour, high-level psychological and social modelling of pedestrian behaviour still remains an open research question that requires many conceptual issues to be clarified. Early work has been done on descriptive and qualitative models of behaviour, but much work is still needed to translate them into quantitative algorithms for practical AV control.}
}

@article{lincoln46139,
          volume = {15},
          number = {5},
           month = {July},
          author = {Mrudul Chellapurath and Sergio Stefanni and Graziano Fiorito and Angelo Maria Sabatini and Cecilia Laschi and Marcello Calisti},
           title = {Locomotory behaviour of the intertidal marble crab (Pachygrapsus marmoratus) supports the underwater spring-loaded inverted pendulum as a fundamental model for punting in animals},
            year = {2020},
         journal = {Bioinspiration \& Biomimetics},
             doi = {10.1088/1748-3190/ab968c},
           pages = {055004},
             url = {https://eprints.lincoln.ac.uk/id/eprint/46139/},
        abstract = {In aquatic pedestrian locomotion the dynamics of terrestrial and aquatic environments are coupled. Here we study terrestrial running and aquatic punting locomotion of the marine-living crab Pachygrapsus marmoratus. We detected both active and passive phases of running and punting through the observation of crab locomotory behaviour in standardized settings and by three-dimensional kinematic analysis of its dynamic gaits using high-speed video cameras. Variations in different stride parameters were studied and compared. The comparison was done based on the dimensionless parameter the Froude number (Fr) to account for the effect of buoyancy and size variability among the crabs. The underwater spring-loaded inverted pendulum (USLIP) model better fitted the dynamics of aquatic punting. USLIP takes account of the damping effect of the aquatic environment, a variable not considered by the spring-loaded inverted pendulum (SLIP) model in reduced gravity. Our results highlight the underlying principles of aquatic terrestrial locomotion by comparing it with terrestrial locomotion. Comparing punting with running, we show and increased stride period, decreased duty cycle and orientation of the carapace more inclined with the horizontal plane, indicating the significance of fluid forces on the dynamics due to the aquatic environment. Moreover, we discovered periodicity in punting locomotion of crabs and two different gaits, namely, long-flight punting and short-flight punting, distinguished by both footfall patterns and kinematic parameters. The generic fundamental model which belongs to all animals performing both terrestrial and aquatic legged locomotion has implications for control strategies, evolution and translation to robotic artefacts.}
}

@article{lincoln48336,
          volume = {4},
          number = {3},
           month = {July},
          author = {Amr Mohamed and Chakravarthini Saaj and Asma Seddaoui and Manu Nair},
           title = {Linear controllers for free-flying and controlled-floating space robots: a new perspective},
       publisher = {MedCrave Group},
            year = {2020},
         journal = {Aeronautics and Aerospace Open Access Journal},
             doi = {10.15406/aaoaj.2020.04.00112},
           pages = {97--114},
        keywords = {ARRAY(0x55988cf20dd0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/48336/},
        abstract = {Autonomous space robots are crucial for performing future in-orbit operations, including 
servicing of a spacecraft, assembly of large structures, maintenance of other space assets 
and active debris removal. Such orbital missions require servicer spacecraft equipped with 
one or more dexterous manipulators. However, unlike its terrestrial counterpart, the base 
of the robotic manipulator is not fixed in inertial space; instead, it is mounted on the base?spacecraft, which itself possess both translational and rotational motions. Additionally, the 
system will be subjected to extreme environmental perturbations, parametric uncertainties 
and system constraints due to the dynamic coupling between the manipulator and the 
base-spacecraft. This paper presents the dynamic model of the space robot and a three?stage control algorithm for this highly dynamic non-linear system. In this approach, feed?forward compensation and feed-forward linearization techniques are used to decouple and 
linearize the highly non-linear system respectively. This approach allows the use of the 
linear Proportional-Integral-Derivative (PID) controller and Linear Quadratic Regulator 
(LQR) in the final stages. Moreover, this paper covers a simulation-based trade-off analysis 
to determine both proposed linear controllers? efficacy. This assessment considers precise 
trajectory tracking requirements whilst minimizing power consumption and improving 
robustness during the close-range operation with the target spacecraft.}
}

@inproceedings{lincoln43819,
           month = {July},
          author = {Hamid Isakhani and Caihua Xiong and Shigang Yue and Wenbin Chen},
       booktitle = {2020 17th International Conference on Ubiquitous Robots (UR)},
           title = {A Bioinspired Airfoil Optimization Technique Using Nash Genetic Algorithm},
       publisher = {IEEE},
             doi = {10.1109/UR49135.2020.9144868},
           pages = {506--513},
            year = {2020},
        keywords = {ARRAY(0x55988cf20e00)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/43819/},
        abstract = {Natural fliers glide and minimize wing articulation to conserve energy for endured and long range flights. Elucidating the underlying physiology of such capability could potentially address numerous challenging problems in flight engineering. However, primitive nature of the bioinspired research impedes such achievements, hence to bypass these limitations, this study introduces a bioinspired non-cooperative multiple objective optimization methodology based on a novel fusion of PARSEC, Nash strategy, and genetic algorithms to achieve insect-level aerodynamic efficiencies. The proposed technique is validated on a conventional airfoil as well as the wing crosssection of a desert locust (Schistocerca gregaria) at low Reynolds number, and we have recorded a 77\% improvement in its gliding ratio.}
}

@article{lincoln41703,
          volume = {9},
           month = {July},
          author = {Xuelong Sun and Shigang Yue and Michael Mangan},
           title = {A decentralised neural model explaining optimal integration of navigational strategies in insects},
       publisher = {eLife Sciences Publications},
         journal = {eLife},
             doi = {10.7554/eLife.54026},
            year = {2020},
        keywords = {ARRAY(0x55988cf20e30)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/41703/},
        abstract = {Insect navigation arises from the coordinated action of concurrent guidance systems but the neural mechanisms through which each functions, and are then coordinated, remains unknown. We propose that insects require distinct strategies to retrace familiar routes (route-following) and directly return from novel to familiar terrain (homing) using different aspects of frequency encoded views that are processed in different neural pathways. We also demonstrate how the Central Complex and Mushroom Bodies regions of the insect brain may work in tandem to coordinate the directional output of different guidance cues through a contextually switched ring-attractor inspired by neural recordings. The resultant unified model of insect navigation reproduces behavioural data from a series of cue conflict experiments in realistic animal environments and offers testable hypotheses of where and how insects process visual cues, utilise the different information that they provide and coordinate their outputs to achieve the adaptive behaviours observed in the wild.}
}

@article{lincoln42805,
          volume = {396},
           month = {July},
          author = {Heriberto Cuayahuitl},
            note = {The final published version of this article can be accessed online at https://www.journals.elsevier.com/neurocomputing/},
           title = {A Data-Efficient Deep Learning Approach for Deployable Multimodal Social Robots},
       publisher = {Elsevier},
            year = {2020},
         journal = {Neurocomputing},
             doi = {10.1016/j.neucom.2018.09.104},
           pages = {587--598},
        keywords = {ARRAY(0x55988cf20e60)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42805/},
        abstract = {The deep supervised and reinforcement learning paradigms (among others) have the potential to endow interactive multimodal social robots with the ability of acquiring skills autonomously. But it is still not very clear yet how they can be best deployed in real world applications. As a step in this direction, we propose a deep learning-based approach for efficiently training a humanoid robot to play multimodal games---and use the game of `Noughts {$\backslash$}\& Crosses' with two variants as a case study. Its minimum requirements for learning to perceive and interact are based on a few hundred example images, a few  example multimodal dialogues and physical demonstrations of robot manipulation, and automatic simulations. In addition, we propose novel algorithms for robust visual game tracking and for competitive policy learning with high winning rates, which substantially outperform DQN-based baselines. While an automatic evaluation shows evidence that the proposed approach can be easily extended to new games with competitive robot behaviours, a human evaluation with 130 humans playing with the \{{$\backslash$}it Pepper\} robot confirms that highly accurate visual perception is required for successful game play.}
}

@article{lincoln42133,
           month = {July},
           title = {Modelling Drosophila motion vision pathways for decoding the direction of translating objects against cluttered moving backgrounds},
          author = {Qinbing Fu and Shigang Yue},
       publisher = {Springer},
            year = {2020},
             doi = {10.1007/s00422-020-00841-x},
         journal = {Biological Cybernetics},
        keywords = {ARRAY(0x55988cf20e90)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42133/},
        abstract = {Decoding the direction of translating objects in front of cluttered moving backgrounds, accurately and ef?ciently, is still a challenging problem. In nature, lightweight and low-powered ?ying insects apply motion vision to detect a moving target in highly variable environments during ?ight, which are excellent paradigms to learn motion perception strategies. This paper investigates the fruit ?y Drosophila motion vision pathways and presents computational modelling based on cuttingedge physiological researches. The proposed visual system model features bio-plausible ON and OFF pathways, wide-?eld horizontal-sensitive (HS) and vertical-sensitive (VS) systems. The main contributions of this research are on two aspects: (1) the proposed model articulates the forming of both direction-selective and direction-opponent responses, revealed as principalfeaturesofmotionperceptionneuralcircuits,inafeed-forwardmanner;(2)italsoshowsrobustdirectionselectivity to translating objects in front of cluttered moving backgrounds, via the modelling of spatiotemporal dynamics including combination of motion pre-?ltering mechanisms and ensembles of local correlators inside both the ON and OFF pathways, which works effectively to suppress irrelevant background motion or distractors, and to improve the dynamic response. Accordingly, the direction of translating objects is decoded as global responses of both the HS and VS systems with positive ornegativeoutputindicatingpreferred-direction or null-direction translation.The experiments have veri?ed the effectiveness of the proposed neural system model, and demonstrated its responsive preference to faster-moving, higher-contrast and larger-size targets embedded in cluttered moving backgrounds.}
}

@inproceedings{lincoln39957,
       booktitle = {ICRA 2020},
           month = {July},
           title = {Enhancing Grasp Pose Computation in Gripper Workspace Spheres},
          author = {Mohamed Sorour and Khaled Elgeneidy and Marc Hanheide and Aravinda Srinivasan},
            year = {2020},
        keywords = {ARRAY(0x55988cf20ec0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/39957/},
        abstract = {In this paper, enhancement to the novel grasp planning algorithm based on gripper workspace spheres is presented. Our development requires a registered point cloud of the target from different views, assuming no prior knowledge of the object, nor any of its properties. This work features
a new set of metrics for grasp pose candidates evaluation, as well as exploring the impact of high object sampling on grasp success rates. In addition to gripper position sampling, we now perform orientation sampling about the x, y, and z-axes, hence the grasping algorithm no longer require object orientation estimation. Successful experiments have been conducted on a simple jaw gripper (Franka Panda gripper) as well as a complex, high Degree of Freedom (DoF) hand (Allegro hand) as a proof of its versatility. Higher grasp success rates of 76\% and 85:5\% respectively has been reported by real world experiments.}
}

@article{lincoln41718,
          volume = {31},
          number = {6},
           month = {June},
          author = {Daqi Liu and Nicola Bellotto and Shigang Yue},
           title = {Deep Spiking Neural Network for Video-based Disguise Face Recognition Based on Dynamic Facial Movements},
       publisher = {IEEE},
            year = {2020},
         journal = {IEEE Transactions on Neural Networks and Learning Systems},
             doi = {10.1109/TNNLS.2019.2927274},
           pages = {1843--1855},
        keywords = {ARRAY(0x55988cf20ef0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/41718/},
        abstract = {With  the  increasing  popularity  of  social  media  andsmart  devices,  the  face  as  one  of  the  key  biometrics  becomesvital  for  person  identification.  Amongst  those  face  recognitionalgorithms, video-based face recognition methods could make useof  both  temporal  and  spatial  information  just  as  humans  do  toachieve  better  classification  performance.  However,  they  cannotidentify individuals when certain key facial areas like eyes or noseare disguised by heavy makeup or rubber/digital masks. To thisend, we propose a novel deep spiking neural network architecturein this study. It takes dynamic facial movements, the facial musclechanges induced by speaking or other activities, as the sole input.An  event-driven  continuous  spike-timing  dependent  plasticitylearning  rule  with  adaptive  thresholding  is  applied  to  train  thesynaptic weights. The experiments on our proposed video-baseddisguise  face  database  (MakeFace  DB)  demonstrate  that  theproposed learning method performs very well - it achieves from95\% to  100\% correct classification rates  under various realisticexperimental  scenarios}
}

@inproceedings{lincoln43425,
           month = {June},
          author = {Justin Le Louedec and Hector A. Montes and Tom Duckett and Grzegorz Cielniak},
       booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
           title = {Segmentation and detection from organised 3D point clouds: a case study in broccoli head detection},
       publisher = {IEEE},
             doi = {10.1109/CVPRW50498.2020.00040},
           pages = {285--293},
            year = {2020},
        keywords = {ARRAY(0x55988cf20f20)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/43425/},
        abstract = {Autonomous harvesting is becoming an important challenge and necessity in agriculture, because of the lack of labour and the growth of population needing to be fed. Perception is a key aspect of autonomous harvesting and is very challenging due to difficult lighting conditions, limited sensing technologies, occlusions, plant growth, etc. 3D vision approaches can bring several benefits addressing the aforementioned challenges such as localisation, size estimation, occlusion handling and shape analysis. In this paper, we propose a novel approach using 3D information for detecting broccoli heads based on Convolutional Neural Networks (CNNs), exploiting the organised nature of the point clouds originating from the RGBD sensors. The proposed algorithm, tested on real-world datasets, achieves better performances than the state-of-the-art, with better accuracy and generalisation in unseen scenarios, whilst significantly reducing inference time, making it better suited for real-time in-field applications.}
}

@inproceedings{lincoln40182,
       booktitle = {IEEE RoboSoft 2020},
           month = {June},
           title = {Structural Optimization of Adaptive Soft Fin Ray Fingers with Variable Stiffening Capability},
          author = {Khaled Elgeneidy and Khaled Goher},
       publisher = {IEEE},
            year = {2020},
             doi = {10.1109/RoboSoft48309.2020.9115969},
        keywords = {ARRAY(0x55988cf20f50)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/40182/},
        abstract = {Soft and adaptable grippers are desired for their ability to operate effectively in unstructured or dynamically changing environments, especially when interacting with delicate or deformable targets. However, utilizing soft bodies often comes at the expense of reduced carrying payload and limited performance in high-force applications. Hence, methods for achieving variable stiffness soft actuators are being investigated to broaden the applications of soft grippers. This paper investigates the structural optimization of adaptive soft fingers based on the Fin Ray? effect (Soft Fin Ray), featuring a passive stiffening mechanism that is enabled via layer jamming between deforming flexible ribs. A finite element model of the proposed Soft Fin Ray structure is developed and experimentally validated, with the aim of enhancing the layer jamming behavior for better grasping performance. The results showed that through structural optimization, initial contact forces before jamming can be minimized and final contact forces after jamming can be significantly enhanced, without downgrading the desired passive adaptation to objects. Thus, applications for Soft Fin Ray fingers can range from adaptive delicate grasping to high-force manipulation tasks.}
}

@inproceedings{lincoln45041,
       booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
           month = {June},
           title = {Segmentation and detection from organised 3D point clouds: a case study in broccoli head detection},
          author = {Justin Le Louedec and Hector Montes and Tom Duckett and Grzegorz Cielniak},
       publisher = {IEEE},
            year = {2020},
        keywords = {ARRAY(0x55988cf20f80)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/45041/},
        abstract = {Autonomous harvesting is becoming an important challenge and necessity in agriculture, because of the lack of labour and the growth of population needing to be fed. Perception is a key aspect of autonomous harvesting and is very challenging due to difficult lighting conditions, limited sensing technologies, occlusions, plant growth, etc. 3D vision approaches can bring several benefits addressing the aforementioned challenges such as localisation, size estimation, occlusion handling and shape analysis. In this paper, we propose a novel approach using 3D information for detecting broccoli heads based on Convolutional Neural Networks (CNNs), exploiting the organised nature of the point clouds originating from the RGBD sensors. The proposed algorithm, tested on real-world datasets, achieves better performances than the state-of-the-art, with better accuracy and generalisation in unseen scenarios, whilst significantly reducing inference time, making it better suited for real-time in-field applications.}
}

@article{lincoln41120,
          volume = {5},
          number = {3},
           month = {June},
          author = {Riccardo Polvara and Manuel Fernandez-Carmona and Marc Hanheide and Gerhard Neumann},
           title = {Next-Best-Sense: a multi-criteria robotic exploration strategy for RFID tags discovery},
       publisher = {IEEE},
            year = {2020},
         journal = {IEEE Robotics and Automation Letters},
             doi = {10.1109/LRA.2020.3001539},
           pages = {4477--4484},
        keywords = {ARRAY(0x55988cf20fb0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/41120/},
        abstract = {Automated exploration is one of the most relevant applications of autonomous robots. In this paper, we suggest a novel online coverage algorithm called Next-Best-Sense (NBS), an extension of the Next-Best-View class of exploration algorithms that optimizes the exploration task balancing multiple criteria. This novel algorithm is applied to the problem of localizing all Radio Frequency Identification (RFID) tags with a mobile robotic platform that is equipped with a RFID reader. We cast this problem as a coverage planning problem by defining a basic sensing operation -- a scan with the RFID reader -- as the field of ?view? of the sensor. NBS evaluates candidate locations with a global utility function which combines utility values for travel distance, information gain, sensing time, battery status and RFID information gain, generalizing the use of Multi-Criteria Decision Making. We developed an RFID reader and tag model in the Gazebo simulator for validation. Experiments performed both in simulation and with a real robot suggest that our NBS approach can successfully localize all the RFID tags while minimizing navigation metrics such sensing operations, total traveling distance and battery consumption. The code developed is publicly available on the authors' repository.}
}

@article{lincoln42131,
          volume = {8},
           month = {June},
          author = {Qinbing Fu and Huatian Wang and Jigen Peng and Shigang Yue},
           title = {Improved Collision Perception Neuronal System Model with Adaptive Inhibition Mechanism and Evolutionary Learning},
       publisher = {IEEE},
            year = {2020},
         journal = {IEEE Access},
             doi = {10.1109/ACCESS.2020.3001396},
           pages = {108896--108912},
        keywords = {ARRAY(0x55988cf20fe0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42131/},
        abstract = {Accurate and timely perception of collision in highly variable environments is still a challenging problem for arti?cial visual systems. As a source of inspiration, the lobula giant movement detectors (LGMDs) in locust?s visual pathways have been studied intensively, and modelled as quick collision detectors against challenges from various scenarios including vehicles and robots. However, the state-of-the-art LGMD models have not achieved acceptable robustness to deal with more challenging scenarios like the various vehicle driving scenes, due to the lack of adaptive signal processing mechanisms. To address this problem, we propose an improved neuronal system model, called LGMD+, that is featured by novel modelling of spatiotemporal inhibition dynamics with biological plausibilities including 1) lateral inhibitionswithglobalbiasesde?nedbyavariantofGaussiandistribution,spatially,and2)anadaptivefeedforward inhibition mediation pathway, temporally. Accordingly, the LGMD+ performs more effectively to detect merely approaching objects threatening head-on collision risks by appropriately suppressing motion distractors caused by vibrations, near-miss or approaching stimuli with deviations from the centre view. Through evolutionary learning with a systematic dataset of various crash and non-collision driving scenarios, the LGMD+ shows improved robustness outperforming the previous related methods. After evolution, its computational simplicity, ?exibility and robustness have also been well demonstrated by real-time experiments of autonomous micro-mobile robots.}
}

@article{lincoln41217,
           month = {June},
           title = {Road users rarely use explicit communication when interacting in today?s traffic: implications for automated vehicles},
          author = {Yee Mun Lee and Ruth Madigan and Oscar Giles and Laura Garach?Morcillo and Gustav Markkula and Charles Fox and Fanta Camara and Markus Rothmueller and Signe Alexandra Vendelbo?Larsen and Pernille Holm Rasmussen and Andre Dietrich and Dimitris Nathanael and Villy Portouli and Anna Schieben and Natasha Merat},
       publisher = {Springer},
            year = {2020},
             doi = {10.1007/s10111-020-00635-y},
         journal = {Cognition, Technology \& Work},
        keywords = {ARRAY(0x55988cf21010)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/41217/},
        abstract = {To be successful, automated vehicles (AVs) need to be able to manoeuvre in mixed traffic in a way that will be accepted by
road users, and maximises traffic safety and efficiency. A likely prerequisite for this success is for AVs to be able to commu-
nicate effectively with other road users in a complex traffic environment. The current study, conducted as part of the European
project interACT, investigates the communication strategies used by drivers and pedestrians while crossing the road at six
observed locations, across three European countries. In total, 701 road user interactions were observed and annotated, using
an observation protocol developed for this purpose. The observation protocols identified 20 event categories, observed from
the approaching vehicles/drivers and pedestrians. These included information about movement, looking behaviour, hand
gestures, and signals used, as well as some demographic data. These observations illustrated that explicit communication
techniques, such as honking, flashing headlights by drivers, or hand gestures by drivers and pedestrians, rarely occurred.
This observation was consistent across sites. In addition, a follow-on questionnaire, administered to a sub-set of the observed
pedestrians after crossing the road, found that when contemplating a crossing, pedestrians were more likely to use vehicle-
based behaviour, rather than communication cues from the driver. Overall, the findings suggest that vehicle-based movement
information such as yielding cues are more likely to be used by pedestrians while crossing the road, compared to explicit
communication cues from drivers, although some cultural differences were observed. The implications of these findings are
discussed with respect to design of suitable external interfaces and communication of intent by future automated vehicles.}
}

@article{lincoln40882,
           month = {June},
           title = {Robot Perception of Static and Dynamic Objects with an Autonomous Floor Scrubber},
          author = {Zhi Yan and Simon Schreiberhuber and Georg Halmetschlager and Tom Duckett and Markus Vincze and Nicola Bellotto},
       publisher = {Springer},
            year = {2020},
             doi = {10.1007/s11370-020-00324-9},
         journal = {Intelligent Service Robotics},
        keywords = {ARRAY(0x55988cf21040)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/40882/},
        abstract = {This paper presents the perception system of a new professional cleaning robot for large public places. The proposed system is based on multiple sensors including 3D and 2D lidar, two RGB-D cameras and a stereo camera. The two lidars together with an RGB-D camera are used for dynamic object (human) detection and tracking, while the second RGB-D and stereo camera are used for detection of static objects (dirt and ground objects). A learning and reasoning module for spatial-temporal representation of the environment based on the perception pipeline is also introduced. Furthermore, a new dataset collected with the robot in several public places, including a supermarket, a warehouse and an airport, is released.Baseline results on this dataset for further research and comparison are provided. The proposed system has been fully implemented into the Robot Operating System (ROS) with high modularity, also publicly available to the community.}
}

@inproceedings{lincoln42389,
           month = {May},
          author = {Adam Binch and Gautham Das and Jaime Pulido Fentanes and Marc Hanheide},
       booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
           title = {Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation},
       publisher = {IEEE},
             doi = {10.1109/ICRA40945.2020.9196550},
           pages = {3937--3943},
            year = {2020},
        keywords = {ARRAY(0x55988cf21070)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42389/},
        abstract = {Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task, while considering the respective navigation algorithms themselves mostly as a "black box", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.}
}

@article{lincoln38163,
          volume = {117},
           month = {May},
          author = {Ibrahim Albayati and Andrey Postnikov and Simon Pearson and Ronald Bickerton and Argyrios Zolotas and Chris Bingham},
           title = {Power and Energy Analysis for a Commercial Retail Refrigeration System Responding to a Static Demand Side Response},
       publisher = {Elsevier},
            year = {2020},
         journal = {International Journal of Electrical Power \& Energy Systems},
             doi = {10.1016/j.ijepes.2019.105645},
           pages = {105645},
        keywords = {ARRAY(0x55988cf210a0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38163/},
        abstract = {The paper considers the impact of Demand Side Response events on supply power profile and energy efficiency of widely distributed aggregated loads applied across commercial refrigeration systems. Responses to secondary grid frequency static DSR events are investigated. Experimental trials are conducted on a system of refrigerators representing a small retail store, and subsequently on the refrigerators of an operational superstore in the UK. Energy consumption and energy savings during 3 hours of operation, pre and post-secondary DSR, are discussed. In addition, a simultaneous secondary DSR event is realised across three operational retail stores located in different geographical regions of the UK. A Simulink model for a 3{\ensuremath{\Phi}} power network is used to investigate the impact of a synchronised return to normal operation of the aggregated refrigeration systems post DSR on the local power network. Results show {\texttt{\char126}}1\% drop in line voltage due to the synchronised return to operation. An analysis of energy consumption shows that DSR events can facilitate energy savings of between 3.8\% and 9.3\% compared to normal operation. This is a result of the refrigerators operating more efficiently during and shortly after the DSR. The use of aggregated refrigeration loads can contribute to the necessary load-shed by 97.3\% at the beginning of DSR and 27\% during 30 minutes DSR, based on a simultaneous DSR event carried out on three retail stores.}
}

@inproceedings{lincoln43349,
           month = {May},
          author = {Li Sun and Daniel Adolfsson and Martin Magnusson and Henrik Andreasson and Ingmar Posner and Tom Duckett},
       booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
           title = {Localising Faster: Efficient and precise lidar-based robot localisation in large-scale environments},
       publisher = {IEEE},
             doi = {10.1109/ICRA40945.2020.9196708},
           pages = {4386--4392},
            year = {2020},
        keywords = {ARRAY(0x55988cf210d0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/43349/},
        abstract = {This paper proposes a novel approach for global localisation of mobile robots in large-scale environments. Our method leverages learning-based localisation and filtering-based localisation, to localise the robot efficiently and precisely through seeding Monte Carlo Localisation (MCL) with a deep learned distribution. In particular, a fast localisation system rapidly estimates the 6-DOF pose through a deep-probabilistic model (Gaussian Process Regression with a deep kernel), then a precise recursive estimator refines the estimated robot pose according to the geometric alignment. More importantly, the Gaussian method (i.e. deep probabilistic localisation) and non-Gaussian method (i.e. MCL) can be integrated naturally via importance sampling. Consequently, the two systems can be integrated seamlessly and mutually benefit from each other. To verify the proposed framework, we provide a case study in large-scale localisation with a 3D lidar sensor. Our experiments on the Michigan NCLT long-term dataset show that the proposed method is able to localise the robot in 1.94 s on average (median of 0.8 s) with precision 0.75 m in a large-scale environment of approximately 0.5 km 2 .}
}

@article{lincoln46143,
          volume = {5},
          number = {42},
           month = {May},
          author = {G. Picardi and M. Chellapurath and S. Iacoponi and S. Stefanni and C. Laschi and M. Calisti},
           title = {Bioinspired underwater legged robot for seabed exploration with low environmental disturbance},
            year = {2020},
         journal = {Science Robotics},
             doi = {10.1126/scirobotics.aaz1012},
           pages = {eaaz1012},
             url = {https://eprints.lincoln.ac.uk/id/eprint/46143/},
        abstract = {Robots have the potential to assist and complement humans in the study and exploration of extreme and hostile environments. For example, valuable scientific data have been collected with the aid of propeller-driven autonomous and remotely operated vehicles in underwater operations. However, because of their nature as swimmers, such robots are limited when closer interaction with the environment is required. Here, we report a bioinspired underwater legged robot, called SILVER2, that implements locomotion modalities inspired by benthic animals (organisms that harness the interaction with the seabed to move; for example, octopi and crabs). Our robot can traverse irregular terrains, interact delicately with the environment, approach targets safely and precisely, and hold position passively and silently. The capabilities of our robot were validated through a series of field missions in real sea conditions in a depth range between 0.5 and 12 meters.}
}

@article{lincoln48337,
          volume = {65},
          number = {10},
           month = {May},
          author = {Lucy Jackson and Chakravarthini M. Saaj and Asma Seddaoui and Calem Whiting and Steve Eckersley and Simon Hadfield},
           title = {Downsizing an orbital space robot: A dynamic system based evaluation},
       publisher = {Elsevier},
            year = {2020},
         journal = {Advances in Space Research},
             doi = {10.1016/j.asr.2020.03.004},
           pages = {2247--2262},
        keywords = {ARRAY(0x55988cf21130)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/48337/},
        abstract = {Small space robots have the potential to revolutionise space exploration by facilitating the on-orbit assembly of infrastructure, in shorter time scales, at reduced costs. Their commercial appeal will be further improved if such a system is also capable of performing on-orbit servicing missions, in line with the current drive to limit space debris and prolong the lifetime of satellites already in orbit. Whilst there have been a limited number of successful demonstrations of technologies capable of these on-orbit operations, the systems remain large and bespoke. The recent surge in small satellite technologies is changing the economics of space and in the near future, downsizing a space robot might become be a viable option with a host of benefits. This industry wide shift means some of the technologies for use with
a downsized space robot, such as power and communication subsystems, now exist. However, there are still dynamic and control issues that need to be overcome before a downsized space robot can be capable of undertaking useful missions. This paper first outlines these issues, before analyzing the effect of downsizing a system on its operational capability. Therefore presenting the smallest controllable system such that the benefits of a small space robot can be achieved with current technologies. The sizing of the base spacecraft and manipulator are addressed here. The design presented consists of a 3 link, 6 degrees of freedom robotic manipulator mounted on a 12U form factor satellite. The feasibility of this 12U space robot was evaluated in simulation and the in-depth results presented here support the hypothesis that a small space robot is a viable solution for in-orbit operations.}
}

@inproceedings{lincoln45011,
           month = {May},
          author = {Tsvetan Zhivkov and Elizabeth Sklar},
       booktitle = {3rd UK-RAS Conference},
           title = {Modelling variable communication signal strength for experiments with multi-robot teams},
       publisher = {UK-RAS},
             doi = {10.31256/Ld2Re8B},
           pages = {128--130},
            year = {2020},
        keywords = {ARRAY(0x55988cf21160)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/45011/},
        abstract = {Reliable communication is a critical factor for ensuring robust performance of multi-robot teams. A selection of results are presented here comparing the impact of poor network quality on team performance under several conditions. Two different processes for emulating degraded network signal strength are compared in a physical environment: modelled signal degradation (MSD), approximated according to increasing distance from a connected network node (ie robot), versus effective signal degradation (ESD). The results of both signal strength processes exhibit similar trends, demonstrating that ESD in a physical environment can be modelled relatively well using MSD.}
}

@inproceedings{lincoln40029,
       booktitle = {8th Transport Research Arena TRA 2020},
           month = {April},
           title = {Examining Pedestrian-Autonomous Vehicle Interactions in Virtual Reality},
          author = {Fanta Camara and Patrick Dickenson and Natasha Merat and Charles Fox},
            year = {2020},
        keywords = {ARRAY(0x55988cf21190)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/40029/},
        abstract = {Autonomous vehicles now have well developed algorithms and open source software for localisation and
navigation in static environments but their future interactions with other road users in mixed traffic
environments, especially with pedestrians, raise some concerns. Pedestrian behaviour is complex to model and
unpredictable, thus creating a big challenge for self-driving cars. This paper examines pedestrian behaviour
during crossing scenarios with a game theoretic autonomous vehicle in virtual reality. In a first experiment, we
recorded participants? trajectories and found that they were crossing more cautiously in VR than in previous
laboratory experiments. In two other experiments, we used a gradient descent approach to investigate
participants? preference for a certain AV driving style. We found that the majority of them were not expecting the
car to stop in these scenarios. These results suggest that VR is an interesting tool for testing autonomous vehicle
algorithms and for finding out about pedestrian preferences.}
}

@article{lincoln39383,
          volume = {5},
          number = {2},
           month = {April},
          author = {Daniel De Barrie and Rebecca Margetts and Khaled Goher},
           title = {SIMPA: Soft-Grasp Infant Myoelectric Prosthetic Arm},
       publisher = {IEEE},
            year = {2020},
         journal = {IEEE Robotics and Automation Letters},
             doi = {10.1109/LRA.2019.2963820},
           pages = {699--704},
        keywords = {ARRAY(0x55988cf211c0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/39383/},
        abstract = {Myoelectric prosthetic arms have primarily focused on adults, despite evidence showing the benefits of early adoption. This work presents SIMPA, a low-cost 3D-printed prosthetic arm with soft grippers. The arm has been designed using CAD and 3D-scanning, and manufactured using
predominantly 3D-printing techniques. A voluntary opening control system utilizing an armband-based sEMG has been developed concurrently. Grasp tests have resulted in an average effectiveness of 87\%, with objects in excess of 400g being securely grasped. The results highlight the effectiveness of soft grippers as an end device in prosthetics, as well as the viability of toddler scale myoelectric devices.}
}

@article{lincoln33420,
          volume = {50},
          number = {4},
           month = {April},
          author = {Hongxin Wang and Jigen Peng and Shigang Yue},
            note = {The final published version of this article can be accessed online at https://ieeexplore.ieee.org/document/8485659},
           title = {A Directionally Selective Small Target Motion Detecting Visual Neural Network in Cluttered Backgrounds},
       publisher = {IEEE},
            year = {2020},
         journal = {IEEE Transactions on Cybernetics},
             doi = {10.1109/TCYB.2018.2869384},
           pages = {1541--1555},
        keywords = {ARRAY(0x55988cf211f0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/33420/},
        abstract = {Discriminating targets moving against a cluttered background is a huge challenge, let alone detecting a target as small as one or a few pixels and tracking it in flight. In the insect's visual system, a class of specific neurons, called small target motion detectors (STMDs), have been identified as showing exquisite selectivity for small target motion. Some of the STMDs have also demonstrated direction selectivity which means these STMDs respond strongly only to their preferred motion direction. Direction selectivity is an important property of these STMD neurons which could contribute to tracking small targets such as mates in flight. However, little has been done on systematically modeling these directionally selective STMD neurons. In this paper, we propose a directionally selective STMD-based neural network for small target detection in a cluttered background. In the proposed neural network, a new correlation mechanism is introduced for direction selectivity via correlating signals relayed from two pixels. Then, a lateral inhibition mechanism is implemented on the spatial field for size selectivity of the STMD neurons. Finally, a population vector algorithm is used to encode motion direction of small targets. Extensive experiments showed that the proposed neural network not only is in accord with current biological findings, i.e., showing directional preferences, but also worked reliably in detecting small targets against cluttered backgrounds.}
}

@inproceedings{lincoln47565,
           month = {April},
          author = {Mohammed Al-Khafajiy and Shatha Ghareeb and Rawaa Al-Jumeily and Rusul Almurshedi and Aseel Hussien and Thar Baker and Yaser Jararweh},
       booktitle = {2019 12th International Conference on Developments in eSystems Engineering (DeSE)},
           title = {A Holistic Study on Emerging IoT Networking Paradigms},
       publisher = {IEEE},
             doi = {10.1109/DeSE.2019.00175},
           pages = {943--949},
            year = {2020},
        keywords = {ARRAY(0x55988cf21220)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/47565/},
        abstract = {With the emerge of Internet of Things, billions of devices and humans are connected directly or indirectly to the internet. This significant growth in the number of connected devices rises the needs for a new development for the current network paradigm (e.g., cloud computing). The new network paradigm, such as fog computing, along with its related edge computing paradigms, are seen as promising solutions for handling the large volume of securely-critical and delay-sensitive data that is being produced by the IoT nodes. In this paper, we give a brief overview on the IoT related computing paradigms, including their similarities and differences as well as challenges. Next, we provide a summary of the challenges and processing and storage capabilities of each network paradigm.}
}

@incollection{lincoln47572,
           month = {April},
          author = {Mohammed Al-Khafajiy and Thar Baker and Aseel Hussien and Alison Cotgrave},
       booktitle = {Unmanned Aerial Vehicles in Smart Cities},
           title = {UAV and Fog Computing for IoE-Based Systems: A Case Study on Environment Disasters Prediction and Recovery Plans},
       publisher = {Springer},
            year = {2020},
         journal = {UAV and Fog Computing for IoE-Based Systems: A Case Study on Environment Disasters Prediction and Recovery Plans},
             doi = {10.1007/978-3-030-38712-9\_8},
           pages = {133--152},
        keywords = {ARRAY(0x55988cf21250)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/47572/},
        abstract = {In the past few years, an exponential upsurge in the development and use of the Internet of Everything (IoE)-based systems has evolved. IoE-based systems bring together the power of embedded smart things (e.g., sensors and actuators), flying-things (e.g., drones), and machine learning and data processing mediums (e.g., fog and edge computing) to create intelligent and powerful networked systems. These systems benefit various aspects of our modern smart cities{--}ranging from healthcare and smart homes to smart motorways, for example, via making informed decisions. In IoE-based systems, sensors sense the surrounding environment and return data for processing: Unmanned aerial vehicles (UAVs) survey and scan areas that are difficult to reach by human beings (e.g., oceans and mountains), and machine learning algorithms are used to classify data, interpret and learn from collected data over fog and edge computing nodes. In fact, the integration of UAVs, fog computing and machine learning provides fast, cost-effective and safe deployments for many civil and military applications. While fog computing is a new network paradigm of distributed computing nodes at the edge of the network, fog extends the cloud?s capability to the edge to provide better quality of service (QoS), and it is particularly suitable for applications that have strict requirements on latency and reliability. Also, fog computing has the advantage of providing the support of mobility, location awareness, scalability and efficient integration with other systems such as cloud computing. Fog computing and UAV are an integral part of the future information and communication technologies (ICT) that are able to achieve higher functionality, optimised resources utilisation and better management to improve both quality of service (QoS) and quality of experiences (QoE). Such systems that can combine both these technologies are natural disaster prediction systems, which could use fog-based algorithms to predict and warn for upcoming disaster threats, such as floods. The fog computing algorithms use data to make decisions and predictions from both the embedded-sensors, such as environmental sensors and data from flying-things, such as data from UAV that include live images and videos.}
}

@inproceedings{lincoln42418,
       booktitle = {6th International Conference on Control, Automation and Robotics (ICCAR)},
           month = {April},
           title = {Agri-Cost-Maps ? Integration of Environmental Constraints into Navigation Systems for Agricultural Robot},
          author = {Vignesh Raja Ponnambalam and Jaime Pulido Fentanes and Gautham Das and Grzegorz Cielniak and Jon Glenn Omholt Gjevestad and Pal From},
       publisher = {IEEE},
            year = {2020},
             doi = {10.1109/ICCAR49639.2020.9108030},
        keywords = {ARRAY(0x55988cf21280)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42418/},
        abstract = {Robust navigation is a key ability for agricultural robots. Such robots must operate safely minimizing their impact on the soil and avoiding crop damage. This paper proposes a method for unified incorporation of the application-specific constraints into the navigation system of robots deployed in different agricultural environments. The constraints are incorporated as an additional cost-map layer into the ROS navigation stack. These so-called Agri-Cost-Maps facilitate the transition from the tailored navigation systems typical for the current generation of agricultural robots to a more flexible ROS-based navigation framework that can be easily deployed for different agricultural applications. We demonstrate the applicability of this framework in three different agricultural scenarios, evaluate its benefits in simulation and demonstrate its validity in a real-world setting.}
}

@inproceedings{lincoln42458,
       booktitle = {6th International Conference on Control, Automation and Robotics (ICCAR)},
           month = {April},
           title = {Agri-Cost-Maps - Integration of Environmental Constraints into Navigation Systems for Agricultural Robots},
          author = {Vignesh Raja Ponnambalam and Jaime Pulido Fentanes and Gautham Das and Grzegorz Cielniak and Jon Glenn Omholt Gjevestad and P{\r a}l Johan From},
       publisher = {IEEE},
            year = {2020},
             doi = {10.1109/ICCAR49639.2020.9108030},
        keywords = {ARRAY(0x55988cf212b0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42458/},
        abstract = {Robust navigation is a key ability for agricultural robots. Such robots must operate safely minimizing their impact on the soil and avoiding crop damage. This paper proposes a method for unified incorporation of the application-specific constraints into the navigation system of robots deployed in different agricultural environments. The constraints are incorporated as an additional cost-map layer into the ROS navigation stack. These so-called Agri-Cost-Maps facilitate the transition from the tailored navigation systems typical for the current generation of agricultural robots to a more flexible ROS-based navigation framework that can be easily deployed for different agricultural applications. We demonstrate the applicability of this framework in three different agricultural scenarios, evaluate its benefits in simulation and demonstrate its validity in a real-world setting.}
}

@inproceedings{lincoln46369,
           month = {April},
          author = {Pratik Somaiya and Marc Hanheide and Grzegorz Cielniak},
       booktitle = {UKRAS20 Conference: ?Robots into the real world?},
           title = {Unsupervised Anomaly Detection for Safe Robot Operations},
       publisher = {UKRAS},
             doi = {10.31256/Wg7Ap8J},
           pages = {154--156},
            year = {2020},
        keywords = {ARRAY(0x55988cf212e0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/46369/},
        abstract = {Faults in robot operations are risky, particularly when robots are operating in the same environment as humans. Early detection of such faults is necessary to prevent further escalation and endangering human life. However, due to sensor noise and unforeseen faults in robots, creating a model for fault prediction is difficult. Existing supervised data-driven approaches rely on large amounts of labelled data for detecting anomalies, which is impractical in real applications. In this paper, we present an unsupervised machine learning approach for this purpose, which requires only data corresponding to the normal operation of the robot. We demonstrate how to fuse multi-modal information from robot motion sensors and evaluate the proposed framework in multiple scenarios collected from a real mobile robot.}
}

@article{lincoln41285,
          volume = {5},
          number = {2},
           month = {April},
          author = {Tommaso Pardi and Valerio Ortenzi and Colin Fairbairn and Tony Pipe and Amir Ghalamzan Esfahani and Rustam Stolkin},
           title = {Planning maximum-manipulability cutting paths},
       publisher = {IEEE},
            year = {2020},
         journal = {IEEE Robotics and Automation Letters},
             doi = {10.1109/LRA.2020.2970949},
           pages = {1999--2006},
        keywords = {ARRAY(0x55988cf21310)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/41285/},
        abstract = {This paper presents a method for constrained motion planning from vision, which enables a robot to move its end-effector over an observed surface, given start and destination points. The robot has no prior knowledge of the surface shape but observes it from a noisy point cloud. We consider the multi-objective optimisation problem of finding robot trajectories which maximise the robot?s manipulability throughout the motion, while also minimising surface-distance travelled between the two points. This work has application in industrial problems of rough robotic cutting, e.g., demolition of the legacy nuclear plant, where the cut path needs not be precise as long as it achieves dismantling. We show how detours in the path can be leveraged to increase the manipulability of the robot at all points along the path. This helps to avoid singularities while maximising the robot?s capability to make small deviations during task execution. We show how a sampling-based planner can be projected onto the Riemannian manifold of a curved surface, and extended to include a term which maximises manipulability. We present the results of empirical experiments, with both simulated and real robots, which are tasked with moving over a variety of different surface shapes. Our planner enables successful task completion while ensuring significantly greater manipulability when compared against a conventional RRT* planner.}
}

@article{lincoln40529,
          volume = {2},
          number = {12},
           month = {April},
          author = {Wayne Martindale and Simon Pearson and Mark Swainson and Lilian Korir and Isobel Wright and Arnold M. Opiyo and Benard Karanja and Samuel Nyalala and Mahesh Kumar},
           title = {Framing food security and food loss statistics for incisive supply chain improvement and knowledge transfer between Kenyan, Indian and United Kingdom food manufacturers},
       publisher = {Emerald},
            year = {2020},
         journal = {Emerald Open Research},
             doi = {10.35241/emeraldopenres.13414.1},
        keywords = {ARRAY(0x55988cf21340)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/40529/},
        abstract = {The application of global indices of nutrition and food sustainability in public health and the improvement of product profiles has facilitated effective actions that increase food security. In the research reported here we develop index measurements further so that they can be applied to food categories and be used by food processors and manufacturers for specific food supply chains. This research considers how they can be used to assess the sustainability of supply chain operations by stimulating more incisive food loss and waste reduction planning. The research demonstrates how an index driven approach focussed on improving both nutritional delivery and reducing food waste will result in improved food security and sustainability. Nutritional improvements are focussed on protein supply and reduction of food waste on supply chain losses and the methods are tested using the food systems of Kenya and India where the current research is being deployed. Innovative practices will emerge when nutritional improvement and waste reduction actions demonstrate market success, and this will result in the co-development of food manufacturing infrastructure and innovation programmes. The use of established indices of sustainability and security enable comparisons that encourage knowledge transfer and the establishment of cross-functional indices that quantify national food nutrition, security and sustainability. The research presented in this initial study is focussed on applying these indices to specific food supply chains for food processors and manufacturers.}
}

@inproceedings{lincoln44710,
          volume = {34},
          number = {10},
           month = {April},
          author = {Helen Harman and Pieter Simoens},
       booktitle = {The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)},
           title = {Action Graphs for Goal Recognition Problems with Inaccurate Initial States (Student Abstract)},
       publisher = {PKP Publishing Services Network},
            year = {2020},
         journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
             doi = {10.1609/aaai.v34i10.7174},
           pages = {13805--13806},
        keywords = {ARRAY(0x55988cf21370)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/44710/},
        abstract = {Goal recognisers attempt to infer an agent's intentions from a sequence of observations. Approaches that adapt classical planning techniques to goal recognition have previously been proposed but, generally, they assume the initial world state is accurately defined. In this paper, a state is inaccurate if any fluent's value is unknown or incorrect. To cope with this, a cyclic Action Graph, which models the order constraints between actions, is traversed to label each node with their distance from each hypothesis goal. These distances are used to calculate the posterior goal probabilities. Our experimental results, for 15 different domains, demonstrate that our approach is unaffected by an inaccurately defined initial state.}
}

@article{lincoln35778,
          volume = {98},
          number = {1},
           month = {April},
          author = {Serhan Cosar and Nicola Bellotto},
           title = {Human Re-Identification with a Robot Thermal Camera using Entropy-based Sampling},
       publisher = {Springer},
            year = {2020},
         journal = {Journal of Intelligent and Robotic Systems},
             doi = {10.1007/s10846-019-01026-w},
           pages = {85--102},
        keywords = {ARRAY(0x55988cf213a0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/35778/},
        abstract = {Human re-identification is an important feature of domestic service robots, in particular for elderly monitoring and assistance, because it allows them to perform personalized tasks and human-robot interactions. However vision-based re-identification systems are subject to limitations due to human pose and poor lighting conditions. This paper presents a new re-identification method for service robots using thermal images. In robotic applications, as the number and size of thermal datasets is limited, it is hard to use approaches that require huge amount of training samples. We propose a re-identification system that can work using only a small amount of data. During training, we perform entropy-based sampling to obtain a thermal dictionary for each person. Then, a symbolic representation is produced by converting each video into sequences of dictionary elements. Finally, we train a classifier using this symbolic representation and geometric distribution within the new representation domain. The experiments are performed on a new thermal dataset for human re-identification, which includes various situations of human motion, poses and occlusion, and which is made publicly available for research purposes. The proposed approach has been tested on this dataset and its improvements over standard approaches have been demonstrated.}
}

@inproceedings{lincoln41273,
           month = {April},
          author = {Xiaodong Li and Charles Fox and Shaun Coutts},
       booktitle = {UKRAS20},
           title = {Deep learning for robotic strawberry harvesting},
       publisher = {UK-RAS},
             doi = {10.31256/Bj3Kl5B},
           pages = {80--82},
            year = {2020},
        keywords = {ARRAY(0x55988cf213d0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/41273/},
        abstract = {Abstract{--}We develop a novel machine learning based robotic
strawberry harvesting system for fruit counting, sizing/weighting,
and yield prediction.}
}

@article{lincoln47559,
          volume = {137},
           month = {March},
          author = {Mohammed Al-Khafajiy and Thar Baker and Muhammad Asim and Zehua Guo and Rajiv Ranjan and Antonella Longo and Deepak Puthal and Mark Taylor},
           title = {COMITMENT: A Fog Computing Trust Management Approach},
       publisher = {Elsevier},
            year = {2020},
         journal = {Journal of Parallel and Distributed Computing},
             doi = {10.1016/j.jpdc.2019.10.006},
           pages = {1--16},
        keywords = {ARRAY(0x55988cf21400)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/47559/},
        abstract = {As an extension of cloud computing, fog computing is considered to be relatively more secure than cloud computing due to data being transiently maintained and analyzed on local fog nodes closer to data sources. However, there exist several security and privacy concerns when fog nodes collaborate and share data to execute certain tasks. For example, offloading data to a malicious fog node can result into an unauthorized collection or manipulation of users? private data. Cryptographic-based techniques can prevent external attacks, but are not useful when fog nodes are already authenticated and part of a networks using legitimate identities. We therefore resort to trust to identify and isolate malicious fog nodes and mitigate security, respectively. In this paper, we present a fog COMputIng Trust manageMENT (COMITMENT) approach that uses quality of service and quality of protection history measures from previous direct and indirect fog node interactions for assessing and managing the trust level of the nodes within the fog computing environment. Using COMITMENT approach, we were able to reduce/identify the malicious attacks/interactions among fog nodes by approximately 66\%, while reducing the service response time by approximately 15s.}
}

@inproceedings{lincoln40509,
       booktitle = {Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
           month = {March},
           title = {Automatic Assessment and Learning of Robot Social Abilities},
          author = {Francesco Del Duchetto and Paul Baxter and Marc Hanheide},
            year = {2020},
           pages = {561--563},
             doi = {10.1145/3371382.3377430},
        keywords = {ARRAY(0x55988cf21430)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/40509/},
        abstract = {One of the key challenges of current state-of-the-art robotic deployments in public spaces, where the robot is supposed to interact with humans, is the generation of behaviors that are engaging for the users. Eliciting engagement during an interaction, and maintaining it after the initial phase of the interaction, is still an issue to be overcome. There is evidence that engagement in learning activities is higher in the presence of a robot, particularly if novel [1], but after the initial engagement state, long and non-interactive behaviors are detrimental to the continued engagement of the users [5, 16]. Overcoming this limitation requires to design robots with enhanced social abilities that go past monolithic behaviours and introduces in-situ learning and adaptation to the specific users and situations. To do so, the robot must have the ability to perceive the state of the humans participating in the interaction and use this feedback for the selection of its own actions over time [27].}
}

@inproceedings{lincoln47564,
           month = {March},
          author = {Mohammed Al-Khafajiy and Thar Baker and Atif Waraich and Omar Alfandi and Aseel Hussien},
       booktitle = {2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA)},
           title = {Enabling High Performance Fog Computing through Fog-2-Fog Coordination Model},
       publisher = {IEEE},
             doi = {10.1109/AICCSA47632.2019.9035353},
           pages = {1--6},
            year = {2020},
        keywords = {ARRAY(0x55988cf21460)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/47564/},
        abstract = {Fog computing is a promising network paradigm in the IoT area as it has a great potential to reduce processing time for time-sensitive IoT applications. However, fog can get congested very easily due to fog resources limitations in term of capacity and computational power. In this paper, we tackle the issue of fog congestion through a request offloading algorithm. The result shows that the performance of fogs nodes can be increased be sharing fog's overload over several fog nodes. The proposed offloading algorithm could have the potential to achieve a sustainable network paradigm and highlights the significant benefits of fog offloading for the future networking paradigm.}
}

@article{lincoln44711,
          volume = {12},
          number = {2},
           month = {March},
          author = {Helen Harman and Pieter Simoens},
           title = {Action graphs for proactive robot assistance in smart environments},
       publisher = {IOS Press},
            year = {2020},
         journal = {Journal of Ambient Intelligence and Smart Environments},
             doi = {10.3233/AIS-200556},
           pages = {79--99},
        keywords = {ARRAY(0x55988cf21490)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/44711/},
        abstract = {Smart environments can already observe the actions of a human through pervasive sensors. Based on these observations, our work aims to predict the actions a human is likely to perform next. Predictions can enable a robot to proactively assist humans by autonomously executing an action on their behalf. In this paper, Action Graphs are introduced to model the order constraints between actions. Action Graphs are derived from a problem defined in Planning Domain Definition Language (PDDL). When an action is observed, the node values are updated and next actions predicted. Subsequently, a robot executes one of the predicted actions if it does not impact the flow of the human by obstructing or delaying them. Our Action Graph approach is applied to a kitchen domain.}
}

@article{lincoln41223,
          volume = {16},
           month = {March},
          author = {Junfeng Gao and Andrew French and Michael Pound and Yong He and Tony Pridmore and Jan Pieters},
           title = {Deep convolutional neural networks for image-based Convolvulus sepium detection in sugar beet fields},
       publisher = {BMC},
            year = {2020},
         journal = {Plant Methods},
             doi = {10.1186/s13007-020-00570-z},
           pages = {19},
        keywords = {ARRAY(0x55988cf214c0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/41223/},
        abstract = {Background
Convolvulus sepium (hedge bindweed) detection in sugar beet fields remains a challenging problem due to variation in appearance of plants, illumination changes, foliage occlusions, and different growth stages under field conditions. Current approaches for weed and crop recognition, segmentation and detection rely predominantly on conventional machine-learning techniques that require a large set of hand-crafted features for modelling. These might fail to generalize over different fields and environments.

Results
Here, we present an approach that develops a deep convolutional neural network (CNN) based on the tiny YOLOv3 architecture for C. sepium and sugar beet detection. We generated 2271 synthetic images, before combining these images with 452 field images to train the developed model. YOLO anchor box sizes were calculated from the training dataset using a k-means clustering approach. The resulting model was tested on 100 field images, showing that the combination of synthetic and original field images to train the developed model could improve the mean average precision (mAP) metric from 0.751 to 0.829 compared to using collected field images alone. We also compared the performance of the developed model with the YOLOv3 and Tiny YOLO models. The developed model achieved a better trade-off between accuracy and speed. Specifically, the average precisions (APs@IoU0.5) of C. sepium and sugar beet were 0.761 and 0.897 respectively with 6.48 ms inference time per image (800 {$\times$} 1200) on a NVIDIA Titan X GPU environment.}
}

@article{lincoln36114,
          volume = {31},
          number = {3},
           month = {March},
          author = {Hongxin Wang and Jigen Peng and Xuqiang Zheng and Shigang Yue},
           title = {A Robust Visual System for Small Target Motion Detection Against Cluttered Moving Backgrounds},
       publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
            year = {2020},
         journal = {IEEE Transactions on Neural Networks and Learning Systems},
             doi = {10.1109/TNNLS.2019.2910418},
           pages = {839--853},
        keywords = {ARRAY(0x55988cf214f0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/36114/},
        abstract = {Monitoring small objects against cluttered moving backgrounds is a huge challenge to future robotic vision systems. As a source of inspiration, insects are quite apt at searching for mates and tracking prey, which always appear as small dim speckles in the visual field. The exquisite sensitivity of insects for small target motion, as revealed recently, is coming from a class of specific neurons called small target motion detectors (STMDs). Although a few STMD-based models have been proposed, these existing models only use motion information for small target detection and cannot discriminate small targets from small-target-like background features (named fake features). To address this problem, this paper proposes a novel visual system model (STMD+) for small target motion detection, which is composed of four subsystems--ommatidia, motion pathway, contrast pathway, and mushroom body. Compared with the existing STMD-based models, the additional contrast pathway extracts directional contrast from luminance signals to eliminate false positive background motion. The directional contrast and the extracted motion information by the motion pathway are integrated into the mushroom body for small target discrimination. Extensive experiments showed the significant and consistent improvements of the proposed visual system model over the existing STMD-based models against fake features.}
}

@inproceedings{lincoln40456,
       booktitle = {The 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
           month = {February},
           title = {Evaluation of 3D Vision Systems for Detection of Small Objects in Agricultural Environments},
          author = {Justin Le Louedec and Bo Li and Grzegorz Cielniak},
       publisher = {SciTePress},
            year = {2020},
             doi = {10.5220/0009182806820689},
        keywords = {ARRAY(0x55988cf21520)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/40456/},
        abstract = {3D information provides unique information about shape, localisation and relations between objects, not found
in standard 2D images. This information would be very beneficial in a large number of applications in agriculture such as fruit picking, yield monitoring, forecasting and phenotyping. In this paper, we conducted a
study on the application of modern 3D sensing technology together with the state-of-the-art machine learning
algorithms for segmentation and detection of strawberries growing in real farms. We evaluate the performance
of two state-of-the-art 3D sensing technologies and showcase the differences between 2D and 3D networks
trained on the images and point clouds of strawberry plants and fruit. Our study highlights limitations of the
current 3D vision systems for the detection of small objects in outdoor applications and sets out foundations for
future work on 3D perception for challenging outdoor applications such as agriculture.}
}

@article{lincoln40216,
          volume = {9},
          number = {1},
           month = {February},
          author = {Riccardo Polvara and Massimiliano Patacchiola and Marc Hanheide and Gerhard Neumann},
           title = {Sim-to-Real Quadrotor Landing via Sequential Deep Q-Networks and Domain Randomization},
       publisher = {MDPI},
            year = {2020},
         journal = {Robotics},
             doi = {doi:10.3390/robotics9010008},
        keywords = {ARRAY(0x55988cf21550)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/40216/},
        abstract = {The autonomous landing of an Unmanned Aerial Vehicle (UAV) on a marker is one of the most challenging problems in robotics. Many solutions have been proposed, with the best results achieved via customized geometric features and external sensors. This paper discusses for the first time the use of deep reinforcement learning as an end-to-end learning paradigm to find a policy for UAVs autonomous landing. Our method is based on a divide-and-conquer paradigm that splits a task into sequential sub-tasks, each one assigned to a Deep Q-Network (DQN), hence the name Sequential Deep Q-Network (SDQN). Each DQN in an SDQN is activated by an internal trigger, and it represents a component of a high-level control policy, which can navigate the UAV towards the marker. Different technical solutions have been implemented, for example combining vanilla and double DQNs, and the introduction of a partitioned buffer replay to address the problem of sample efficiency. One of the main contributions of this work consists in showing how an SDQN trained in a simulator via domain randomization, can effectively generalize to real-world scenarios of increasing complexity. The performance of SDQNs is comparable with a state-of-the-art algorithm and human pilots while being quantitatively better in noisy conditions.}
}

@inproceedings{lincoln42101,
           month = {February},
          author = {Raymond Kirk and Michael Mangan and Grzegorz Cielniak},
       booktitle = {UKRAS20 Conference: ?Robots into the real world? Proceedings},
           title = {Feasibility Study of In-Field Phenotypic Trait Extraction for Robotic Soft-Fruit Operations},
       publisher = {UKRAS},
             doi = {doi:10.31256/Uk4Td6I},
           pages = {21--23},
            year = {2020},
        keywords = {ARRAY(0x55988cf21580)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42101/},
        abstract = {There are many agricultural applications that would benefit from robotic monitoring of soft-fruit, examples include harvesting and yield forecasting. Autonomous mobile robotic platforms enable digitisation of horticultural processes in-field reducing labour demand and increasing efficiency through con- tinuous operation. It is critical for vision-based fruit detection methods to estimate traits such as size, mass and volume for quality assessment, maturity estimation and yield forecasting. Estimating these traits from a camera mounted on a mobile robot is a non-destructive/invasive approach to gathering qualitative fruit data in-field. We investigate the feasibility of using vision- based modalities for precise, cheap, and real time computation of phenotypic traits: mass and volume of strawberries from planar RGB slices and optionally point data. Our best method achieves a marginal error of 3.00cm3 for volume estimation. The planar RGB slices can be computed manually or by using common object detection methods such as Mask R-CNN.}
}

@article{lincoln40108,
          volume = {11},
          number = {81},
           month = {February},
          author = {M Bartlett and C Costescu and Paul Baxter and S Thill},
           title = {Requirements for Robotic Interpretation of Social Signals ?in the Wild?: Insights from Diagnostic Criteria of Autism Spectrum Disorder},
       publisher = {MDPI},
            year = {2020},
         journal = {MDPI Information},
             doi = {10.3390/info11020081},
           pages = {1--20},
        keywords = {ARRAY(0x55988cf215b0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/40108/},
        abstract = {The last few decades have seen widespread advances in technological means to characterise
observable aspects of human behaviour such as gaze or posture. Among others, these developments
have also led to significant advances in social robotics. At the same time, however, social robots
are still largely evaluated in idealised or laboratory conditions, and it remains unclear whether
the technological progress is sufficient to let such robots move ?into the wild?. In this paper, we
characterise the problems that a social robot in the real world may face, and review the technological
state of the art in terms of addressing these. We do this by considering what it would entail
to automate the diagnosis of Autism Spectrum Disorder (ASD). Just as for social robotics, ASD
diagnosis fundamentally requires the ability to characterise human behaviour from observable
aspects. However, therapists provide clear criteria regarding what to look for. As such, ASD diagnosis
is a situation that is both relevant to real-world social robotics and comes with clear metrics. Overall,
we demonstrate that even with relatively clear therapist-provided criteria and current technological
progress, the need to interpret covert behaviour cannot yet be fully addressed. Our discussions have
clear implications for ASD diagnosis, but also for social robotics more generally. For ASD diagnosis,
we provide a classification of criteria based on whether or not they depend on covert information
and highlight present-day possibilities for supporting therapists in diagnosis through technological
means. For social robotics, we highlight the fundamental role of covert behaviour, show that the
current state-of-the-art is unable to characterise this, and emphasise that future research should tackle
this explicitly in realistic settings.}
}

@article{lincoln39575,
          volume = {280},
          number = {3},
           month = {February},
          author = {Bowei Chen and Jingmin Huang and Yufei Huang and Stefanos Kollias and Shigang Yue},
           title = {Combining  guaranteed and spot markets in display advertising: Selling guaranteed page views with stochastic demand},
       publisher = {Elsevier},
            year = {2020},
         journal = {European Journal of Operational Research},
             doi = {10.1016/j.ejor.2019.07.067},
           pages = {1144--1159},
        keywords = {ARRAY(0x55988cf215e0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/39575/},
        abstract = {While page views are often sold instantly through real-time auctions when users visit Web pages, they can also be sold in advance via guaranteed contracts. In this paper, we combine guaranteed and spot markets in display advertising, and present a dynamic programming model to study how a media seller should optimally allocate and price page
views between guaranteed contracts and advertising auctions. This optimisation problem is challenging because the allocation and pricing of guaranteed contracts endogenously affects the expected revenue from advertising auctions in the future. We take into consideration several distinct characteristics regarding the media buyers? purchasing behaviour, such as risk aversion, stochastic demand arrivals, and devise a scalable and efficient algorithm to solve the optimisation problem. Our work is one of a few studies that investigate the auction-based posted price guaranteed contracts for display advertising. The proposed model is further empirically validated with a display advertising data set from a UK supply-side platform. The results show that the optimal pricing and allocation strategies from our model can significantly increase the media seller?s expected total revenue, and the model suggests different optimal strategies based on the level of competition in advertising auctions.}
}

@article{lincoln37350,
          volume = {37},
          number = {1},
           month = {January},
          author = {Jaime Pulido Fentanes and Amir Badiee and Tom Duckett and Jonathan Evans and Simon Pearson and Grzegorz Cielniak},
           title = {Kriging?based robotic exploration for soil moisture mapping using a cosmic?ray sensor},
       publisher = {Wiley Periodicals, Inc.},
            year = {2020},
         journal = {Journal of Field Robotics},
             doi = {10.1002/rob.21914},
           pages = {122--136},
        keywords = {ARRAY(0x55988cf21610)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/37350/},
        abstract = {Soil moisture monitoring is a fundamental process to enhance agricultural outcomes and to protect the environment. The traditional methods for measuring moisture content in the soil are laborious and expensive, and therefore there is a growing interest in developing sensors and technologies which can reduce the effort and costs. In this work, we propose to use an autonomous mobile robot equipped with a state?of?the?art noncontact soil moisture sensor building moisture maps on the fly and automatically selecting the most optimal sampling locations. We introduce an autonomous exploration strategy driven by the quality of the soil moisture model indicating areas of the field where the information is less precise. The sensor model follows the Poisson distribution and we demonstrate how to integrate such measurements into the kriging framework. We also investigate a range of different exploration strategies and assess their usefulness through a set of evaluation experiments based on real soil moisture data collected from two different fields. We demonstrate the benefits of using the adaptive measurement interval and adaptive sampling strategies for building better quality soil moisture models. The presented method is general and can be applied to other scenarios where the measured phenomena directly affect the acquisition time and need to be spatially mapped.}
}

@inproceedings{lincoln46145,
       booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
           month = {January},
           title = {Design, Modeling and Testing of a Flagellum-inspired Soft Underwater Propeller Exploiting Passive Elasticity},
          author = {Marcello Calisti and Francesco Giorgio-Serchi and Cesare Stefanini and Madiha Farman and Irfan Hussain and Costanza Armanini and Dongming Gan and Lakmal Seneviratne and Federico Renda},
            year = {2020},
           pages = {3328--3334},
             doi = {10.1109/IROS40897.2019.8967700},
             url = {https://eprints.lincoln.ac.uk/id/eprint/46145/},
        abstract = {Flagellated micro-organism are regarded as excellent swimmers within their size scales. This, along with the simplicity of their actuation and the richness of their dynamics makes them a valuable source of inspiration to design continuum, self-propelled underwater robots. Here we introduce a soft, flagellum-inspired system which exploits the compliance of its own body to passively attain a range of geometrical configurations from the interaction with the surrounding fluid. The spontaneous formation of stable helical waves along the length of the flagellum is responsible for the generation of positive net thrust. We investigate the relationship between actuation frequency and material elasticity in determining the steady-state configuration of the system and its thrust output. This is ultimately used to perform a parameter identification procedure of an elastodynamic model aimed at investigating the scaling laws in the propulsion of flagellated robots.}
}

@article{lincoln39125,
          volume = {10},
           month = {January},
          author = {Piotr Chudzik and Arthur Mitchell and Mohammad Alkaseem and Yingie Wu and Shibo Fang and Taghread Hudaib and Simon Pearson and Bashir Al-Diri},
           title = {Mobile Real-Time Grasshopper Detection and Data Aggregation Framework},
       publisher = {Springer},
            year = {2020},
         journal = {Scientific Reports},
             doi = {10.1038/s41598-020-57674-8},
           pages = {1150},
        keywords = {ARRAY(0x55988cf21670)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/39125/},
        abstract = {nsects of the family Orthoptera: Acrididae including grasshoppers and locust devastate crops and eco-systems around the globe. The effective control of these insects requires large numbers of trained extension agents who try to spot concentrations of the insects on the ground so that they can be destroyed before they take flight. This is a challenging and difficult task. No automatic detection system is yet available to increase scouting productivity, data scale and fidelity. Here we demonstrate MAESTRO, a novel grasshopper detection framework that deploys deep learning within RBG images
to detect insects. MAeStRo uses a state-of-the-art two-stage training deep learning approach. the framework can be deployed not only on desktop computers but also on edge devices without internet connection such as smartphones. MAeStRo can gather data using cloud storage for further research and in-depth analysis. In addition, we provide a challenging new open dataset (GHCID) of highly variable grasshopper populations imaged in inner Mongolia. the detection performance of the stationary method and the mobile App are 78 and 49 percent respectively; the stationary method requires around 1000 ms to analyze a single image, whereas the mobile app uses only around 400 ms per image. The algorithms are purely data-driven and can be used for other detection tasks in agriculture (e.g. plant disease detection) and beyond. This system can play a crucial role in the collection and analysis of data to enable more effective control of this critical global pest.}
}

@misc{lincoln40031,
           month = {January},
           title = {Use and citation of paper "Empirical game theory of pedestrian interaction for autonomous vehicles" by the Royal Society's "Digital technologies and human transformations" policy workshop.},
          author = {Royal Society Royal Society and Charles Fox},
            year = {2020},
         journal = {Digital technologies and human transformations},
        keywords = {ARRAY(0x55988cf216a0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/40031/},
        abstract = {The use of tools has always conditioned and been conditioned by humans and their societies. From stone-age implements, to writing and printing, to the mechanisation of manufacturing, technology design and use have always been entwined with the evolution of human capabilities. Digital technologies are transforming human experiences, and significant questions about how individuals interact with digital technologies, and how
these technologies mediate interactions between people, follow. To explore the implications of this wave of technological change, the Royal Society convened a series of workshops in 2019.}
}

@article{lincoln39423,
          volume = {20},
          number = {1},
           month = {January},
          author = {Raymond Kirk and Grzegorz Cielniak and Michael Mangan},
           title = {L*a*b*Fruits: A Rapid and Robust Outdoor Fruit Detection System Combining Bio-Inspired Features with One-Stage Deep Learning Networks},
       publisher = {MDPI},
            year = {2020},
         journal = {Sensors},
             doi = {10.3390/s20010275},
           pages = {275},
        keywords = {ARRAY(0x55988cf216d0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/39423/},
        abstract = {Automation of agricultural processes requires systems that can accurately detect and classify produce in real industrial environments that include variation in fruit appearance due to illumination, occlusion, seasons, weather conditions, etc. In this paper, we combine a visual processing approach inspired by colour-opponent theory in humans with recent advancements in one-stage deep learning networks to accurately, rapidly and robustly detect ripe soft fruits (strawberries) in real industrial settings and using standard (RGB) camera input. The resultant system was tested on an existent data-set captured in controlled conditions as well our new real-world data-set captured on a real strawberry farm over two months. We utilise F1 score, the harmonic mean of precision and recall, to show our system matches the state-of-the-art detection accuracy ( F1: 0.793 vs. 0.799) in controlled conditions; has greater generalisation and robustness to variation of spatial parameters (camera viewpoint) in the real-world data-set ( F1: 0.744); and at a fraction of the computational cost allowing classification at almost 30fps. We propose that the L*a*b*Fruits system addresses some of the most pressing limitations of current fruit detection systems and is well-suited to application in areas such as yield forecasting and harvesting. Beyond the target application in agriculture, this work also provides a proof-of-principle whereby increased performance is achieved through analysis of the domain data, capturing features at the input level rather than simply increasing model complexity.}
}

@article{lincoln35535,
          volume = {37},
          number = {1},
           month = {January},
          author = {Petra Bosilj and Erchan Aptoula and Tom Duckett and Grzegorz Cielniak},
           title = {Transfer learning between crop types for semantic segmentation of crops versus weeds in precision agriculture},
       publisher = {Wiley},
            year = {2020},
         journal = {Journal of Field Robotics},
             doi = {10.1002/rob.21869},
           pages = {7--19},
        keywords = {ARRAY(0x55988cf21700)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/35535/},
        abstract = {Agricultural robots rely on semantic segmentation for distinguishing between crops and weeds in order to perform selective treatments, increase yield and crop health while reducing the amount of chemicals used. Deep learning approaches have recently achieved both excellent classification performance and real-time execution. However, these techniques also rely on a large amount of training data, requiring a substantial labelling effort, both of which are scarce in precision agriculture. Additional design efforts are required to achieve commercially viable performance levels under varying environmental conditions and crop growth stages. In this paper, we explore the role of knowledge transfer between deep-learning-based classifiers for different crop types, with the goal of reducing the retraining time and labelling efforts required for a new crop. We examine the classification performance on three datasets with different crop types and containing a variety of weeds, and compare the performance and retraining efforts required when using data labelled at pixel level with partially labelled data obtained through a less time-consuming procedure of annotating the segmentation output. We show that transfer learning between different crop types is possible, and reduces training times for up to \$80{$\backslash$}\%\$. Furthermore, we show that even when the data used for re-training is imperfectly annotated, the classification performance is within \$2{$\backslash$}\%\$ of that of networks trained with laboriously annotated pixel-precision data.}
}

@article{lincoln35151,
           month = {January},
          author = {Claudio Coppola and Serhan Cosar and Diego R. Faria and Nicola Bellotto},
           title = {Social Activity Recognition on Continuous RGB-D Video Sequences},
       publisher = {Springer},
         journal = {International Journal of Social Robotics},
             doi = {10.1007/s12369-019-00541-y},
           pages = {1--15},
            year = {2020},
        keywords = {ARRAY(0x55988cf21730)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/35151/},
        abstract = {Modern service robots are provided with one or more sensors, often including RGB-D cameras, to perceive objects and humans in the environment. This paper proposes a new system for the recognition of human social activities from a continuous stream of RGB-D data. Many of the works until now have succeeded in recognising activities from clipped videos in datasets, but for robotic applications it is important to be able to move to more realistic scenarios in which such activities are not manually selected. For this reason, it is useful to detect the time intervals when humans are performing social activities, the recognition of which can contribute to trigger human-robot interactions or to detect situations of potential danger. The main contributions of this research work include a novel system for the recognition of social activities from continuous RGB-D data, combining temporal segmentation and classification, as well as a model for learning the proximity-based priors of the social activities. A new public dataset with RGB-D videos of social and individual activities is also provided and used for evaluating the proposed solutions. The results show the good performance of the system in recognising social activities from continuous RGB-D data.}
}

@article{lincoln36535,
          volume = {44},
          number = {2},
           month = {January},
          author = {Zhi Yan and Tom Duckett and Nicola Bellotto},
           title = {Online Learning for 3D LiDAR-based Human Detection: Experimental Analysis of Point Cloud Clustering and Classification Methods},
       publisher = {Springer},
            year = {2020},
         journal = {Autonomous Robots},
             doi = {10.1007/s10514-019-09883-y},
           pages = {147--164},
        keywords = {ARRAY(0x55988cf21760)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/36535/},
        abstract = {This paper presents a system for online learning of human classifiers by mobile service robots using 3D{\texttt{\char126}}LiDAR sensors, and its experimental evaluation in a large indoor public space. The learning framework requires a minimal set of labelled samples (e.g. one or several samples) to initialise a classifier. The classifier is then retrained iteratively during operation of the robot. New training samples are generated automatically using multi-target tracking and a pair of "experts" to estimate false negatives and false positives. Both classification and tracking utilise an efficient real-time clustering algorithm for segmentation of 3D point cloud data. We also introduce a new feature to improve human classification in sparse, long-range point clouds. We provide an extensive evaluation of our the framework using a 3D LiDAR dataset of people moving in a large indoor public space, which is made available to the research community. The experiments demonstrate the influence of the system components and improved classification of humans compared to the state-of-the-art.}
}

@book{lincoln39209,
           month = {January},
           title = {Intelligent Data Mining and Fusion Systems in Agriculture},
          author = {Xanthoula Eirini Pantazi and Dimitrios Moshou and Dionysis Bochtis},
       publisher = {Elsevier},
            year = {2020},
        keywords = {ARRAY(0x55988cf21790)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/39209/},
        abstract = {ntelligent Data Mining and Fusion Systems in Agriculture presents methods of computational intelligence and data fusion that have applications in agriculture for the non-destructive testing of agricultural products and crop condition monitoring. Sections cover the combination of sensors with artificial intelligence architectures in precision agriculture, including algorithms, bio-inspired hierarchical neural maps, and novelty detection algorithms capable of detecting sudden changes in different conditions. This book offers advanced students and entry-level professionals in agricultural science and engineering, geography and geoinformation science an in-depth overview of the connection between decision-making in agricultural operations and the decision support features offered by advanced computational intelligence algorithms.}
}

@article{lincoln42876,
           title = {Space Invaders: Pedestrian Proxemic Utility Functions and Trust Zones for Autonomous Vehicle Interactions},
          author = {Fanta Camara and Charles Fox},
       publisher = {Springer},
            year = {2020},
             doi = {10.1007/s12369-020-00717-x},
         journal = {International Journal of Social Robotics},
        keywords = {ARRAY(0x55988cf217c0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/42876/},
        abstract = {Understanding pedestrian proxemic utility and trust will help autonomous vehicles to plan and control interactions with pedestrians more safely and efficiently. When pedestrians cross the road in front of human-driven vehicles, the two agents use knowledge of each other?s preferences to negotiate and to determine who will yield to the other. Autonomous vehicles will require similar understandings, but previous work has shown a need for them to be provided
in the form of continuous proxemic utility functions, which are not available from previous proxemics stud-
ies based on Hall?s discrete zones. To fill this gap, a new Bayesian method to infer continuous pedestrian
proxemic utility functions is proposed, and related to a new definition of ?physical trust requirement? (PTR)
for road-crossing scenarios. The method is validated on simulation data then its parameters are inferred empirically from two public datasets. Results show that pedestrian proxemic utility is best described by a hyperbolic function, and that trust by the pedestrian is required in a discrete ?trust zone? which emerges naturally from simple physics. The PTR concept is then shown to be capable of generating and explaining the
empirically observed zone sizes of Hall's discrete theory of proxemics.}
}

@inproceedings{lincoln41701,
       booktitle = {IEEE WCCI 2020-IJCNN regular session},
           title = {Competition between ON and OFF Neural Pathways Enhancing Collision Selectivity},
          author = {Fang Lei and Zhiping Peng and Vassilis Cutsuridis and Mei Liu and Yicheng Zhang and Shigang Yue},
            year = {2020},
             doi = {10.1109/IJCNN48605.2020.9207131},
        keywords = {ARRAY(0x55988cf217f0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/41701/},
        abstract = {The LGMD1 neuron of locusts shows strong looming-sensitive property for both light and dark objects. Although a few LGMD1 models have been proposed, they are not reliable to inhibit the translating motion under certain conditions compare to the biological LGMD1 in the locust. To address this issue, we propose a bio-plausible model to enhance the collision selectivity by inhibiting the translating motion. The proposed model contains three parts, the retina to lamina layer for receiving luminance change signals, the lamina to medulla layer for extracting motion cues via ON and OFF pathways separately, the medulla to lobula layer for eliminating translational excitation with neural competition. We tested the model by synthetic stimuli and real physical stimuli. The experimental results demonstrate that the proposed LGMD1 model has a strong preference for objects in direct collision course-it can detect looming objects in
different conditions while completely ignoring translating objects.}
}

@article{lincoln41544,
           title = {Experimental Analysis of a Spatialised Audio Interface for People with Visual Impairments},
          author = {Jacobus Lock and Iain Gilchrist and Grzegorz Cielniak and Nicola Bellotto},
       publisher = {Association for Computing Machinery},
            year = {2020},
         journal = {ACM Transactions on Accessible Computing},
        keywords = {ARRAY(0x55988cf21820)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/41544/},
        abstract = {Sound perception is a fundamental skill for many people with severe sight impairments. The research presented in this paper is part of an ongoing project with the aim to create a mobile guidance aid to help people with vision impairments find objects within an unknown indoor environment. This system requires an effective non-visual interface and uses bone-conduction headphones to transmit audio instructions to the user. It has been implemented and tested with spatialised audio cues, which convey the direction of a predefined target in 3D space. We present an in-depth evaluation of the audio interface with several experiments that involve a large number of participants, both blindfolded and with actual visual impairments, and analyse the pros and cons of our design choices. In addition to producing results comparable to the state-of-the-art, we found that Fitts?s Law (a predictive model for human movement) provides a suitable a metric that can be used to improve and refine the quality of the audio interface in future mobile navigation aids.}
}

@article{lincoln40137,
           title = {Haptic-Guided Teleoperation of a 7-DoF Collaborative Robot Arm with an Identical Twin Master},
          author = {Jayant Singh and Aravinda Ramakrishnan Srinivasan and Gerhard Neumann and Ayse Kucukyilmaz},
       publisher = {IEEE},
            year = {2020},
           pages = {1--1},
             doi = {10.1109/TOH.2020.2971485},
         journal = {IEEE Transactions on Haptics},
        keywords = {ARRAY(0x55988cf19b88)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/40137/},
        abstract = {In this study, we describe two techniques to enable haptic-guided teleoperation using 7-DoF cobot arms as master and slave devices. A shortcoming of using cobots as master-slave systems is the lack of force feedback at the master side. However, recent developments in cobot technologies have brought in affordable, flexible, and safe torque-controlled robot arms, which can be programmed to generate force feedback to mimic the operation of a haptic device. In this study, we use two Franka Emika Panda robot arms as a twin master-slave system to enable haptic-guided teleoperation. We propose a two layer mechanism to implement force feedback due to 1) object interactions in the slave workspace, and 2) virtual forces, e.g. those that can repel from static obstacles in the remote environment or provide task-related guidance forces. We present two different approaches for force rendering and conduct an experimental study to evaluate the performance and usability of these approaches in comparison to teleoperation without haptic guidance. Our results indicate that the proposed joint torque coupling method for rendering task forces improves energy requirements during haptic guided telemanipulation, providing realistic force feedback by accurately matching the slave torque readings at the master side.}
}

@article{lincoln43704,
           title = {A bioinspired angular velocity decoding neural network  model for visually guided flights},
          author = {Huatian Wang and Qinbing Fu and Hongxin Wang and Paul Baxter and Jigen Peng and Shigang Yue},
       publisher = {Elsevier},
            year = {2020},
             doi = {10.1016/j.neunet.2020.12.008},
         journal = {Neural Networks},
        keywords = {ARRAY(0x55988cf19bb8)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/43704/},
        abstract = {Efficient and robust motion perception systems are important pre-requisites for achieving visually guided flights in future micro air vehicles. As a source of inspiration, the visual neural networks of flying insects such as honeybee and Drosophila provide ideal examples on which to base artificial motion perception models. In this paper, we have used this approach to develop a novel method that solves the fundamental problem of estimating angular velocity for visually guided flights. Compared with previous models, our elementary motion detector (EMD) based model uses a separate texture estimation pathway to effectively decode angular velocity, and demonstrates considerable independence from the spatial frequency and contrast of the gratings. Using the Unity development platform the model is further tested for tunnel centering and terrain following paradigms in order to reproduce the visually guided flight behaviors of honeybees. In a series of controlled trials, the virtual bee utilizes the proposed angular velocity control schemes to accurately navigate through a patterned tunnel, maintaining a suitable distance from the  undulating textured terrain. The results are consistent with both neuron spike recordings and behavioral path recordings of real honeybees, thereby demonstrating the model?s potential for implementation in micro air  vehicles which have only visual sensors.}
}

