@article{Hanheide2021_Preface,
    author = {Fox, C. and Gao, J. and Esfahani, A.G. and Saaj, M. and Hanheide, M. and Parsons, S.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {v},
    title = {Preface},
    volume = {13054 LNAI},
    year = {2021}
}

@inproceedings{lirolem6945_Interactive_Model_Acquisition,
    abstract = {Systems that perform in real environments need to bind the internal state to externally
perceived objects, events, or complete scenes. How to learn this correspondence has been a long
standing problem in computer vision as well as artificial intelligence. Augmented Reality provides
an interesting perspective on this problem because a human user can directly relate displayed
system results to real environments. In the following we present a system that is able to bootstrap
internal models from user-system interactions. Starting from pictorial representations it learns
symbolic object labels that provide the basis for storing observed episodes. In a second step, more
complex relational information is extracted from stored episodes that enables the system to react
on specific scene contexts.},
    author = {Sven Wachsmuth and Marc Hanheide and Sebastian Wrede and Christian Bauckhage},
    booktitle = {KI 2005 Workshop on Mixed-reality as a Challenge to Image Understanding and Artificial Intelligence},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1428)},
    month = {September},
    note = {Systems that perform in real environments need to bind the internal state to externally
perceived objects, events, or complete scenes. How to learn this correspondence has been a long
standing problem in computer vision as well as artificial intelligence. Augmented Reality provides
an interesting perspective on this problem because a human user can directly relate displayed
system results to real environments. In the following we present a system that is able to bootstrap
internal models from user-system interactions. Starting from pictorial representations it learns
symbolic object labels that provide the basis for storing observed episodes. In a second step, more
complex relational information is extracted from stored episodes that enables the system to react
on specific scene contexts.},
    pages = {41--46},
    title = {From images via symbols to contexts: using augmented reality for interactive model acquisition},
    url = {http://eprints.lincoln.ac.uk/6945/},
    year = {2005}
}

@article{Hanheide2023_Continuously_Changing_Environments,
    author = {Hroob, I. and Molina, S. and Polvara, R. and Cielniak, G. and Hanheide, M.},
    journal = {Proceedings of the 11th European Conference on Mobile Robots, ECMR 2023},
    title = {Learned Long-Term Stability Scan Filtering for Robust Robot Localisation in Continuously Changing Environments},
    year = {2023}
}

@article{Hanheide2017_Hai_Chairs,
    author = {Wrede, B. and Nagai, Y. and Komatsu, T. and Hanheide, M. and Natale, L.},
    journal = {HAI 2017 - Proceedings of the 5th International Conference on Human Agent Interaction},
    pages = {iii},
    title = {HAI 2017 Chairs{'} Welcome},
    year = {2017}
}

@article{Hanheide2017_Integrating_Educational_Robotic,
    author = {Gyebi, E.B.B. and Hanheide, M. and Cielniak, G.},
    journal = {Advances in Intelligent Systems and Computing},
    pages = {73-87},
    title = {The effectiveness of integrating educational robotic activities into higher education computer science curricula: A case study in a developing country},
    volume = {560},
    year = {2017}
}

@article{Hanheide2007_Cognitive_Vision_System,
    author = {Siegl, H. and Hanheide, M. and Wrede, S. and Pinz, A.},
    journal = {Image and Vision Computing},
    number = {12},
    pages = {1895-1903},
    title = {An augmented reality human-computer interface for object localization in a cognitive vision system},
    volume = {25},
    year = {2007}
}

@article{Hanheide2010_Spatial_Attention_System,
    author = {Holthaus, P. and L{\"u}tkebohle, I. and Hanheide, M. and Wachsmuth, S.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {325-334},
    title = {Can I help you? A spatial attention system for a receptionist robot},
    volume = {6414 LNAI},
    year = {2010}
}

@article{Hanheide2008_Automatic_Initialization,
    author = {Rabie, A. and Lang, C. and Hanheide, M. and Castrill{\'o}n-Santana, M. and Sagerer, G.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {517-526},
    title = {Automatic initialization for facial analysis in interactive robotics},
    volume = {5008 LNCS},
    year = {2008}
}

@article{Hanheide2014_Longterm_Topological_Localisation,
    author = {Krajnik, T. and Fentanes, J.P. and Mozos, O.M. and Duckett, T. and Ekekrantz, J. and Hanheide, M.},
    journal = {IEEE International Conference on Intelligent Robots and Systems},
    pages = {4537-4542},
    title = {Long-term topological localisation for service robots in dynamic environments using spectral maps},
    year = {2014}
}

@article{Hanheide2024_Stable_Points_Segmentation,
    author = {Hroob, I. and Mersch, B. and Stachniss, C. and Hanheide, M.},
    journal = {IEEE Robotics and Automation Letters},
    number = {4},
    pages = {3546-3553},
    title = {Generalizable Stable Points Segmentation for 3D LiDAR Scan-to-Map Long-Term Localization},
    volume = {9},
    year = {2024}
}

@article{Hanheide2007_Coordinating_Interactive_Vision,
    author = {Wachsmuth, S. and Wrede, S. and Hanheide, M.},
    journal = {Computer Vision and Image Understanding},
    number = {1-2},
    pages = {135-149},
    title = {Coordinating interactive vision behaviors for cognitive assistance},
    volume = {108},
    year = {2007}
}

@article{Hanheide2009_Learning_By_Interacting,
    author = {Wrede, B. and Rohlfing, K.J. and Hanheide, M. and Sagerer, G.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {139-150},
    title = {Towards learning by interacting},
    volume = {5436},
    year = {2009}
}

@article{Hanheide2012_Qualitative_Trajectory_Calculus,
    author = {Hanheide, M. and Peters, A. and Bellotto, N.},
    journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
    pages = {689-694},
    title = {Analysis of human-robot spatial behaviour applying a qualitative trajectory calculus},
    year = {2012}
}

@article{Hanheide2008_Planar_Surface_Extraction,
    author = {Swadzba, A. and Vollmer, A. and Hanheide, M. and Wachsmuth, S.},
    journal = {Proceedings - International Conference on Pattern Recognition},
    title = {Reducing noise and redundancy in registered range data for planar surface extraction},
    year = {2008}
}

@inproceedings{lirolem6937_Spatial_Contextaware_Personfollowing,
    abstract = {Domestic robots are in the focus of research in
terms of service providers in households and even as robotic
companion that share the living space with humans. A major
capability of mobile domestic robots that is joint exploration
of space. One challenge to deal with this task is how could we
let the robots move in space in reasonable, socially acceptable
ways so that it will support interaction and communication
as a part of the joint exploration. As a step towards this
challenge, we have developed a context-aware following behav-
ior considering these social aspects and applied these together
with a multi-modal person-tracking method to switch between
three basic following approaches, namely direction-following,
path-following and parallel-following. These are derived from
the observation of human-human following schemes and are
activated depending on the current spatial context (e.g. free
space) and the relative position of the interacting human.
A combination of the elementary behaviors is performed in
real time with our mobile robot in different environments.
First experimental results are provided to demonstrate the
practicability of the proposed approach.},
    author = {Fang Yuan and Marc Hanheide and Gerhard Sagerer},
    booktitle = {International Workshop on Cognition for Technical Systems},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c0fd8)},
    month = {December},
    note = {Domestic robots are in the focus of research in
terms of service providers in households and even as robotic
companion that share the living space with humans. A major
capability of mobile domestic robots that is joint exploration
of space. One challenge to deal with this task is how could we
let the robots move in space in reasonable, socially acceptable
ways so that it will support interaction and communication
as a part of the joint exploration. As a step towards this
challenge, we have developed a context-aware following behav-
ior considering these social aspects and applied these together
with a multi-modal person-tracking method to switch between
three basic following approaches, namely direction-following,
path-following and parallel-following. These are derived from
the observation of human-human following schemes and are
activated depending on the current spatial context (e.g. free
space) and the relative position of the interacting human.
A combination of the elementary behaviors is performed in
real time with our mobile robot in different environments.
First experimental results are provided to demonstrate the
practicability of the proposed approach.},
    title = {Spatial context-aware person-following for a domestic robot},
    url = {http://eprints.lincoln.ac.uk/6937/},
    year = {2008}
}

@article{Hanheide2020_Robust_Robot_Navigation,
    author = {Binch, A. and Das, G.P. and Pulido Fentanes, J. and Hanheide, M.},
    journal = {Proceedings - IEEE International Conference on Robotics and Automation},
    pages = {3937-3943},
    title = {Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation},
    year = {2020}
}

@article{Hanheide2010_Probabilistic_Selfawareness_Model,
    author = {Golombek, R. and Wrede, S. and Hanheide, M. and Heckmann, M.},
    journal = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
    pages = {2745-2750},
    title = {Learning a probabilistic self-awareness model for robotic systems},
    year = {2010}
}

@article{Hanheide2011_System_For_Interactive,
    author = {Sko?aj, D. and Kristan, M. and Vre?ko, A. and Mahni?, M. and Jan{\'i}?ek, M. and Kruijff, G.-J.M. and Hanheide, M. and Hawes, N. and Keller, T. and Zillich, M. and Zhou, K.},
    journal = {IEEE International Conference on Intelligent Robots and Systems},
    pages = {3387-3394},
    title = {A system for interactive learning in dialogue with a tutor},
    year = {2011}
}

@article{Hanheide2021_Lidar_Slam_Systems,
    author = {Hroob, I. and Polvara, R. and Molina, S. and Cielniak, G. and Hanheide, M.},
    journal = {arXiv},
    title = {Benchmark of visual and 3D lidar SLAM systems in simulation environment for vineyards},
    year = {2021}
}

@article{Hanheide2011_Exploiting_Probabilistic_Knowledge,
    author = {Hanheide, M. and Gretton, C. and Dearden, R. and Hawes, N. and Wyatt, J. and Pronobis, A. and Aydemir, A. and G{\"o}belbecker, M. and Zender, H.},
    journal = {IJCAI International Joint Conference on Artificial Intelligence},
    pages = {2442-2449},
    title = {Exploiting probabilistic knowledge under uncertain sensing for efficient robot behaviour},
    year = {2011}
}

@article{Hanheide2009_Multimodal_Emotion_Recognition,
    author = {Rabie, A. and Wrede, B. and Vogt, T. and Hanheide, M.},
    journal = {2009 International Conference on Computer and Electrical Engineering, ICCEE 2009},
    pages = {598-602},
    title = {Evaluation and discussion of multi-modal emotion recognition},
    volume = {1},
    year = {2009}
}

@inproceedings{lirolem6935_Wrong_Error_Detection,
    abstract = {A matter of course for the researchers and developers of state-of-the-art technology for human-computer- or human-robot-interaction is to create not only systems that can precisely fulfill a certain task. They must provide a strong robustness against internal and external errors or user-dependent application errors. Especially when creating service robots for a variety of applications or robots for accompanying humans in everyday situations sufficient error robustness is crucial for acceptance by users. But experience unveils that operating such systems under real world conditions with unexperienced users is an extremely challenging task which still is not solved satisfactorily. In this paper we will present an approach for handling both internal errors and application errors within an integrated system capable of performing extended HRI on different robotic platforms and in unspecified surroundings like a real world apartment. Based on the gathered experience from user studies and evaluating integrated systems in the real world, we implemented several ways to generalize and handle unexpected situations. Adding such a kind of error awareness to HRI systems in cooperation with the interaction partner avoids to get stuck in an unexpected situation or state and handle mode confusion. Instead of shouldering the enormous effort to account for all possible problems, this paper proposes a more general solution and underpins this with findings from naive user studies. This enhancement is crucial for the development of a new generation of robots as despite diligent preparations might be made, no one can predict how an interaction with a robotic system will develop and which kind of environment it has to cope with.},
    author = {Thorsten P. Spexard and Marc Hanheide and Shuyin Li and Britta Wrede},
    booktitle = {ICRA Workshop on Social Interaction with Intelligent Indoor Robots (2008)},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c10f8)},
    month = {May},
    note = {A matter of course for the researchers and developers of state-of-the-art technology for human-computer- or human-robot-interaction is to create not only systems that can precisely fulfill a certain task. They must provide a strong robustness against internal and external errors or user-dependent application errors. Especially when creating service robots for a variety of applications or robots for accompanying humans in everyday situations sufficient error robustness is crucial for acceptance by users. But experience unveils that operating such systems under real world conditions with unexperienced users is an extremely challenging task which still is not solved satisfactorily. In this paper we will present an approach for handling both internal errors and application errors within an integrated system capable of performing extended HRI on different robotic platforms and in unspecified surroundings like a real world apartment. Based on the gathered experience from user studies and evaluating integrated systems in the real world, we implemented several ways to generalize and handle unexpected situations. Adding such a kind of error awareness to HRI systems in cooperation with the interaction partner avoids to get stuck in an unexpected situation or state and handle mode confusion. Instead of shouldering the enormous effort to account for all possible problems, this paper proposes a more general solution and underpins this with findings from naive user studies. This enhancement is crucial for the development of a new generation of robots as despite diligent preparations might be made, no one can predict how an interaction with a robotic system will develop and which kind of environment it has to cope with.},
    title = {Oops, something is wrong - error detection and recovery for advanced human-robot-interaction},
    url = {http://eprints.lincoln.ac.uk/6935/},
    year = {2008}
}

@article{Hanheide2023_Generalise_Vtagexploiting_Perceptual,
    author = {Cox, J. and Tsagkopoulos, N. and Rozsyp{\'a}lek, Z. and Krajn{\'i}k, T. and Sklar, E. and Hanheide, M.},
    journal = {Computers and Electronics in Agriculture},
    title = {Visual teach and generalise (VTAG)?Exploiting perceptual aliasing for scalable autonomous robotic navigation in horticultural environments},
    volume = {212},
    year = {2023}
}

@article{Hanheide2011_Robot_Companions_Challenges,
    author = {Aylett, R.S. and Castellano, G. and Raducanu, B. and Paiva, A. and Hanheide, M.},
    journal = {ICMI{'}11 - Proceedings of the 2011 ACM International Conference on Multimodal Interaction},
    pages = {323-326},
    title = {Long-term socially perceptive and interactive robot companions: Challenges and future perspectives},
    year = {2011}
}

@inproceedings{lirolem6930_Robot_Applying_Heuristics,
    abstract = {In a society that keeps getting closer in touch with social robots it is very important to include potential users throughout the design of the systems. This is an important rationale to build robots that provide services and assistance in a socially acceptable way and influence societies in a positive way. In the process, methods are needed to rate the robot interaction performance. We present a multimodal corpus of na\"ive users interacting with an autonomously operating system. It comprises data that, to our conviction, reveal a lot about human-robot interaction (HRI) in general and social acceptance, in particular. In both, the evaluation and the design process we took into account Clarkson and Arkin's heuristics for HRI (developed by adapting Nielsen's and Scholtz' heuristics to robotics) 1. We discuss exemplary results to show the use of heuristics in the design of socially acceptable robots.},
    author = {Manja Lohse and Marc Hanheide},
    booktitle = {Robots as Social Actors Workshop: International Symposium on Robot and Human Interactive Communication (RO-MAN 08)},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1068)},
    month = {August},
    note = {In a society that keeps getting closer in touch with social robots it is very important to include potential users throughout the design of the systems. This is an important rationale to build robots that provide services and assistance in a socially acceptable way and influence societies in a positive way. In the process, methods are needed to rate the robot interaction performance. We present a multimodal corpus of na\"ive users interacting with an autonomously operating system. It comprises data that, to our conviction, reveal a lot about human-robot interaction (HRI) in general and social acceptance, in particular. In both, the evaluation and the design process we took into account Clarkson and Arkin's heuristics for HRI (developed by adapting Nielsen's and Scholtz' heuristics to robotics) 1. We discuss exemplary results to show the use of heuristics in the design of socially acceptable robots.},
    pages = {1584--1589},
    title = {Evaluating a social home tour robot applying heuristics},
    url = {http://eprints.lincoln.ac.uk/6930/},
    year = {2008}
}

@article{Hanheide2014_Humanrobot_Headon_Encounters,
    author = {Dondrup, C. and Lichtenth{\"a}ler, C. and Hanheide, M.},
    journal = {ACM/IEEE International Conference on Human-Robot Interaction},
    pages = {154-155},
    title = {Hesitation signals in human-robot head-on encounters: A pilot study},
    year = {2014}
}

@article{Hanheide2022_Job_Longterm_Behavioural,
    author = {Duchetto, F.D. and Hanheide, M.},
    journal = {IEEE Robotics and Automation Letters},
    number = {3},
    pages = {6934-6941},
    title = {Learning on the Job: Long-Term Behavioural Adaptation in Human-Robot Interactions},
    volume = {7},
    year = {2022}
}

@article{Hanheide2022_Interactive_Movement_Primitives,
    author = {Mghames, S. and Hanheide, M.},
    journal = {IEEE International Conference on Automation Science and Engineering},
    pages = {493-498},
    title = {Environment-aware Interactive Movement Primitives for Object Reaching in Clutter},
    volume = {2022-August},
    year = {2022}
}

@article{Hanheide2009_Interactive_Robotic_Learning,
    author = {Peltason, J. and L{\"u}tkebohle, I. and Wrede, B. and Hanheide, M.},
    journal = {CEUR Workshop Proceedings},
    title = {Mixed-initiative in interactive robotic learning},
    volume = {693},
    year = {2009}
}

@incollection{lirolem6719_Stereo_Video_Sequences,
    abstract = {lthough mosaics are well established as a compact and non-redundant representation of image sequences, their application still suffers from restrictions of the camera motion or has to deal with parallax errors. We present an approach that allows construction of mosaics from arbitrary motion of a head-mounted camera pair. As there are no parallax errors when creating mosaics from planar objects, our approach first decomposes the scene into planar sub-scenes from stereo vision and creates a mosaic for each plane individually. The power of the presented mosaicing technique is evaluated in an office scenario, including the analysis of the parallax error.},
    author = {Nicholas Gorges and Marc Hanheide and William Christmas and Christian Bauckhage and Gerhard Sagerer and Joseph Kittler},
    booktitle = {Pattern recognition},
    editor = {Carl Edward Rasmussen and Heinrich H. Buelthoff and Bernhard Schoelkopf and Martin Giese},
    keywords = {ARRAY(0x7f43493c1518)},
    month = {December},
    note = {lthough mosaics are well established as a compact and non-redundant representation of image sequences, their application still suffers from restrictions of the camera motion or has to deal with parallax errors. We present an approach that allows construction of mosaics from arbitrary motion of a head-mounted camera pair. As there are no parallax errors when creating mosaics from planar objects, our approach first decomposes the scene into planar sub-scenes from stereo vision and creates a mosaic for each plane individually. The power of the presented mosaicing technique is evaluated in an office scenario, including the analysis of the parallax error.},
    number = {3175},
    pages = {342--349},
    publisher = {Springer},
    series = {Lecture Notes in Computer Science},
    title = {Mosaics from arbitrary stereo video sequences},
    url = {http://eprints.lincoln.ac.uk/6719/},
    year = {2004}
}

@inproceedings{lirolem6933_Moving_From_Augmented,
    abstract = {Recently1 there has been a growing interest in human
augmented mapping[1, 2]. That is: a mobile robot builds
a low level spatial representation of the environment based
on its sensor readings while a human provides labels for
human concepts, such as rooms, which are then augmented
or anchored to this representation or map [3]. Given such an
augmented map the robot has the ability to communicate with
the human about spatial concepts using the labels that the
human understand. For instance, the robot could report it is
in the ?kitchen?, instead of a set Cartesian coordinates which
are probably meaningless to the human.},
    author = {Olaf Booij and Ben Kr\"ose and Julia Peltason and Thorsten P. Spexard and Marc Hanheide},
    booktitle = {Robotics: Science and Systems Workshop on Interactive Robot Learning},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c10c8)},
    month = {June},
    note = {Recently1 there has been a growing interest in human
augmented mapping[1, 2]. That is: a mobile robot builds
a low level spatial representation of the environment based
on its sensor readings while a human provides labels for
human concepts, such as rooms, which are then augmented
or anchored to this representation or map [3]. Given such an
augmented map the robot has the ability to communicate with
the human about spatial concepts using the labels that the
human understand. For instance, the robot could report it is
in the ?kitchen?, instead of a set Cartesian coordinates which
are probably meaningless to the human.},
    title = {Moving from augmented to interactive mapping},
    url = {http://eprints.lincoln.ac.uk/6933/},
    year = {2008}
}

@article{Hanheide2010_Dynamic_Path_Planning,
    author = {Yuan, F. and Twardon, L. and Hanheide, M.},
    journal = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
    pages = {3275-3281},
    title = {Dynamic path planning adopting human navigation strategies for a domestic mobile robot},
    year = {2010}
}

@inproceedings{Onoufriou_2020_Time_Series_Forecasting,
    author = {George Onoufriou and   and Marc Hanheide and Georgios Leontidis and},
    booktitle = {{UKRAS}20 Conference: {\textquotedblleft}Robots into the real world{\textquotedblright} Proceedings},
    doi = {10.31256/qm1fu7l},
    month = {may},
    publisher = {{EPSRC} {UK}-{RAS} Network},
    title = {The Augmented Agronomist Pipeline and Time Series Forecasting},
    url = {https://doi.org/10.31256%2Fqm1fu7l},
    year = {2020}
}

@article{Hanheide2005_Vision_Systems,
    author = {Bauckhage, C. and Hanheide, M. and Wrede, S. and K{\"a}ster, T. and Pfeiffer, M. and Sagerer, G.},
    journal = {Eurasip Journal on Applied Signal Processing},
    number = {14},
    pages = {2375-2390},
    title = {Vision systems with the human in the loop},
    volume = {2005},
    year = {2005}
}

@phdthesis{lirolem6748_Konturmodelle_In_Intensitatsbildern,
    abstract = {Abstract},
    author = {Marc Hanheide},
    keywords = {ARRAY(0x7f43493c15d8)},
    month = {July},
    note = {Abstract},
    school = {Universitat Bielefeld},
    title = {Objektbezogene 3D-Erkennung automatisch generierter Konturmodelle in Intensit\"atsbildern},
    url = {http://eprints.lincoln.ac.uk/6748/},
    year = {2001}
}

@article{Hanheide2024_Learning_Manipulation_Tasks,
    author = {Arunachalam, H. and Hanheide, M. and Mghames, S.},
    journal = {arXiv},
    title = {Learning Manipulation Tasks in Dynamic and Shared 3D Spaces},
    year = {2024}
}

@inproceedings{lirolem8314_Longterm_Socially_Perceptive,
    abstract = {This paper gives a brief overview of the challenges for multi-model perception and generation applied to robot companions located in human social environments. It reviews the current position in both perception and generation and the immediate technical challenges and goes on to consider the extra issues raised by embodiment and social context. Finally, it briefly discusses the impact of systems that must function continually over months rather than just for a few hours. \^A\\copyright 2011 ACM.},
    address = {Alicante},
    author = {Ruth S. Aylett and Ginevra Castellano and Bogdan Raducanu and Ana Paiva and Marc Hanheide},
    booktitle = {Conference of 2011 ACM International Conference on Multimodal Interaction, ICMI'11},
    journal = {ICMI'11 - Proceedings of the 2011 ACM International Conference on Multimodal Interaction},
    keywords = {ARRAY(0x7f43493bfa10)},
    month = {November},
    note = {Conference Code: 87685},
    pages = {323--326},
    publisher = {ACM},
    title = {Long-term socially perceptive and interactive robot companions: challenges and future perspective},
    url = {http://eprints.lincoln.ac.uk/8314/},
    year = {2011}
}

@article{Hanheide2006_Cognitive_Vision_System,
    author = {Wrede, S. and Hanheide, M. and Wachsmuth, S. and Sagerer, G.},
    journal = {Proceedings of the Fourth IEEE International Conference on Computer Vision Systems, ICVS{'}06},
    pages = {1},
    title = {Integration and coordination in a cognitive vision system},
    volume = {2006},
    year = {2006}
}

@article{Hanheide2015_Qualitative_Trajectory_Calculus,
    author = {Dondrup, C. and Bellotto, N. and Hanheide, M. and Eder, K. and Leonards, U.},
    journal = {Robotics},
    number = {1},
    pages = {63-102},
    title = {A computational model of human-robot spatial interactions based on a qualitative trajectory calculus},
    volume = {4},
    year = {2015}
}

@article{Hanheide2023_Robot_Sensor_Data,
    author = {Castri, L. and Mghames, S. and Hanheide, M. and Bellotto, N.},
    journal = {Proceedings of Machine Learning Research},
    pages = {243-258},
    title = {Enhancing Causal Discovery from Robot Sensor Data in Dynamic Scenarios},
    volume = {213},
    year = {2023}
}

@article{Hanheide2022_Strawberry_Yield_Forecasting,
    author = {Onoufriou, G. and Hanheide, M. and Leontidis, G.},
    journal = {Sensors},
    number = {21},
    title = {EDLaaS:Fully Homomorphic Encryption over Neural Network Graphs for Vision and Private Strawberry Yield Forecasting},
    volume = {22},
    year = {2022}
}

@article{Hanheide2021_Deep_Learning_Method,
    author = {De Barrie, D. and Pandya, M. and Pandya, H. and Hanheide, M. and Elgeneidy, K.},
    journal = {Frontiers in Robotics and AI},
    title = {A Deep Learning Method for Vision Based Force Prediction of a Soft Fin Ray Gripper Using Simulation Data},
    volume = {8},
    year = {2021}
}

@article{Hanheide2023_Softfruit_Harvesting_Operations,
    author = {Guevara, L. and Hanheide, M. and Parsons, S.},
    journal = {Journal of Field Robotics},
    title = {Implementation of a human-aware robot navigation module for cooperative soft-fruit harvesting operations},
    year = {2023}
}

@inproceedings{lirolem6927_Synchrony_In_Parentchild,
    abstract = {In our approach, we aim at an objective measurement of
synchrony in multimodal tutoring behavior. The use of signal
correlation provides a well formalized method that yields
gradual information about the degree of synchrony. For our
analysis, we used and extended an algorithm proposed by
Hershey \& Movellan (2000) that correlates single-pixel
values of a video signal with the loudness of the
corresponding audio track over time. The results of all pixels are integrated over the video to achieve a scalar estimate of synchrony.},
    author = {Matthias Rolf and Marc Hanheide and Katharina J. Rohlfing},
    booktitle = {SRCD 2009 Biennial Meeting},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c0f48)},
    month = {April},
    note = {In our approach, we aim at an objective measurement of
synchrony in multimodal tutoring behavior. The use of signal
correlation provides a well formalized method that yields
gradual information about the degree of synchrony. For our
analysis, we used and extended an algorithm proposed by
Hershey \& Movellan (2000) that correlates single-pixel
values of a video signal with the loudness of the
corresponding audio track over time. The results of all pixels are integrated over the video to achieve a scalar estimate of synchrony.},
    publisher = {Society for Research in Child Development},
    title = {The use of synchrony in parent-child interaction can be measured on a signal-level},
    url = {http://eprints.lincoln.ac.uk/6927/},
    year = {2009}
}

@article{Hanheide2017_Scalable_Identity_Encoding,
    author = {Lightbody, P. and Krajn{\'i}k, T. and Hanheide, M.},
    journal = {Proceedings of the ACM Symposium on Applied Computing},
    pages = {276-282},
    title = {A versatile high-performance visual fiducial marker detection system with scalable identity encoding},
    volume = {Part F128005},
    year = {2017}
}

@article{Hanheide2023_Multiagent_Spatial_Interactions,
    author = {Mghames, S. and Castri, L. and Hanheide, M. and Bellotto, N.},
    journal = {IEEE International Workshop on Robot and Human Communication, RO-MAN},
    pages = {1170-1175},
    title = {Qualitative Prediction of Multi-Agent Spatial Interactions},
    year = {2023}
}

@article{Hanheide2020_Incorporating_Spatial_Constraints,
    author = {Khan, M.W. and Das, G.P. and Hanheide, M. and Cielniak, G.},
    journal = {IEEE International Conference on Intelligent Robots and Systems},
    pages = {2440-2445},
    title = {Incorporating spatial constraints into a bayesian tracking framework for improved localisation in agricultural environments},
    year = {2020}
}

@article{Hanheide2023_Human_Motion_Prediction,
    author = {Mghames, S. and Castri, L. and Hanheide, M. and Bellotto, N.},
    journal = {Proceedings of the International Joint Conference on Neural Networks},
    title = {A Neuro-Symbolic Approach for Enhanced Human Motion Prediction},
    volume = {2023-June},
    year = {2023}
}

@article{Hanheide2022_Tabletop_Yield_Forecasting,
    author = {Onoufriou, G. and Hanheide, M. and Leontidis, G.},
    journal = {arXiv},
    title = {Premonition Net, A Multi-Timeline Transformer Network Architecture Towards Strawberry Tabletop Yield Forecasting},
    year = {2022}
}

@article{Hanheide2020_Inverse_Dynamics_Learning,
    author = {Shaj, V. and Becker, P. and B{\"u}chler, D. and Pandya, H. and van Duijkeren, N. and Taylor, C.J. and Hanheide, M. and Neumann, G.},
    journal = {Proceedings of Machine Learning Research},
    pages = {765-781},
    title = {Action-Conditional Recurrent Kalman Networks For Forward and Inverse Dynamics Learning},
    volume = {155},
    year = {2020}
}

@article{Hanheide2017_Robot_Task_Planning,
    author = {Hanheide, M. and G{\"o}belbecker, M. and Horn, G.S. and Pronobis, A. and Sj{\"o}{\"o}, K. and Aydemir, A. and Jensfelt, P. and Gretton, C. and Dearden, R. and Janicek, M. and Zender, H. and Kruijff, G.-J. and Hawes, N. and Wyatt, J.L.},
    journal = {Artificial Intelligence},
    pages = {119-150},
    title = {Robot task planning and explanation in open and uncertain worlds},
    volume = {247},
    year = {2017}
}

@article{Hanheide2018_Safe_Humanrobot_Interaction,
    author = {Baxter, P. and Cielniak, G. and Hanheide, M. and From, P.},
    journal = {ACM/IEEE International Conference on Human-Robot Interaction},
    pages = {59-60},
    title = {Safe Human-Robot Interaction in Agriculture},
    year = {2018}
}

@article{Hanheide2004_Cognitive_Vision_System,
    author = {Bauckhage, C. and Hanheide, M. and Wrede, S. and Sagerer, G.},
    journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
    title = {A cognitive vision system for action recognition in office environments},
    volume = {2},
    year = {2004}
}

@article{Hanheide2008_Social_Robots,
    author = {Hanheide, M. and Wrede, S. and Lang, C. and Sagerer, G.},
    journal = {Proceedings - IEEE International Conference on Robotics and Automation},
    pages = {3660-3665},
    title = {Who am I talking with? A face memory for social robots},
    year = {2008}
}

@article{Hanheide2011_Home_Alone_Autonomous,
    author = {Hawes, N. and Hanheide, M. and Hargreaves, J. and Page, B. and Zender, H. and Jensfelt, P.},
    journal = {Proceedings - IEEE International Conference on Robotics and Automation},
    pages = {3907-3914},
    title = {Home alone: Autonomous extension and correction of spatial representations},
    year = {2011}
}

@article{Hanheide2008_Evaluating_Extrovert,
    author = {Lohse, M. and Hanheide, M. and Wrede, B. and Walters, M.L. and Koay, K.L. and Syrdal, D.S. and Green, A. and H{\"u}ttenrauch, H. and Dautenhahn, K. and Sagerer, G. and Severinson-Eklundh, K.},
    journal = {Proceedings of the 17th IEEE International Symposium on Robot and Human Interactive Communication, RO-MAN},
    pages = {488-493},
    title = {Evaluating extrovert and introvert behaviour of a domestic robot -a video study},
    year = {2008}
}

@article{Hanheide2018_Providing_Cognitive_Assistance,
    author = {Baxter, P. and Lightbody, P. and Hanheide, M.},
    journal = {ACM/IEEE International Conference on Human-Robot Interaction},
    pages = {57-58},
    title = {Robots Providing Cognitive Assistance in Shared Workspaces},
    year = {2018}
}

@article{Hanheide2017_Project_Longterm_Autonomy,
    author = {Hawes, N. and Burbridge, C. and Jovan, F. and Kunze, L. and Lacerda, B. and Mudrov{\'a}, L. and Young, J. and Wyatt, J. and Hebesberger, D. and K{\"o}rtner, T. and Ambrus, R. and Bore, N. and Folkesson, J. and Jensfelt, P. and Beyer, L. and Hermans, A. and Leibe, B. and Aldoma, A. and Fa{\"u}lhammer, T. and Zillich, M. and Vincze, M. and Chinellato, E. and Al-Omari, M. and Duckworth, P. and Gatsoulis, Y. and Hogg, D.C. and Cohn, A.G. and Dondrup, C. and Pulido Fentanes, J. and Krajn{\'i}k, T. and Santos, J.M. and Duckett, T. and Hanheide, M.},
    journal = {IEEE Robotics and Automation Magazine},
    number = {3},
    pages = {146-156},
    title = {The STRANDS Project: Long-Term Autonomy in Everyday Environments},
    volume = {24},
    year = {2017}
}

@inproceedings{lirolem6939_Active_Visionbased_Localization,
    abstract = {Self-Localization is a crucial task for mobile robots. It is not only a requirement
for auto navigation but also provides contextual information to support
human robot interaction (HRI). In this paper we present an active vision-based
localization method for integration in a complex robot system to work in human
interaction scenarios (e.g. home-tour) in a real world apartment. The holistic
features used are robust to illumination and structural changes in the scene. The
system uses only a single pan-tilt camera shared between different vision applications
running in parallel to reduce the number of sensors. Additional information
from other modalities (like laser scanners) can be used, profiting of an integration
into an existing system. The camera view can be actively adapted and the
evaluation showed that different rooms can be discerned.},
    author = {Falk Schubert and Thorsten P. Spexard and Marc Hanheide and Sven Wachsmuth},
    booktitle = {5th International Conference on Computer Vision Systems (ICVS 2007)},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c12d8)},
    note = {Self-Localization is a crucial task for mobile robots. It is not only a requirement
for auto navigation but also provides contextual information to support
human robot interaction (HRI). In this paper we present an active vision-based
localization method for integration in a complex robot system to work in human
interaction scenarios (e.g. home-tour) in a real world apartment. The holistic
features used are robust to illumination and structural changes in the scene. The
system uses only a single pan-tilt camera shared between different vision applications
running in parallel to reduce the number of sensors. Additional information
from other modalities (like laser scanners) can be used, profiting of an integration
into an existing system. The camera view can be actively adapted and the
evaluation showed that different rooms can be discerned.},
    publisher = {Applied Computer Science Group, Bielefeld University, Germany},
    title = {Active vision-based localization for robots in a home-tour scenario},
    url = {http://eprints.lincoln.ac.uk/6939/},
    year = {2007}
}

@article{Hanheide2007_Joint_Environment_Exploration,
    author = {Spexard, T. and Li, S. and Wrede, B. and Hanheide, M. and Topp, E.A. and H{\"u}ttenrauch, H.},
    journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
    pages = {546-551},
    title = {Interaction awareness for joint environment exploration},
    year = {2007}
}

@article{Hanheide2004_Memory_Consistency_Validation,
    author = {Hanheide, M. and Bauckhage, C. and Sagerer, G.},
    journal = {Proceedings - International Conference on Pattern Recognition},
    pages = {459-462},
    title = {Memory consistency validation in a cognitive vision system},
    volume = {2},
    year = {2004}
}

@article{Hanheide2023_Autonomous_Topological_Optimisation,
    author = {Zhu, Z. and Das, G. and Hanheide, M.},
    journal = {Proceedings of the ACM Symposium on Applied Computing},
    pages = {791-799},
    title = {Autonomous Topological Optimisation for Multi-robot Systems in Logistics},
    year = {2023}
}

@article{Poozhiyil_2025_Autonomous_Robotic_Sorting,
    author = {Poozhiyil, Mithun and Argin, Omer F. and Rai, Mini and Esfahani, Amir G. and Hanheide, Marc and King, Ryan and Saunderson, Phil and Moulin-Ramsden, Mike and Yang, Wen and García, Laura Palacio and Mackay, Iain and Mishra, Abhishek and Okamoto, Sho and Yeung, Kelvin},
    doi = {10.3390/machines13030214},
    issn = {2075-1702},
    journal = {Machines},
    month = {March},
    number = {3},
    pages = {214},
    publisher = {MDPI AG},
    title = {A Framework for Real-Time Autonomous Robotic Sorting and Segregation of Nuclear Waste: Modelling, Identification and Control of DexterTM Robot},
    url = {http://dx.doi.org/10.3390/machines13030214},
    volume = {13},
    year = {2025}
}

@article{Castri_2024_Candoit_Causal_Discovery,
    author = {Castri, Luca and Mghames, Sariah and Hanheide, Marc and Bellotto, Nicola},
    doi = {10.1002/aisy.202400181},
    issn = {2640-4567},
    journal = {Advanced Intelligent Systems},
    month = {November},
    number = {12},
    publisher = {Wiley},
    title = {CAnDOIT: Causal Discovery with Observational and Interventional Data from Time Series},
    url = {http://dx.doi.org/10.1002/aisy.202400181},
    volume = {6},
    year = {2024}
}

@article{Hanheide2007_Humanoriented_Interaction,
    author = {Spexard, T.P. and Hanheide, M. and Sagerer, G.},
    journal = {IEEE Transactions on Robotics},
    number = {5},
    pages = {852-862},
    title = {Human-oriented interaction with an anthropomorphic robot},
    volume = {23},
    year = {2007}
}

@article{Hanheide2005_Combining_Environmental_Cues,
    author = {Hanheide, M. and Bauckhage, C. and Sagerer, G.},
    journal = {Proceedings of the Seventh International Conference on Multimodal Interfaces, ICMI{'}05},
    pages = {25-31},
    title = {Combining environmental cues \&amp; head gestures to interact with wearable devices},
    year = {2005}
}

@article{Poozhiyil_2025_Autonomous_Robotic_Sorting_Autonomous_Robotic_Sorting,
    author = {Poozhiyil, Mithun and Argin, Omer F and Rai, Mini and Esfahani, Amir G and Hanheide, Marc and King, Ryan and Saunderson, Phil and Moulin-Ramsden, Mike and Yang, Wen and García, Laura Palacio and Mackay, Iain and Mishra, Abhishek and Okamoto, Sho and Yeung, Kelvin},
    doi = {10.20944/preprints202501.1606.v1},
    month = {January},
    publisher = {MDPI AG},
    title = {A Framework for Real-Time Autonomous Robotic Sorting and Segregation of Nuclear Waste: Modelling, Identification, and Control of Dexter Robot},
    url = {http://dx.doi.org/10.20944/preprints202501.1606.v1},
    year = {2025}
}

@inproceedings{lirolem6918_Enhancing_Human_Cooperation,
    abstract = {Humans naturally use an impressive variety of ways to com-
municate. In this work, we investigate the possibilities of complementing these natural communication channels with articial ones. For this, augmented reality is used as a technique to add synthetic visual and auditory stimuli to people's perception. A system for the mutual display
of the gaze direction of two interactants is presented and its acceptance is shown through a study. Finally, future possibilities of promoting this novel concept of articial communication channels are explored},
    author = {Christian Mertes and Angelika Dierker and Thomas Hermann and Marc Hanheide and Gerhard Sagerer},
    booktitle = {Proceedings of the 13th International Conference on Human-Computer Interaction},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c0e88)},
    month = {July},
    note = {Humans naturally use an impressive variety of ways to com-
municate. In this work, we investigate the possibilities of complementing these natural communication channels with articial ones. For this, augmented reality is used as a technique to add synthetic visual and auditory stimuli to people's perception. A system for the mutual display
of the gaze direction of two interactants is presented and its acceptance is shown through a study. Finally, future possibilities of promoting this novel concept of articial communication channels are explored},
    pages = {447--451},
    publisher = {Springer},
    title = {Enhancing human cooperation with multimodal augmented reality},
    url = {http://eprints.lincoln.ac.uk/6918/},
    year = {2009}
}

@article{Hanheide2020_Continuous_Engagement_Assessment,
    author = {Del Duchetto, F. and Baxter, P. and Hanheide, M.},
    journal = {Frontiers in Robotics and AI},
    title = {Are You Still With Me? Continuous Engagement Assessment From a Robot{'}s Point of View},
    volume = {7},
    year = {2020}
}

@article{Hanheide2020_Interactive_Movement_Primitives,
    author = {Mghames, S. and Hanheide, M. and Amir Ghalamzan, E.},
    journal = {arXiv},
    title = {Planning Actions by Interactive Movement Primitives: pushing occluding pieces to pick a ripe fruit},
    year = {2020}
}

@phdthesis{lirolem6743_Cognitive_Egovision_System,
    abstract = {With increasing computational power and decreasing size, computers nowadays are already wearable and mobile. They become attendant of peoples' everyday life. Personal digital assistants and mobile phones equipped with adequate software gain a lot of interest in public, although the functionality they provide in terms of assistance is little more than a mobile databases for appointments, addresses, to-do lists and photos. Compared to the assistance a human can provide, such systems are hardly to call real assistants. The motivation to construct more human-like assistance systems that develop a certain level of cognitive capabilities leads to the exploration of two central paradigms in this work. The first paradigm is termed cognitive vision systems. Such systems take human cognition as a design principle of underlying concepts and develop learning and adaptation capabilities to be more flexible in their application. They are embodied, active, and situated. Second, the ego-vision paradigm is introduced as a very tight interaction scheme between a user and a computer system that especially eases close collaboration and assistance between these two. Ego-vision systems (EVS) take a user's (visual) perspective and integrate the human in the system's processing loop by means of a shared perception and augmented reality. EVSs adopt techniques of cognitive vision to identify objects, interpret actions, and understand the user's visual perception. And they articulate their knowledge and interpretation by means of augmentations of the user's own view. These two paradigms are studied as rather general concepts, but always with the goal in mind to realize more flexible assistance systems that closely collaborate with its users. This work provides three major contributions. First, a definition and explanation of ego-vision as a novel paradigm is given. Benefits and challenges of this paradigm are discussed as well. Second, a configuration of different approaches that permit an ego-vision system to perceive its environment and its user is presented in terms of object and action recognition, head gesture recognition, and mosaicing. These account for the specific challenges identified for ego-vision systems, whose perception capabilities are based on wearable sensors only. Finally, a visual active memory (VAM) is introduced as a flexible conceptual architecture for cognitive vision systems in general, and for assistance systems in particular. It adopts principles of human cognition to develop a representation for information stored in this memory. So-called memory processes continuously analyze, modify, and extend the content of this VAM. The functionality of the integrated system emerges from their coordinated interplay of these memory processes. An integrated assistance system applying the approaches and concepts outlined before is implemented on the basis of the visual active memory. The system architecture is discussed and some exemplary processing paths in this system are presented and discussed. It assists users in object manipulation tasks and has reached a maturity level that allows to conduct user studies. Quantitative results of different integrated memory processes are as well presented as an assessment of the interactive system by means of these user studies.},
    author = {Marc Hanheide},
    keywords = {ARRAY(0x7f43493c1308)},
    month = {October},
    note = {With increasing computational power and decreasing size, computers nowadays are already wearable and mobile. They become attendant of peoples' everyday life. Personal digital assistants and mobile phones equipped with adequate software gain a lot of interest in public, although the functionality they provide in terms of assistance is little more than a mobile databases for appointments, addresses, to-do lists and photos. Compared to the assistance a human can provide, such systems are hardly to call real assistants. The motivation to construct more human-like assistance systems that develop a certain level of cognitive capabilities leads to the exploration of two central paradigms in this work. The first paradigm is termed cognitive vision systems. Such systems take human cognition as a design principle of underlying concepts and develop learning and adaptation capabilities to be more flexible in their application. They are embodied, active, and situated. Second, the ego-vision paradigm is introduced as a very tight interaction scheme between a user and a computer system that especially eases close collaboration and assistance between these two. Ego-vision systems (EVS) take a user's (visual) perspective and integrate the human in the system's processing loop by means of a shared perception and augmented reality. EVSs adopt techniques of cognitive vision to identify objects, interpret actions, and understand the user's visual perception. And they articulate their knowledge and interpretation by means of augmentations of the user's own view. These two paradigms are studied as rather general concepts, but always with the goal in mind to realize more flexible assistance systems that closely collaborate with its users. This work provides three major contributions. First, a definition and explanation of ego-vision as a novel paradigm is given. Benefits and challenges of this paradigm are discussed as well. Second, a configuration of different approaches that permit an ego-vision system to perceive its environment and its user is presented in terms of object and action recognition, head gesture recognition, and mosaicing. These account for the specific challenges identified for ego-vision systems, whose perception capabilities are based on wearable sensors only. Finally, a visual active memory (VAM) is introduced as a flexible conceptual architecture for cognitive vision systems in general, and for assistance systems in particular. It adopts principles of human cognition to develop a representation for information stored in this memory. So-called memory processes continuously analyze, modify, and extend the content of this VAM. The functionality of the integrated system emerges from their coordinated interplay of these memory processes. An integrated assistance system applying the approaches and concepts outlined before is implemented on the basis of the visual active memory. The system architecture is discussed and some exemplary processing paths in this system are presented and discussed. It assists users in object manipulation tasks and has reached a maturity level that allows to conduct user studies. Quantitative results of different integrated memory processes are as well presented as an assessment of the interactive system by means of these user studies.},
    school = {Universitat Bielefeld},
    title = {A cognitive ego-vision system for interactive assistance},
    url = {http://eprints.lincoln.ac.uk/6743/},
    year = {2006}
}

@article{Hanheide2021_Allocation_Ofparking_Spaces,
    author = {Ravikanna, R. and Hanheide, M. and Das, G. and Zhu, Z.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {337-348},
    title = {Maximising Availability of Transportation Robots Through Intelligent Allocation of~Parking Spaces},
    volume = {13054 LNAI},
    year = {2021}
}

@inproceedings{lirolem6923_Interactive_Robotic_Learning,
    abstract = {In learning tasks, interaction is mostly about the exchange
of knowledge. The interaction process shall be governed on the one hand by the knowledge the tutor wants to convey and on the other by the lacks of knowledge of the learner. In human-robot interaction (HRI), it is usually the human demonstrating or explicitly verbalizing her knowl-
edge and the robot acquiring a respective representation. The ultimate goal in interactive robot learning is thus to enable inexperienced, un- trained users to tutor robots in a most natural and intuitive manner.
This goal is often impeded by a lack of knowledge of the human about the internal processing and expectations of the robot and by the inflexibility of the robot to understand open-ended, unconstrained tutoring or demonstration. Hence, we propose mixed-initiative strategies to allow both to mutually contribute to the interactive learning process as
a bi-directional negotiation about knowledge. Along this line this paper discusses two initially different case studies on object manipulation and learning of spatial environments. We present different styles of mixed-
initiative in these scenarios and discuss the merits in each case.},
    author = {Julia Peltason and Ingo L\"utkebohle and Britta Wrede and Marc Hanheide},
    booktitle = {Mixed Initiative Workshop on Improving Human-Robot Communication with Mixed-Initiative and Context-Awareness at the 18th IEEE International Symposium on Robot and Human Interactive Communication},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c0df8)},
    month = {September},
    note = {In learning tasks, interaction is mostly about the exchange
of knowledge. The interaction process shall be governed on the one hand by the knowledge the tutor wants to convey and on the other by the lacks of knowledge of the learner. In human-robot interaction (HRI), it is usually the human demonstrating or explicitly verbalizing her knowl-
edge and the robot acquiring a respective representation. The ultimate goal in interactive robot learning is thus to enable inexperienced, un- trained users to tutor robots in a most natural and intuitive manner.
This goal is often impeded by a lack of knowledge of the human about the internal processing and expectations of the robot and by the inflexibility of the robot to understand open-ended, unconstrained tutoring or demonstration. Hence, we propose mixed-initiative strategies to allow both to mutually contribute to the interactive learning process as
a bi-directional negotiation about knowledge. Along this line this paper discusses two initially different case studies on object manipulation and learning of spatial environments. We present different styles of mixed-
initiative in these scenarios and discuss the merits in each case.},
    publisher = {IEEE},
    title = {Mixed initiative in interactive robotic learning},
    url = {http://eprints.lincoln.ac.uk/6923/},
    year = {2009}
}

@article{Hanheide2024_Causal_Analysis_Framework,
    author = {Castri, L. and Beraldo, G. and Mghames, S. and Hanheide, M. and Bellotto, N.},
    journal = {arXiv},
    title = {ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot Interaction Applications},
    year = {2024}
}

@article{Hanheide2015_Moves_Conveying_Navigation,
    author = {May, A.D. and Dondrup, C. and Hanheide, M.},
    journal = {2015 European Conference on Mobile Robots, ECMR 2015 - Proceedings},
    title = {Show me your moves! Conveying navigation intention of a mobile robot to humans},
    year = {2015}
}

@article{Hanheide2004_Information_Fusion,
    author = {Wrede, S. and Hanheide, M. and Bauckhage, C. and Sagerer, G.},
    journal = {Proceedings of the Seventh International Conference on Information Fusion, FUSION 2004},
    pages = {198-205},
    title = {An active memory as a model for information fusion},
    volume = {1},
    year = {2004}
}

@article{Hanheide2017_Care_Home_Residents,
    author = {Hanheide, M. and Hebesberger, D. and Krajn{\'i}k, T.},
    journal = {ACM/IEEE International Conference on Human-Robot Interaction},
    pages = {341-349},
    title = {The When, Where, and How: An Adaptive Robotic Info-Terminal for Care Home Residents},
    volume = {Part F127194},
    year = {2017}
}

@article{Hanheide2018_Tabletop_Manipulation_Tasks,
    author = {Lightbody, P. and Baxter, P. and Hanheide, M.},
    journal = {ACM/IEEE International Conference on Human-Robot Interaction},
    pages = {177-178},
    title = {Studying Table-Top Manipulation Tasks: A Robust Framework for Object Tracking in Collaboration},
    year = {2018}
}

@article{lirolem6742_Computer_Vision_Systems,
    abstract = {Computer vision is becoming an integral part in human-machine interfaces as research increasingly aims at a seamless
and natural interaction between a user and an application system. Gesture recognition, context awareness, and grounding
concepts in the commonly perceived environment as well as in the interaction history are key abilities of such systems.
Simultaneously, recent computer vision research has indicated that integrated systems which are embedded in the world
and interact with their environment seem a prerequisite for solving more general vision tasks. Cognitive computer vision
systems which enable the generation of knowledge on the basis of perception, reasoning, and extension of prior models
are a major step towards this goal. For these, the integration, interaction and organization of memory becomes a key
issue in system design. In this article we will present a computational framework for integrated vision systems that is
centered around an active memory component. It supports a fast integration and substitution of system components,
various means of interaction patterns, and enables a system to reason about its own memory content. This framework
will be exemplified by means of a cognitive human-machine interface in an Augmented Reality scenario. The system is
able to acquire new concepts from interaction and provides a context aware scene augmentation for the user.},
    author = {Sven Wachsmuth and Sebastian Wrede and Marc Hanheide and Christian Bauckhage},
    journal = {KI - K\"unstliche Intelligenz},
    keywords = {ARRAY(0x7f43493c14b8)},
    month = {March},
    note = {Computer vision is becoming an integral part in human-machine interfaces as research increasingly aims at a seamless
and natural interaction between a user and an application system. Gesture recognition, context awareness, and grounding
concepts in the commonly perceived environment as well as in the interaction history are key abilities of such systems.
Simultaneously, recent computer vision research has indicated that integrated systems which are embedded in the world
and interact with their environment seem a prerequisite for solving more general vision tasks. Cognitive computer vision
systems which enable the generation of knowledge on the basis of perception, reasoning, and extension of prior models
are a major step towards this goal. For these, the integration, interaction and organization of memory becomes a key
issue in system design. In this article we will present a computational framework for integrated vision systems that is
centered around an active memory component. It supports a fast integration and substitution of system components,
various means of interaction patterns, and enables a system to reason about its own memory content. This framework
will be exemplified by means of a cognitive human-machine interface in an Augmented Reality scenario. The system is
able to acquire new concepts from interaction and provides a context aware scene augmentation for the user.},
    number = {2},
    pages = {25--31},
    publisher = {Springer for Fachbereiches KI in der Gesellschaft f\"ur Informatik},
    title = {An active memory model for cognitive computer vision systems},
    url = {http://eprints.lincoln.ac.uk/6742/},
    volume = {19},
    year = {2005}
}

@article{Hanheide2009_Human_Robot_Interaction,
    author = {Lang, C. and Hanheide, M. and Lohse, M. and Wersing, H. and Sagerer, G.},
    journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
    pages = {189-194},
    title = {Feedback interpretation based on facial expressions in human - Robot interaction},
    year = {2009}
}

@article{Hanheide2021_Dynamic_Behaviour_Prediction,
    author = {Nazari, K. and Mandill, W. and Hanheide, M. and Esfahani, A.G.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {284-293},
    title = {Tactile Dynamic Behaviour Prediction Based on Robot Action},
    volume = {13054 LNAI},
    year = {2021}
}

@article{Hanheide2020_Grasping_Collisionfree_Manipulation,
    author = {Parsa, S. and Kamale, D. and Mghames, S. and Nazari, K. and Pardi, T. and Srinivasan, A.R. and Neumann, G. and Hanheide, M. and Amir, G.E.},
    journal = {IEEE International Conference on Automation Science and Engineering},
    pages = {1552-1557},
    title = {Haptic-guided shared control grasping: Collision-free manipulation},
    volume = {2020-August},
    year = {2020}
}

@article{Hanheide2023_Smart_Parking_System,
    author = {Ravikanna, R. and Heselden, J. and Khan, M.A. and Perrett, A. and Zhu, Z. and Das, G. and Hanheide, M.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {38-50},
    title = {Smart Parking System Using Heuristic Optimization for~Autonomous Transportation Robots in~Agriculture},
    volume = {14136 LNAI},
    year = {2023}
}

@article{Hanheide2004_Industrial_Car_Assembly,
    author = {St{\"o}{\ss}el, D. and Hanheide, M. and Sagerer, G. and Kr{\"u}ger, L. and Ellenrieder, M.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {528-535},
    title = {Feature and Viewpoint Selection for Industrial Car Assembly},
    volume = {3175},
    year = {2004}
}

@article{Hanheide2021_Fruit_Picking_Applications,
    author = {Wagner, N. and Kirk, R. and Hanheide, M. and Cielniak, G.},
    journal = {Proceedings - IEEE International Conference on Robotics and Automation},
    pages = {6818-6823},
    title = {Efficient and Robust Orientation Estimation of Strawberries for Fruit Picking Applications},
    volume = {2021-May},
    year = {2021}
}

@article{Hanheide2014_Qualitative_Trajectory_Calculus,
    author = {Dondrup, C. and Bellotto, N. and Hanheide, M.},
    journal = {AAAI Spring Symposium - Technical Report},
    pages = {18-25},
    title = {A probabilistic model of human-robot spatial interaction using a qualitative trajectory calculus},
    volume = {SS-14-06},
    year = {2014}
}

@article{Hanheide2013_Robot_George_Interactive,
    author = {Zillich, M. and Zhou, K. and Sko?aj, D. and Kristan, M. and Vre?ko, A. and Mahni?, M. and Jan{\'i}?ek, M. and Kruijff, G.-J.M. and Keller, T. and Hanheide, M. and Hawes, N.},
    journal = {ACM/IEEE International Conference on Human-Robot Interaction},
    pages = {425},
    title = {Robot George - Interactive continuous learning of visual concepts},
    year = {2013}
}

@article{Hanheide2024_Robotic_Fleet_Deployment,
    author = {Zhu, Z. and Das, G.P. and Hanheide, M.},
    journal = {Lecture Notes in Networks and Systems},
    pages = {285-301},
    title = {On Optimising Topology of Agricultural Fields for Efficient Robotic Fleet Deployment},
    volume = {794},
    year = {2024}
}

@article{Hanheide2010_Remembering_Interaction_Episodes,
    author = {Gieselmann, S. and Hanheide, M. and Wrede, B.},
    journal = {2010 10th IEEE-RAS International Conference on Humanoid Robots, Humanoids 2010},
    pages = {566-571},
    title = {Remembering interaction episodes: An unsupervised learning approach for a humanoid robot},
    year = {2010}
}

@article{Hanheide2011_Online_Datadriven_Fault,
    author = {Golombek, R. and Wrede, S. and Hanheide, M. and Heckmann, M.},
    journal = {IEEE International Conference on Intelligent Robots and Systems},
    pages = {3011-3016},
    title = {Online data-driven fault detection for robotic systems},
    year = {2011}
}

@article{Hanheide2022_Collection_And_Evaluation,
    author = {Polvara, R. and Mellado, S.M. and Hroob, I. and Cielniak, G. and Hanheide, M.},
    journal = {arXiv},
    title = {Collection and Evaluation of a Long-Term 4D Agri-Robotic Dataset},
    year = {2022}
}

@article{Hanheide2013_Humanrobot_Spatial_Interactions,
    author = {Bellotto, N. and Hanheide, M. and Van De Weghe, N.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {331-340},
    title = {Qualitative design and implementation of human-robot spatial interactions},
    volume = {8239 LNAI},
    year = {2013}
}

@article{Hanheide2014_Hri_Recent_Perspectives,
    author = {Lemaignan, S. and Hanheide, M. and Karg, M. and Khambhaita, H. and Kunze, L. and Lier, F. and L{\"u}tkebohle, I. and Milliez, G.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {13-24},
    title = {Simulation and HRI recent perspectives with the MORSE simulator},
    volume = {8810},
    year = {2014}
}

@article{Hanheide2019_Making_The_Case,
    author = {Fernandez Carmona, M. and Parekh, T. and Hanheide, M.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {449-453},
    title = {Making the case for human-aware navigation in warehouses},
    volume = {11650 LNAI},
    year = {2019}
}

@article{Hanheide2010_Calibrationfree_Head_Gesture,
    author = {W{\"o}hler, N.-C. and Gro{\ss}ekath{\"o}fer, U. and Dierker, A. and Hanheide, M. and Kopp, S. and Hermann, T.},
    journal = {Proceedings - International Conference on Pattern Recognition},
    pages = {3814-3817},
    title = {A calibration-free head gesture recognition system with online capability},
    year = {2010}
}

@article{Hanheide2016_Frequency_Map_Enhancement,
    author = {Krajn{\'i}k, T. and Fentanes, J.P. and Hanheide, M. and Duckett, T.},
    journal = {IEEE International Conference on Intelligent Robots and Systems},
    pages = {4558-4563},
    title = {Persistent localization and life-long mapping in changing environments using the frequency map enhancement},
    volume = {2016-November},
    year = {2016}
}

@article{Hanheide2013_Facial_Communicative_Signal,
    author = {Lang, C. and Wachsmuth, S. and Hanheide, M. and Wersing, H.},
    journal = {Proceedings - IEEE International Conference on Robotics and Automation},
    pages = {170-177},
    title = {Facial communicative signal interpretation in human-robot interaction by discriminative video subsequence selection},
    year = {2013}
}

@article{Hanheide2018_Learning_Local_Recovery,
    author = {Del Duchetto, F. and Kucukyilmaz, A. and Iocchi, L. and Hanheide, M.},
    journal = {IEEE Robotics and Automation Letters},
    number = {4},
    pages = {4084-4091},
    title = {Do Not Make the Same Mistakes Again and Again: Learning Local Recovery Policies for Navigation from Human Demonstrations},
    volume = {3},
    year = {2018}
}

@article{Hanheide2009_Systemic_Interaction_Analysis,
    author = {Lohse, M. and Hanheide, M. and Pitsch, K. and Rohlfing, K.J. and Sagerer, G.},
    journal = {Interaction Studies},
    number = {3},
    pages = {298-323},
    title = {Improving HRI design by applying Systemic Interaction Analysis (SInA)},
    volume = {10},
    year = {2009}
}

@article{Hanheide2012_Intentions_And_Actions,
    author = {Hanheide, M. and Lohse, M. and Zender, H.},
    journal = {International Journal of Social Robotics},
    number = {2},
    pages = {107-108},
    title = {Expectations, Intentions, and Actions in Human-Robot Interaction},
    volume = {4},
    year = {2012}
}

@article{Hanheide2009_Synchrony_Making,
    author = {Rolf, M. and Hanheide, M. and Rohlfing, K.J.},
    journal = {IEEE Transactions on Autonomous Mental Development},
    number = {1},
    pages = {55-67},
    title = {Attention via synchrony: Making use of multimodal cues in social learning},
    volume = {1},
    year = {2009}
}

@article{Hanheide2016_Interactive_Continuous_Learning,
    author = {Sko?aj, D. and Vre?ko, A. and Mahni?, M. and Jan{\'i}?ek, M. and Kruijff, G.-J.M. and Hanheide, M. and Hawes, N. and Wyatt, J.L. and Keller, T. and Zhou, K. and Zillich, M. and Kristan, M.},
    journal = {Journal of Experimental and Theoretical Artificial Intelligence},
    number = {5},
    pages = {823-848},
    title = {An integrated system for interactive continuous learning of categorical knowledge},
    volume = {28},
    year = {2016}
}

@inproceedings{lirolem14893_Spatiotemporal_Representation,
    abstract = {The FP-7 Integrated Project STRANDS [1] is aimed at producing intelligent mobile robots that are able to operate robustly for months in dynamic human environments. To achieve long-term autonomy, the robots would need to understand the environment and how it changes over time. For that, we will have to develop novel approaches to extract 3D shapes, objects, people, and models of activity from sensor data gathered during months of autonomous operation.
So far, the environment models used in mobile robotics have been tailored to capture static scenes and environment variations are largely treated as noise. Therefore, utilization of the static models in ever-changing, real world environments is difficult. We propose to represent the environment?s spatio-temporal dynamics by its frequency spectrum.},
    author = {Tom Duckett and Marc Hanheide and Tomas Krajnik and Jaime Pulido Fentanes and Christian Dondrup},
    booktitle = {International IEEE/EPSRC Workshop on Autonomous Cognitive Robotics},
    keywords = {ARRAY(0x7f43493bf038)},
    month = {March},
    title = {Spatio-temporal representation for cognitive control in long-term scenarios},
    url = {http://eprints.lincoln.ac.uk/14893/},
    year = {2013}
}

@article{Hanheide2010_Cast_Middleware,
    author = {Hawes, N. and Hanheide, M.},
    journal = {AAAI Workshop - Technical Report},
    pages = {11-12},
    title = {CAST: Middleware for memory-based architectures},
    volume = {WS-10-09},
    year = {2010}
}

@article{Hanheide2018_Rasberry_Robotic,
    author = {From, P.J. and Grimstad, L. and Hanheide, M. and Pearson, S. and Cielniak, G.},
    journal = {Mechanical Engineering},
    number = {6},
    pages = {14-18},
    title = {RASberry: Robotic and autonomous systems: For berry production},
    volume = {140},
    year = {2018}
}

@article{Hanheide2017_Progressed_Dementia_Interact,
    author = {Hebesberger, D.V. and Dondrup, C. and Gisinger, C. and Hanheide, M.},
    journal = {ACM/IEEE International Conference on Human-Robot Interaction},
    pages = {131-132},
    title = {Patterns of use: How older adults with progressed dementia interact with a robot},
    year = {2017}
}

@article{Hanheide2020_Gripper_Workspace_Spheres,
    author = {Sorour, M. and Elgeneidy, K. and Hanheide, M. and Abdalmjed, M. and Srinivasan, A. and Neumann, G.},
    journal = {Proceedings - IEEE International Conference on Robotics and Automation},
    pages = {1539-1545},
    title = {Enhancing Grasp Pose Computation in Gripper Workspace Spheres},
    year = {2020}
}

@article{Hanheide2010_Automatic_Recognition_Performances,
    author = {Lang, C. and Wachsmuth, S. and Wersing, H. and Hanheide, M.},
    journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010},
    pages = {79-85},
    title = {Facial expressions as feedback cue in human-robot interaction-a comparison between human and automatic recognition performances},
    year = {2010}
}

@article{Hanheide2009_Concept_In_Hri,
    author = {Peters, A. and Spexard, T.P. and Wei{\ss}, P. and Hanheide, M.},
    journal = {Workshop Proceedings - 32nd Annual Conference on Artificial Intelligence, KI 2009 - Workshop on Planning, Scheduling, Design, and Configuration, PuK 2009},
    pages = {10},
    title = {Make room for me a study outline on a spatial and situational concept in HRI},
    year = {2009}
}

@article{Hanheide2024_Human_Injury_Assessment,
    author = {Guevara, L. and Khalid, M. and Hanheide, M. and Parsons, S.},
    journal = {Computers and Electronics in Agriculture},
    title = {Probabilistic model-checking of collaborative robots: A human injury assessment in agricultural applications},
    volume = {222},
    year = {2024}
}

@article{Hanheide2023_Ltsnet_Endtoend_Unsupervised,
    author = {Hroob, I. and Molina, S. and Polvara, R. and Cielniak, G. and Hanheide, M.},
    journal = {arXiv},
    title = {LTS-NET: End-to-end Unsupervised Learning of Long-Term 3D Stable objects},
    year = {2023}
}

@article{Hanheide2024_Spatial_Interaction_Scenarios,
    author = {Castri, L. and Beraldo, G. and Mghames, S. and Hanheide, M. and Bellotto, N.},
    journal = {arXiv},
    title = {Experimental Evaluation of ROS-Causal in Real-World Human-Robot Spatial Interaction Scenarios},
    year = {2024}
}

@article{Hanheide2012_Communicative_Signals_Valence,
    author = {Lang, C. and Wachsmuth, S. and Hanheide, M. and Wersing, H.},
    journal = {International Journal of Social Robotics},
    number = {3},
    pages = {249-262},
    title = {Facial Communicative Signals: Valence Recognition in Task-Oriented Human-Robot Interaction},
    volume = {4},
    year = {2012}
}

@article{lirolem6712_Cognitive_Vision_System,
    abstract = {The European Cognitive Vision project VAMPIRE uses mobile AR-kits to interact with a visual active memory for teaching and retrieval purposes. This paper describes concept and technical realization of the used mobile AR-kits and discusses interactive learning and retrieval in office environments, and the active memory infrastructure. The focus is on 3D interaction for pointing in a scene coordinate system. This is achieved by 3D augmented pointing, which combines inside-out tracking for head pose recovery and 3D stereo human?computer interaction. Experimental evaluation shows that the accuracy of this 3D cursor is within a few centimeters, which is sufficient to point at an object in an office. Finally, an application of the cursor in VAMPIRE is presented, where in addition to the mobile system, at least one stationary active camera is used to obtain different views of an object. There are many potential applications, for example an improved view-based object recognition.},
    author = {H. Siegl and Marc Hanheide and S. Wrede and A. Pinz},
    journal = {Image and Vision Computing},
    keywords = {ARRAY(0x7f43493c1218)},
    month = {December},
    note = {The European Cognitive Vision project VAMPIRE uses mobile AR-kits to interact with a visual active memory for teaching and retrieval purposes. This paper describes concept and technical realization of the used mobile AR-kits and discusses interactive learning and retrieval in office environments, and the active memory infrastructure. The focus is on 3D interaction for pointing in a scene coordinate system. This is achieved by 3D augmented pointing, which combines inside-out tracking for head pose recovery and 3D stereo human?computer interaction. Experimental evaluation shows that the accuracy of this 3D cursor is within a few centimeters, which is sufficient to point at an object in an office. Finally, an application of the cursor in VAMPIRE is presented, where in addition to the mobile system, at least one stationary active camera is used to obtain different views of an object. There are many potential applications, for example an improved view-based object recognition.},
    number = {12},
    pages = {1895--1903},
    publisher = {Elsevier},
    title = {An augmented reality human?computer interface for object localization in a cognitive vision system},
    url = {http://eprints.lincoln.ac.uk/6712/},
    volume = {25},
    year = {2007}
}

@article{Hanheide2015_Personality_And_Verbal,
    author = {Walters, M.L. and Lohse, M. and Hanheide, M. and Wrede, B. and Syrdal, D.S. and Koay, K.L. and Green, A. and H{\"u}ttenrauch, H. and Dautenhahn, K. and Sagerer, G. and Severinson-Eklundh, K.},
    journal = {Household Service Robotics},
    pages = {467-486},
    title = {Evaluating the Robot Personality and Verbal Behavior of Domestic Robots Using Video-Based Studies},
    year = {2015}
}

@article{Hanheide2022_Performance_In_Teleoperation,
    author = {Parsa, S. and Maior, H.A. and Thumwood, A.R.E. and Wilson, M.L. and Hanheide, M. and Esfahani, A.G.},
    journal = {Conference on Human Factors in Computing Systems - Proceedings},
    title = {The Impact of Motion Scaling and Haptic Guidance on Operators{'} Workload and Performance in Teleoperation},
    year = {2022}
}

@article{Das_2024_Unified_Topological_Representation,
    author = {Das, Gautham and Cielniak, Grzegorz and Heselden, James and Pearson, Simon and Duchetto, Francesco Del and Zhu, Zuyuan and Dichtl, Johann and Hanheide, Marc and Fentanes, Jaime Pulido and Binch, Adam and Hutchinson, Michael and From, Pal},
    doi = {10.1002/rob.22494},
    issn = {1556-4967},
    journal = {Journal of Field Robotics},
    month = {December},
    publisher = {Wiley},
    title = {A Unified Topological Representation for Robotic Fleets in Agricultural Applications},
    url = {http://dx.doi.org/10.1002/rob.22494},
    year = {2024}
}

@article{Hanheide2020_Rfid_Tags_Discovery,
    author = {Polvara, R. and Fernandez-Carmona, M. and Neumann, G. and Neumann, G. and Neumann, G. and Hanheide, M.},
    journal = {IEEE Robotics and Automation Letters},
    number = {3},
    pages = {4477-4484},
    title = {Next-Best-Sense: A Multi-Criteria Robotic Exploration Strategy for RFID Tags Discovery},
    volume = {5},
    year = {2020}
}

@article{Hanheide2009_Human_Augmented_Mapping,
    author = {Peltason, J. and Siepmann, F.H.K. and Spexard, T.P. and Wrede, B. and Hanheide, M. and Topp, E.A.},
    journal = {Proceedings - IEEE International Conference on Robotics and Automation},
    pages = {2146-2153},
    title = {Mixed-initiative in human augmented mapping},
    year = {2009}
}

@article{Hanheide2020_Simtoreal_Quadrotor_Landing,
    author = {Polvara, R. and Patacchiola, M. and Hanheide, M. and Neumann, G.},
    journal = {Robotics},
    number = {1},
    title = {Sim-to-Real quadrotor landing via sequential deep Q-Networks and domain randomization},
    volume = {9},
    year = {2020}
}

@article{Hanheide2018_Lightweight_Navigation_System,
    author = {L{\'a}zaro, M.T. and Grisetti, G. and Iocchi, L. and Fentanes, J.P. and Hanheide, M.},
    journal = {Advances in Intelligent Systems and Computing},
    pages = {295-306},
    title = {A Lightweight Navigation System for Mobile Robots},
    volume = {694},
    year = {2018}
}

@article{Hanheide2021_Human_Motion_Prediction,
    author = {Palmieri, L. and Andrey, R. and Mainprice, J. and Hanheide, M. and Alahi, A. and Lilienthal, A. and Arras, K.O.},
    journal = {IEEE Robotics and Automation Letters},
    number = {3},
    pages = {5613-5617},
    title = {Guest Editorial: Introduction to the Special Issue on Long-Term Human Motion Prediction},
    volume = {6},
    year = {2021}
}

@article{Hanheide2016_Spatiotemporal_Affordance_Maps,
    author = {Riccio, F. and Capobianco, R. and Hanheide, M. and Nardi, D.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {271-280},
    title = {STAM: A framework for spatio-temporal affordance maps},
    volume = {9991 LNCS},
    year = {2016}
}

@article{Hanheide2018_Dof_Pedestrian_Trajectory,
    author = {Sun, L. and Yan, Z. and Mellado, S.M. and Hanheide, M. and Duckett, T.},
    journal = {Proceedings - IEEE International Conference on Robotics and Automation},
    pages = {5942-5948},
    title = {3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous Mobile Robot Deployment Data},
    year = {2018}
}

@article{Hanheide2020_Robot_Social_Abilities,
    author = {Del Duchetto, F. and Baxter, P. and Hanheide, M.},
    journal = {ACM/IEEE International Conference on Human-Robot Interaction},
    pages = {561-563},
    title = {Automatic assessment and learning of robot social abilities},
    year = {2020}
}

@article{Hanheide2022_Agricultural_Event_Prediction,
    author = {Pal, A. and Das, G. and Hanheide, M. and Leite, A.C. and From, P.J.},
    journal = {Agronomy},
    number = {6},
    title = {An Agricultural Event Prediction Framework towards Anticipatory Scheduling of Robot Fleets: General Concepts and Case Studies},
    volume = {12},
    year = {2022}
}

@incollection{lirolem11964_System_Integration_Supporting,
    abstract = {With robotic systems entering our daily life, they have to become more flexible and subsuming a multitude of abilities in one single integrated system. Sub- sequently an increased extensibility of the robots? system architectures is needed. The goal is to facilitate a long-time evolution of the integrated system in-line with the scientific progress on the algorithmic level. In this paper we present an approach developed for an event-driven robot architecture, focussing on the coordination and interplay of new abilities and components. Appropriate timing, sequencing strategies, execution guaranties, and process flow synchronisation are taken into account to allow appropriate arbitration and interaction between components as well as between the integrated system and the user. The presented approach features dynamic reconfiguration and global coordination based on simple production rules. These are applied first time in conjunction with flexible representations in global memory spaces and an event-driven architecture. As a result a highly adaptive robot control compared to alternative approaches is achieved, allowing system modification during runtime even within complex interactive human-robot scenarios. },
    author = {Thorsten P. Spexard and Marc Hanheide},
    booktitle = {Human centered robot systems: cognition, interaction, technology },
    keywords = {ARRAY(0x7f43493c0d38)},
    month = {November},
    number = {6},
    pages = {1--9},
    publisher = {Springer Berlin Heidelberg},
    series = {Cognitive Systems Monographs},
    title = {System integration supporting evolutionary development and design},
    url = {http://eprints.lincoln.ac.uk/11964/},
    year = {2009}
}

@article{Hanheide2008_Realtime_Object_Tracking,
    author = {J{\"u}ngling, K. and Arens, M. and Hanheide, M. and Sagerer, G.},
    journal = {Proceedings of the 11th International Conference on Information Fusion, FUSION 2008},
    title = {Fusion of perceptual processes for real-time object tracking},
    year = {2008}
}

@article{Hanheide2018_Customers_Automated_Analysis,
    author = {Herrero, R.P. and Fentanes, J.P. and Hanheide, M.},
    journal = {IEEE Robotics and Automation Letters},
    number = {4},
    pages = {3733-3740},
    title = {Getting to Know Your Robot Customers: Automated Analysis of User Identity and Demographics for Robots in the Wild},
    volume = {3},
    year = {2018}
}

@article{Hanheide2005_Versatile_Modelbased_Visibility,
    author = {Ellenrieder, M.M. and Kr{\"u}ger, L. and St{\"o}{\ss}e, D. and Hanheide, M.},
    journal = {Lecture Notes in Computer Science},
    pages = {669-678},
    title = {A versatile model-based visibility measure for geometric primitives},
    volume = {3540},
    year = {2005}
}

@article{Hanheide2022_Human_Spatial_Interactions,
    author = {Castri, L. and Mghames, S. and Hanheide, M. and Bellotto, N.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {154-164},
    title = {Causal Discovery of Dynamic Models for Predicting Human Spatial Interactions},
    volume = {13817 LNAI},
    year = {2022}
}

@article{Hanheide2023_Survey_Of_Maps,
    author = {Kucner, T.P. and Magnusson, M. and Mghames, S. and Palmieri, L. and Verdoja, F. and Swaminathan, C.S. and Krajn{\'i}k, T. and Schaffernicht, E. and Bellotto, N. and Hanheide, M. and Lilienthal, A.J.},
    journal = {International Journal of Robotics Research},
    number = {11},
    pages = {977-1006},
    title = {Survey of maps of dynamics for mobile robots},
    volume = {42},
    year = {2023}
}

@article{Hanheide2020_Movement_Primitives_Planning,
    author = {Mghames, S. and Hanheide, M. and Ghalamzan E., A.},
    journal = {IEEE International Conference on Intelligent Robots and Systems},
    pages = {2616-2623},
    title = {Interactive movement primitives: Planning to push occluding pieces for fruit picking},
    year = {2020}
}

@article{Hanheide2014_Humanrobot_Spatial_Interaction,
    author = {Dondrup, C. and Bellotto, N. and Hanheide, M.},
    journal = {IEEE RO-MAN 2014 - 23rd IEEE International Symposium on Robot and Human Interactive Communication: Human-Robot Co-Existence: Adaptive Interfaces and Systems for Daily Life, Therapy, Assistance and Socially Engaging Interactions},
    pages = {519-524},
    title = {Social distance augmented qualitative trajectory calculus for Human-Robot Spatial Interaction},
    year = {2014}
}

@article{Hanheide2020_Abstract_Visual_Programming,
    author = {Brown, O. and Roberts-Elliott, L. and Del Duchetto, F. and Hanheide, M. and Baxter, P.},
    journal = {ACM/IEEE International Conference on Human-Robot Interaction},
    pages = {154-156},
    title = {Abstract visual programming of social robots for novice users},
    year = {2020}
}

@article{Hanheide2023_Blt_Data_Set,
    author = {Polvara, R. and Molina, S. and Hroob, I. and Papadimitriou, A. and Tsiolis, K. and Giakoumis, D. and Likothanassis, S. and Tzovaras, D. and Cielniak, G. and Hanheide, M.},
    journal = {Journal of Field Robotics},
    title = {Bacchus Long-Term (BLT) data set: Acquisition of the agricultural multimodal BLT data set with automated robot deployment},
    year = {2023}
}

@article{Zahidi_2024_Optimising_Robotic_Operation,
    author = {Zahidi, Usman A. and Khan, Arshad and Zhivkov, Tsvetan and Dichtl, Johann and Li, Dom and Parsa, Soran and Hanheide, Marc and Cielniak, Grzegorz and Sklar, Elizabeth I. and Pearson, Simon and Ghalamzan‐E., Amir},
    doi = {10.1002/rob.22384},
    issn = {1556-4967},
    journal = {Journal of Field Robotics},
    month = {July},
    number = {8},
    pages = {2771–2789},
    publisher = {Wiley},
    title = {Optimising robotic operation speed with edge computing via 5G network: Insights from selective harvesting robots},
    url = {http://dx.doi.org/10.1002/rob.22384},
    volume = {41},
    year = {2024}
}

@inproceedings{lirolem6925_System_Integration_Supporting,
    abstract = {Abstract With robotic systems entering our daily life, they have to become more flexible and subsuming a multitude of abilities in one single integrated system. Subsequently an increased extensibility of the robots? system architectures is needed. 
The goal is to facilitate a long-time evolution of the integrated system in-line with the scientific progress on the algorithmic level. In this paper we present an approach developed for an event-driven robot architecture, focussing on the coordination and interplay of new abilities and components. Appropriate timing, sequencing strategies, execution guaranties, and process flow synchronization are taken into account to allow appropriate arbitration and interaction between components as well as between the integrated system and the user. The presented approach features dynamic reconfiguration and global coordination based on simple production rules. These are applied fist time in conjunction with flexible representations in global memory spaces and an event-driven architecture. As a result a highly adaptive robot control compared to alternative approaches is achieved, allowing system modification during runtime even within complex interactive human-robot scenarios},
    author = {Thorsten P. Spexard and Marc Hanheide},
    booktitle = {Conference on Human Centered Robotic Systems},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493bfce0)},
    month = {November},
    note = {Abstract With robotic systems entering our daily life, they have to become more flexible and subsuming a multitude of abilities in one single integrated system. Subsequently an increased extensibility of the robots? system architectures is needed. 
The goal is to facilitate a long-time evolution of the integrated system in-line with the scientific progress on the algorithmic level. In this paper we present an approach developed for an event-driven robot architecture, focussing on the coordination and interplay of new abilities and components. Appropriate timing, sequencing strategies, execution guaranties, and process flow synchronization are taken into account to allow appropriate arbitration and interaction between components as well as between the integrated system and the user. The presented approach features dynamic reconfiguration and global coordination based on simple production rules. These are applied fist time in conjunction with flexible representations in global memory spaces and an event-driven architecture. As a result a highly adaptive robot control compared to alternative approaches is achieved, allowing system modification during runtime even within complex interactive human-robot scenarios},
    pages = {1--9},
    publisher = {Springer},
    title = {System integration supporting evolutionary development and design},
    url = {http://eprints.lincoln.ac.uk/6925/},
    year = {2009}
}

@inproceedings{lirolem8323_Automatic_Recognition_Performances,
    abstract = {Facial expressions are one important nonverbal communication cue, as they can provide feedback in conversations between people and also in human-robot interaction. This paper presents an evaluation of three standard pattern recognition techniques (active appearance models, gabor energy filters, and raw images) for facial feedback interpretation in terms of valence (success and failure) and compares the results to the human performance. The used database contains videos of people interacting with a robot by teaching the names of several objects to it. After teaching, the robot should term the objects correctly. The subjects reacted to its answer while showing spontaneous facial expressions, which were classified in this work. One main result is that an automatic classification of facial expressions in terms of valence using simple standard pattern recognition techniques is possible with an accuracy comparable to the average human classification rate, but with a high variance between different subjects, likewise to the human performance. \^A\\copyright 2010 IEEE.},
    address = {San Francisco, CA},
    author = {C. Lang and S. Wachsmuth and H. Wersing and Marc Hanheide},
    booktitle = {Conference of 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010},
    journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010},
    keywords = {ARRAY(0x7f43493bfc80)},
    month = {June},
    note = {Conference Code: 81678},
    pages = {79--85},
    title = {Facial expressions as feedback cue in human-robot interaction: a comparison between human and automatic recognition performances},
    url = {http://eprints.lincoln.ac.uk/8323/},
    year = {2010}
}

@article{Hanheide2009_Multimodal_Augmented_Reality,
    author = {Dierker, A. and Mertes, C. and Hermann, T. and Hanheide, M. and Sagerer, G.},
    journal = {ICMI-MLMI{'}09 - Proceedings of the International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interfaces},
    pages = {245-252},
    title = {Mediated attention with multimodal augmented reality},
    year = {2009}
}

@article{Hanheide2018_Special_Issue,
    author = {Kunze, L. and Hawes, N. and Duckett, T. and Hanheide, M.},
    journal = {IEEE Robotics and Automation Letters},
    number = {4},
    pages = {4431-4434},
    title = {Introduction to the Special Issue on AI for Long-Term Autonomy},
    volume = {3},
    year = {2018}
}

@article{Hanheide2019_Tour_Guide_Robot,
    author = {Duchetto, F.D. and Baxter, P. and Hanheide, M.},
    journal = {2019 28th IEEE International Conference on Robot and Human Interactive Communication, RO-MAN 2019},
    title = {Lindsey the Tour Guide Robot - Usage Patterns in a Museum Long-Term Deployment},
    year = {2019}
}

@inproceedings{lirolem6921_Concept_In_Hri,
    abstract = {Mobile robots are already applied in factories and hospitals, merely to do a distinct task. It is envisioned that robots assist in households, soon. Those service robots will have to cope with several situations and tasks and of course with sophisticated Human-Robot Interaction (HRI)},
    author = {Annika Peters and Thorsten P. Spexard and Petra Wei\ss and Marc Hanheide},
    booktitle = {Workshop on Behavior Monitoring and Interpretation - Well Being},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c0e28)},
    month = {September},
    note = {Mobile robots are already applied in factories and hospitals, merely to do a distinct task. It is envisioned that robots assist in households, soon. Those service robots will have to cope with several situations and tasks and of course with sophisticated Human-Robot Interaction (HRI)},
    title = {Make room for me: a spatial and situational movement concept in HRI},
    url = {http://eprints.lincoln.ac.uk/6921/},
    year = {2009}
}

@article{Hanheide2018_Longterm_Robot_Autonomy,
    author = {Kunze, L. and Hawes, N. and Duckett, T. and Hanheide, M. and Krajnik, T.},
    journal = {IEEE Robotics and Automation Letters},
    number = {4},
    pages = {4023-4030},
    title = {Artificial Intelligence for Long-Term Robot Autonomy: A Survey},
    volume = {3},
    year = {2018}
}

@article{Hanheide2020_Dialogue_Interactivity_Development,
    author = {Baxter, P. and Del Duchetto, F. and Hanheide, M.},
    journal = {Advances in Intelligent Systems and Computing},
    pages = {147-160},
    title = {Engaging Learners in Dialogue Interactivity Development for Mobile Robots},
    volume = {946 AISC},
    year = {2020}
}

@article{Hanheide2020_Taxonomy_Of_Trustrelevant,
    author = {Tolmeijer, S. and Weiss, A. and Hanheide, M. and Lindner, F. and Powers, T.M. and Dixon, C. and Tielman, M.L.},
    journal = {ACM/IEEE International Conference on Human-Robot Interaction},
    pages = {3-12},
    title = {Taxonomy of trust-relevant failures and mitigation strategies},
    year = {2020}
}

@article{Hanheide2010_Selfunderstanding_And_Selfextension,
    author = {Wyatt, J.L. and Aydemir, A. and Brenner, M. and Hanheide, M. and Hawes, N. and Jensfelt, P. and Kristan, M. and Kruijff, G.-J.M. and Lison, P. and Pronobis, A. and Sj{\"o}{\"o}, K. and Vrecko, A. and Zender, H. and Zillich, M. and Skocaj, D.},
    journal = {IEEE Transactions on Autonomous Mental Development},
    number = {4},
    pages = {282-303},
    title = {Self-understanding and self-extension: A systems and representational approach},
    volume = {2},
    year = {2010}
}

@article{Hanheide2020_Humanrobot_Spatial_Interaction,
    author = {Roberts-Elliott, L. and Fernandez-Carmona, M. and Hanheide, M.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {249-260},
    title = {Towards Safer Robot Motion: Using a Qualitative Motion Model to Classify Human-Robot Spatial Interaction},
    volume = {12228 LNAI},
    year = {2020}
}

@article{Hanheide2019_Gripper_Workspace_Spheres,
    author = {Sorour, M. and Elgeneidy, K. and Srinivasan, A. and Hanheide, M. and Neumann, G.},
    journal = {IEEE International Conference on Intelligent Robots and Systems},
    pages = {1541-1547},
    title = {Grasping Unknown Objects Based on Gripper Workspace Spheres},
    year = {2019}
}

@article{Hanheide2024_Ltsnet_Endtoend_Unsupervised,
    author = {Hroob, I. and Molina, S. and Polvara, R. and Cielniak, G. and Hanheide, M.},
    journal = {Lecture Notes in Networks and Systems},
    pages = {17-29},
    title = {LTS-NET: End-to-End Unsupervised Learning of~Long-Term 3D Stable Objects},
    volume = {794},
    year = {2024}
}

@article{Hanheide2016_Unterstutzung_Von_Physiotherapie,
    author = {Gerling, K. and Hebesberger, D. and Dondrup, C. and K{\"o}rtner, T. and Hanheide, M.},
    journal = {Zeitschrift fur Gerontologie und Geriatrie},
    number = {4},
    pages = {288-297},
    title = {Robot deployment in long-term care: Case study on using a mobile robot to support physiotherapy,Robotereinsatz in der Langzeitpflege: Fallstudie {\"u}ber den Gebrauch eines mobilen Roboters zur Unterst{\"u}tzung von Physiotherapie},
    volume = {49},
    year = {2016}
}

@article{lirolem6702_Spatial_Movement_Concept,
    abstract = {In humanhuman interaction, social signals or unconscious cues are sent and received by interaction partners. These signals and cues influence the interaction partner wanted and unwantedsometimes to achieve a distinct goal. To be aware of those signals and especially of implicit cues is crucial when interaction between a robot and a human is modelled. This project aims to use implicit body and machine movements to make HRI smoother and simpler. A robot should not only consider social rules with respect to proxemics in communication or in encounter people. It should also be able to signal and understand certain spatial constraints. A first spatial and situational constraint, which this research project currently focuses on, is avoiding each other. This includes not only passing by but also especially making room for each other. Consider a narrow place, e.g. hallways, door frames or a small kitchen. A robot might block the way or drives towards you, pursuing its own goal like you. Humans do not even speak to each other in order to pass by and avoid bumping into each other even if the space is narrow. A first study is currently conducted to find out which behaviour is the most appropriate avoiding strategy and how participants express their wish to pass by. Therefore, a variety of defensive and more offensive avoiding strategies of the robot are applied in experiments. The results of the study will be used to equip the robot with spatial concepts to make interaction faster and more appropriate.},
    author = {Annika Peters and Petra Weiss and Marc Hanheide},
    journal = {Cognitive Processing},
    keywords = {ARRAY(0x7f43493c0e58)},
    month = {September},
    note = {In humanhuman interaction, social signals or unconscious cues are sent and received by interaction partners. These signals and cues influence the interaction partner wanted and unwantedsometimes to achieve a distinct goal. To be aware of those signals and especially of implicit cues is crucial when interaction between a robot and a human is modelled. This project aims to use implicit body and machine movements to make HRI smoother and simpler. A robot should not only consider social rules with respect to proxemics in communication or in encounter people. It should also be able to signal and understand certain spatial constraints. A first spatial and situational constraint, which this research project currently focuses on, is avoiding each other. This includes not only passing by but also especially making room for each other. Consider a narrow place, e.g. hallways, door frames or a small kitchen. A robot might block the way or drives towards you, pursuing its own goal like you. Humans do not even speak to each other in order to pass by and avoid bumping into each other even if the space is narrow. A first study is currently conducted to find out which behaviour is the most appropriate avoiding strategy and how participants express their wish to pass by. Therefore, a variety of defensive and more offensive avoiding strategies of the robot are applied in experiments. The results of the study will be used to equip the robot with spatial concepts to make interaction faster and more appropriate.},
    number = {S2},
    pages = {177--178},
    publisher = {Springer},
    title = {Avoid me: a spatial movement concept in human-robot interaction},
    url = {http://eprints.lincoln.ac.uk/6702/},
    volume = {10},
    year = {2009}
}

@incollection{lirolem6714_Concept_In_Hri,
    abstract = {Mobile robots are already applied in factories and hospitals, merely to do a distinct task. It is envisioned that robots assist in households soon. Those service robots will have to cope with several situations and tasks and of course with sophisticated human-robot interactions (HRI). Therefore, a robot has not only to consider social rules with respect to proxemics, it must detect in which (interaction) situation it is in and act accordingly. With respect to spatial HRI, we concentrate on the use of non-verbal communication. This chapter stresses the meaning of both, machine movements as signals towards a human and human body language. Considering these aspects will make interaction simpler and smoother. An observational study is presented to acquire a concept of spatial prompting by a robot and by a human. When a person and robot meet in a narrow hallway in order to pass by, they have to make room for each other. But how can a robot make sure that both really want to pass by instead of starting interaction? This especially concerns narrow, non-artificial surroundings. Which social signals are expected by the user and on the other side, can be generated or processed by a robot? The results will show what an appropriate passing behaviour is and how to distinguish between passage situations and others. The results shed light upon the readability of signals in spatial HRI.},
    author = {Annika Peters and Thorsten P. Spexard and Marc Hanheide and Petra Weiss},
    booktitle = {Behaviour Monitoring and Interpretation - BMI Well-being},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493bfb00)},
    month = {April},
    note = {Mobile robots are already applied in factories and hospitals, merely to do a distinct task. It is envisioned that robots assist in households soon. Those service robots will have to cope with several situations and tasks and of course with sophisticated human-robot interactions (HRI). Therefore, a robot has not only to consider social rules with respect to proxemics, it must detect in which (interaction) situation it is in and act accordingly. With respect to spatial HRI, we concentrate on the use of non-verbal communication. This chapter stresses the meaning of both, machine movements as signals towards a human and human body language. Considering these aspects will make interaction simpler and smoother. An observational study is presented to acquire a concept of spatial prompting by a robot and by a human. When a person and robot meet in a narrow hallway in order to pass by, they have to make room for each other. But how can a robot make sure that both really want to pass by instead of starting interaction? This especially concerns narrow, non-artificial surroundings. Which social signals are expected by the user and on the other side, can be generated or processed by a robot? The results will show what an appropriate passing behaviour is and how to distinguish between passage situations and others. The results shed light upon the readability of signals in spatial HRI.},
    publisher = {IOS Press},
    title = {Hey robot, get out of my way: survey on a spatial and situational movement concept in HRI},
    url = {http://eprints.lincoln.ac.uk/6714/},
    year = {2011}
}

@inproceedings{lirolem8365_Robot_George_Interactive,
    abstract = {The video presents the robot George learning visual concepts in dialogue with a tutor},
    author = {Michael Zillich and Kai Zhou and Danijel Skocaj and Matej Kristan and Alen Vrecko and Marko Mahnic and Miroslav Janicek and Geert-Jan M. Kruijff and Thomas Keller and Marc Hanheide and Nick Hawes},
    booktitle = {Proceedings of the 8th ACM/IEEE international conference on Human-robot interaction},
    keywords = {ARRAY(0x7f43493bf920)},
    month = {March},
    pages = {425},
    publisher = {IEEE Press},
    title = {Robot George: interactive continuous learning of visual concepts},
    url = {http://eprints.lincoln.ac.uk/8365/},
    year = {2013}
}

@article{Hanheide2008_Active_Memorybased_Interaction,
    author = {Hanheide, M. and Sagerer, G.},
    journal = {Proceedings of the 17th IEEE International Symposium on Robot and Human Interactive Communication, RO-MAN},
    pages = {101-106},
    title = {Active memory-based interaction strategies for learning-enabling behaviors},
    year = {2008}
}

@article{Hanheide2021_Responsible_Development,
    author = {Rose, D.C. and Lyon, J. and de Boon, A. and Hanheide, M. and Pearson, S.},
    journal = {Nature Food},
    number = {5},
    pages = {306-309},
    title = {Responsible development of autonomous robotics in agriculture},
    volume = {2},
    year = {2021}
}

@article{Hanheide2016_Velocity_Costmaps,
    author = {Dondrup, C. and Hanheide, M.},
    journal = {25th IEEE International Symposium on Robot and Human Interactive Communication, RO-MAN 2016},
    pages = {586-592},
    title = {Qualitative constraints for human-aware robot navigation using Velocity Costmaps},
    year = {2016}
}

@article{Hanheide2015_Predicting_And_Maximising,
    author = {Fentanes, J.P. and Lacerda, B. and Krajnik, T. and Hawes, N. and Hanheide, M.},
    journal = {Proceedings - IEEE International Conference on Robotics and Automation},
    number = {June},
    pages = {1112-1117},
    title = {Now or later? Predicting and maximising success of navigation actions from long-term experience},
    volume = {2015-June},
    year = {2015}
}

@article{Hanheide2009_Laserbased_Navigation_Enhanced,
    author = {Yuan, F. and Swadzba, A. and Philippsen, R. and Engin, O. and Hanheide, M. and Wachsmuth, S.},
    journal = {Proceedings - IEEE International Conference on Robotics and Automation},
    pages = {2844-2850},
    title = {Laser-based navigation enhanced with 3D time-of-flight data},
    year = {2009}
}

@inproceedings{lirolem6931_Evaluating_Extrovert,
    abstract = {Human-robot interaction (HRI) research is here presented into social robots that have to be able to interact with inexperienced users. In the design of these robots many research findings of human-human interaction and human-computer interaction are adopted but the direct applicability of these theories is limited because a robot is different from both humans and computers. Therefore, new methods have to be developed in HRI in order to build robots that are suitable for inexperienced users. In this paper we present a video study we conducted employing our robot BIRON (Bielefeld robot companion) which is designed for use in domestic environments. Subjects watched the system during the interaction with a human and rated two different robot behaviours (extrovert and introvert). The behaviours differed regarding verbal output and person following of the robot. Aiming to improve human-robot interaction, participantspsila ratings of the behaviours were evaluated and compared.},
    author = {Manja Lohse and Marc Hanheide and Britta Wrede and Michael L. Walters and Kheng Lee Koay and Dag Sverre Syrdal and Anders Green and Helge Huttenrauch and Kerstin Dautenhahn and Gerhard Sagerer and Kerstin Severinson-Eklundh},
    booktitle = {The 17th IEEE International Symposium on Robot and Human Interactive Communication},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1098)},
    month = {August},
    note = {Human-robot interaction (HRI) research is here presented into social robots that have to be able to interact with inexperienced users. In the design of these robots many research findings of human-human interaction and human-computer interaction are adopted but the direct applicability of these theories is limited because a robot is different from both humans and computers. Therefore, new methods have to be developed in HRI in order to build robots that are suitable for inexperienced users. In this paper we present a video study we conducted employing our robot BIRON (Bielefeld robot companion) which is designed for use in domestic environments. Subjects watched the system during the interaction with a human and rated two different robot behaviours (extrovert and introvert). The behaviours differed regarding verbal output and person following of the robot. Aiming to improve human-robot interaction, participantspsila ratings of the behaviours were evaluated and compared.},
    pages = {488--493},
    publisher = {IEEE},
    title = {Evaluating extrovert and introvert behaviour of a domestic robot -- a video study},
    url = {http://eprints.lincoln.ac.uk/6931/},
    year = {2008}
}

@article{Hanheide2023_Industrial_Mobile_Robots,
    author = {Molina, S. and Mannucci, A. and Magnusson, M. and Adolfsson, D. and Andreasson, H. and Hamad, M. and Abdolshah, S. and Chadalavada, R.T. and Palmieri, L. and Linder, T. and Swaminathan, C.S. and Kucner, T.P. and Hanheide, M. and Fernandez-Carmona, M. and Cielniak, G. and Duckett, T. and Pecora, F. and Bokesand, S. and Arras, K.O. and Haddadin, S. and Lilienthal, A.J.},
    journal = {IEEE Robotics and Automation Magazine},
    title = {The ILIAD Safety Stack: Human-Aware Infrastructure-Free Navigation of Industrial Mobile Robots},
    year = {2023}
}

@article{Hanheide2008_Integrated_Recognition_Systems,
    author = {Bauckhage, C. and Wachsmuth, S. and Hanheide, M. and Wrede, S. and Sagerer, G. and Heidemann, G. and Ritter, H.},
    journal = {Image and Vision Computing},
    number = {1},
    pages = {5-14},
    title = {The visual active memory perspective on integrated recognition systems},
    volume = {26},
    year = {2008}
}

@article{Hanheide2010_Mobile_Robot_Motivated,
    author = {Aydemir, A. and Brenner, M. and G{\"o}belbecker, M. and Hanheide, M. and Hawes, N. and Jensfelt, P. and Kruijff, G.-J.M. and Kruijff-Korbayov{\'a}, I. and Lison, P. and Sj{\"o}o, K. and Wyatt, J. and Zender, H. and Zillich, M.},
    journal = {4th International Conference on Cognitive Systems, CogSys 2010},
    title = {Dora the explorer: A mobile robot motivated by curiosity},
    year = {2010}
}

@article{Hanheide2021_Navigateandseek_A_Robotics,
    author = {Polvara, R. and Del Duchetto, F. and Neumann, G. and Hanheide, M.},
    journal = {arXiv},
    title = {Navigate-and-seek: A robotics framework for people localization in agricultural environments},
    year = {2021}
}

@article{Hanheide2008_Systemic_Interaction_Analysis,
    author = {Lohse, M. and Hanheide, M. and Rohlfing, K.J. and Sagerer, G.},
    journal = {Proceedings of the 4th ACM/IEEE International Conference on Human-Robot Interaction, HRI{'}09},
    pages = {93-100},
    title = {Systemic interaction analysis (SInA) in HRI},
    year = {2008}
}

@inproceedings{lirolem6944_Combining_Environmental_Cues,
    abstract = {As wearable sensors and computing hardware are becoming a reality,
new and unorthodox approaches to seamless human-computer
interaction can be explored. This paper presents the prototype of a
wearable, head-mounted device for advanced human-machine interaction
that integrates speech recognition and computer vision
with head gesture analysis based on inertial sensor data. We will
focus on the innovative idea of integrating visual and inertial data
processing for interaction. Fusing head gestures with results from
visual analysis of the environment provides rich vocabularies for
human-machine communication because it renders the environment
into an interface: if objects or items in the surroundings are
being associated with system activities, head gestures can trigger
commands if the corresponding object is being looked at. We will
explain the algorithmic approaches applied in our prototype and
present experiments that highlight its potential for assistive technology.
Apart from pointing out a new direction for seamless interaction
in general, our approach provides a new and easy to use
interface for disabled and paralyzed users in particular.},
    author = {Marc Hanheide and Christian Bauckhage and Gerhard Sagerer},
    booktitle = {7th international conference on Multimodal interfaces},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c13f8)},
    month = {October},
    note = {As wearable sensors and computing hardware are becoming a reality,
new and unorthodox approaches to seamless human-computer
interaction can be explored. This paper presents the prototype of a
wearable, head-mounted device for advanced human-machine interaction
that integrates speech recognition and computer vision
with head gesture analysis based on inertial sensor data. We will
focus on the innovative idea of integrating visual and inertial data
processing for interaction. Fusing head gestures with results from
visual analysis of the environment provides rich vocabularies for
human-machine communication because it renders the environment
into an interface: if objects or items in the surroundings are
being associated with system activities, head gestures can trigger
commands if the corresponding object is being looked at. We will
explain the algorithmic approaches applied in our prototype and
present experiments that highlight its potential for assistive technology.
Apart from pointing out a new direction for seamless interaction
in general, our approach provides a new and easy to use
interface for disabled and paralyzed users in particular.},
    pages = {25--31},
    publisher = {ACM Association of Computing Machinery},
    title = {Combining environmental cues \& head gestures to interact with wearable devices},
    url = {http://eprints.lincoln.ac.uk/6944/},
    year = {2005}
}

@article{Hanheide2016_Reproduction_In_Robotics,
    author = {Lier, F. and Hanheide, M. and Natale, L. and Schulz, S. and Weisz, J. and Wachsmuth, S. and Wrede, S.},
    journal = {IEEE International Conference on Intelligent Robots and Systems},
    pages = {3298-3305},
    title = {Towards automated system and experiment reproduction in robotics},
    volume = {2016-November},
    year = {2016}
}

@article{Hanheide2006_Building_Modular_Vision,
    author = {L{\"o}mker, F. and Wrede, S. and Hanheide, M. and Fritsch, J.},
    journal = {Proceedings of the Fourth IEEE International Conference on Computer Vision Systems, ICVS{'}06},
    pages = {2},
    title = {Building modular vision systems with a graphical plugin environment},
    volume = {2006},
    year = {2006}
}

@article{Hanheide2006_Wearable_Assistance_System,
    author = {Hanheide, M. and Hofemann, N. and Sagerer, G.},
    journal = {Proceedings - International Conference on Pattern Recognition},
    pages = {1254-1257},
    title = {Action recognition in a wearable assistance system},
    volume = {2},
    year = {2006}
}
