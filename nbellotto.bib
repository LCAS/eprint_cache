@article{Bellotto2014_Mobile_Robot_Swarm,
    author = {Arvin, F. and Turgut, A.E. and Bazyari, F. and Arikan, K.B. and Bellotto, N. and Yue, S.},
    journal = {Adaptive Behavior},
    number = {3},
    pages = {189-206},
    title = {Cue-based aggregation with a mobile robot swarm: A novel fuzzy-based method},
    volume = {22},
    year = {2014}
}

@article{Bellotto2016_Social_Activity_Recognition,
    author = {Coppola, C. and Faria, D.R. and Nunes, U. and Bellotto, N.},
    journal = {IEEE International Conference on Intelligent Robots and Systems},
    pages = {5055-5061},
    title = {Social activity recognition based on probabilistic merging of skeleton features with proximity priors from RGB-D data},
    volume = {2016-November},
    year = {2016}
}

@article{Bellotto2013_Integrating_Mobile_Robotics,
    author = {Cielniak, G. and Bellotto, N. and Duckett, T.},
    journal = {IEEE Transactions on Education},
    number = {1},
    pages = {48-53},
    title = {Integrating mobile robotics and vision with undergraduate computer science},
    volume = {56},
    year = {2013}
}

@article{Bellotto2017_Volumebased_Human_Reidentification,
    author = {Co?ar, S. and Coppola, C. and Bellotto, N.},
    journal = {VISIGRAPP 2017 - Proceedings of the 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
    pages = {389-397},
    title = {Volume-based human re-identification with RGB-D cameras},
    volume = {4},
    year = {2017}
}

@article{Carnelos_2025_Efficient_Rustbased_Inference,
    author = {Carnelos, Matteo and Pasti, Francesco and Bellotto, Nicola},
    doi = {10.1016/j.iot.2025.101498},
    issn = {2542-6605},
    journal = {Internet of Things},
    month = {March},
    pages = {101498},
    publisher = {Elsevier BV},
    title = {MicroFlow: An Efficient Rust-Based Inference Engine for TinyML},
    url = {http://dx.doi.org/10.1016/j.iot.2025.101498},
    volume = {30},
    year = {2025}
}

@article{Bellotto2014_Vice_Versa_Solving,
    author = {Iliopoulos, K. and Bellotto, N. and Mavridis, N.},
    journal = {AAAI Spring Symposium - Technical Report},
    pages = {57-64},
    title = {From sequence to trajectory and vice versa: Solving the inverse QTC problem and coping with real-world trajectories},
    volume = {SS-14-06},
    year = {2014}
}

@conference{11577_3475012.4_Human_Motion_Prediction,
    author = {Mghames, Sariah and Castri, Luca and Hanheide, Marc and Bellotto, Nicola},
    booktitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
    title = {A Neuro-Symbolic Approach for Enhanced Human Motion Prediction},
    year = {2023}
}

@article{Schofield_2018_Understanding_Images,
    author = {Andrew J. Schofield and Iain D. Gilchrist and Marina Bloj and Ales Leonardis and Nicola Bellotto},
    doi = {10.1098/rsfs.2018.0027},
    journal = {Interface Focus},
    month = {jun},
    number = {4},
    pages = {20180027},
    publisher = {The Royal Society},
    title = {Understanding images in biological and computer vision},
    url = {https://doi.org/10.1098%2Frsfs.2018.0027},
    volume = {8},
    year = {2018}
}

@article{Bellotto2012_Cognitive_Active_Vision,
    author = {Utsumi, Y. and Sommerlade, E. and Bellotto, N. and Reid, I.},
    journal = {Proceedings - IEEE International Conference on Robotics and Automation},
    pages = {1238-1245},
    title = {Cognitive active vision for human identification},
    year = {2012}
}

@article{Fernandez_Carmona_2024_Waveletbased_Temporal_Models,
    author = {Fernandez-Carmona, Manuel and Mghames, Sariah and Bellotto, Nicola},
    doi = {10.3233/ais-230144},
    issn = {1876-1364},
    journal = {Journal of Ambient Intelligence and Smart Environments},
    month = {June},
    number = {2},
    pages = {181â€“200},
    publisher = {SAGE Publications},
    title = {Wavelet-based temporal models of human activity for anomaly detection in smart robot-assisted environments1},
    url = {http://dx.doi.org/10.3233/AIS-230144},
    volume = {16},
    year = {2024}
}

@article{Bellotto2014_Qualitative_Trajectory_Calculus,
    author = {Dondrup, C. and Bellotto, N. and Hanheide, M.},
    journal = {AAAI Spring Symposium - Technical Report},
    pages = {18-25},
    title = {A probabilistic model of human-robot spatial interaction using a qualitative trajectory calculus},
    volume = {SS-14-06},
    year = {2014}
}

@article{Liu_2019_Dynamic_Facial_Movements,
    author = {Daqi Liu and Nicola Bellotto and Shigang Yue},
    doi = {10.1109/tnnls.2019.2927274},
    journal = {{IEEE} Transactions on Neural Networks and Learning Systems},
    pages = {1--10},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    title = {Deep Spiking Neural Network for Video-Based Disguise Face Recognition Based on Dynamic Facial Movements},
    url = {https://doi.org/10.1109%2Ftnnls.2019.2927274},
    year = {2019}
}

@article{Bellotto2015_Qtcltinfgtdltinfgt_Extending,
    author = {Mavridis, N. and Bellotto, N. and Iliopoulos, K. and Van De Weghe, N.},
    journal = {Information Sciences},
    pages = {20-30},
    title = {QTC\&lt;inf\&gt;3D\&lt;/inf\&gt;: Extending the qualitative trajectory calculus to three dimensions},
    volume = {322},
    year = {2015}
}

@conference{11577_3549116_Predict_Human_Motion,
    author = {Rudenko, A. and Zhu, Y. and Rodrigues de Almeida, T. and Schreiter, T. and Castri, L. and Bellotto, N. and Linder, T. and Vaskevicius, N. and Palmieri, L. and Magnusson, M. and Lilienthal, A. J.},
    booktitle = {German Robotics Conference},
    title = {Hierarchical System to Predict Human Motion and Intentions for Efficient and Safe Human-Robot Interaction in Industrial Environments},
    year = {2025}
}

@article{Camara_2021_Autonomous_Driving_Part,
    author = {Fanta Camara and Nicola Bellotto and Serhan Cosar and Florian Weber and Dimitris Nathanael and Matthias Althoff and Jingyuan Wu and Johannes Ruenz and Andre Dietrich and Gustav Markkula and Anna Schieben and Fabio Tango and Natasha Merat and Charles Fox},
    doi = {10.1109/tits.2020.3006767},
    journal = {{IEEE} Transactions on Intelligent Transportation Systems},
    month = {sep},
    number = {9},
    pages = {5453--5472},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    title = {Pedestrian Models for Autonomous Driving Part {II}: High-Level Models of Human Behavior},
    url = {https://doi.org/10.1109%2Ftits.2020.3006767},
    volume = {22},
    year = {2021}
}

@article{Bellotto2007_Comparison_Of_Kalman,
    author = {Bellotto, N. and Hu, O.},
    journal = {Proceedings of the 13th IASTED International Conference on Robotics and Applications, RA 2007 and Proceedings of the IASTED International Conference on Telematics},
    pages = {388-393},
    title = {People tracking with a mobile robot: A comparison of Kalman and particle filters},
    year = {2007}
}

@conference{11577_3455198_Visual_Fixation_Model,
    abstract = {In nature, lightweight and low-powered insects are ideal model systems to study motion perception strategies. Understanding the underlying characteristics and functionality of insects' visual systems is not only attractive to neural system modellers but also critical in providing effective solutions to future robotics. This paper presents a novel modelling of dynamic vision system inspired by Drosophila physiology for mimicking fast motion tracking and a closed-loop behavioural response to fixation. The proposed model was realised on the embedded system in an autonomous micro robot which has limited computational resources. A monocular camera was applied as the only motion sensing modality. Systematic experiments including open-loop and closed-loop bio-robotic tests validated the proposed visual fixation model: the robot showed motion tracking and fixation behaviours similarly to insects; the image processing frequency can maintain 25 45Hz. Arena tests also demonstrated a successful following behaviour aroused by fixation in navigation.},
    author = {Fu, Q. and Bellotto, N. and Hu, C. and Yue, S.},
    booktitle = {2018 IEEE International Conference on Robotics and Biomimetics, ROBIO 2018},
    doi = {10.1109/ROBIO.2018.8665074},
    isbn = {978-1-7281-0377-8},
    pages = {1802--1808},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    title = {Performance of a Visual Fixation Model in an Autonomous Micro Robot Inspired by Drosophila Physiology},
    year = {2018}
}

@article{Bellotto2006_Laser_Data_Fusion,
    author = {Bellotto, N. and Hu, H.},
    journal = {2006 IEEE International Conference on Robotics and Biomimetics, ROBIO 2006},
    pages = {7-12},
    title = {Vision and laser data fusion for tracking people with a mobile robot},
    year = {2006}
}

@conference{11577_3469177_Lowcost_Embedded_Systems,
    author = {Pasti, Francesco and Bellotto, Nicola},
    booktitle = {Proceedings of the 18th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP 2023)},
    doi = {10.5220/0011797400003417},
    isbn = {978-989-758-634-7},
    pages = {282--293},
    title = {Evaluation of Computer Vision-Based Person Detection on Low-Cost Embedded Systems},
    url = {https://www.scitepress.org/Link.aspx?doi=10.5220/0011797400003417},
    volume = {5},
    year = {2023}
}

@conference{11577_3515927_Spatial_Interaction_Scenarios,
    author = {Castri, L. and Beraldo, G. and Mghames, S. and Hanheide, M. and Bellotto, N.},
    booktitle = {Proceedings of the IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},
    doi = {10.1109/RO-MAN60168.2024.10731290},
    title = {Experimental Evaluation of ROS-Causal in Real-World Human-Robot Spatial Interaction Scenarios},
    year = {2024}
}

@conference{11577_3460074_Human_Spatial_Interactions,
    abstract = {Exploiting robots for activities in human-shared environments, whether warehouses, shopping centres or hospitals, calls for such robots to understand the underlying physical interactions between nearby agents and objects. In particular, modelling cause-and-effect relations between the latter can help to predict unobserved human behaviours and anticipate the outcome of specific robot interventions. In this paper, we propose an application of causal discovery methods to model human-robot spatial interactions, trying to understand human behaviours from real-world sensor data in two possible scenarios: humans interacting with the environment, and humans interacting with obstacles. New methods and practical solutions are discussed to exploit, for the first time, a state-of-the-art causal discovery algorithm in some challenging human environments, with potential application in many service robotics scenarios. To demonstrate the utility of the causal models obtained from real-world datasets, we present a comparison between causal and non-causal prediction approaches. Our results show that the causal model correctly captures the underlying interactions of the considered scenarios and improves its prediction accuracy.},
    author = {Castri, Luca and Mghames, Sariah and Hanheide, Marc and Bellotto, Nicola},
    booktitle = {Lecture Notes in Computer Science - International Conference on Social Robotics (ICSR)},
    doi = {10.1007/978-3-031-24667-8_14},
    isbn = {978-3-031-24666-1},
    pages = {154--164},
    title = {Causal Discovery of Dynamic Models for Predicting Human Spatial Interactions},
    volume = {13817},
    year = {2022}
}

@conference{11577_3481942_Multiagent_Spatial_Interactions,
    author = {Mghames, Sariah and Castri, Luca and Hanheide, Marc and Bellotto, Nicola},
    booktitle = {Proceedings of the IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},
    title = {Qualitative Prediction of Multi-Agent Spatial Interactions},
    year = {2023}
}

@article{Camara_2021_Autonomous_Driving_Part_Autonomous_Driving_Part,
    author = {Fanta Camara and Nicola Bellotto and Serhan Cosar and Dimitris Nathanael and Matthias Althoff and Jingyuan Wu and Johannes Ruenz and Andre Dietrich and Charles W. Fox},
    doi = {10.1109/tits.2020.3006768},
    journal = {{IEEE} Transactions on Intelligent Transportation Systems},
    month = {oct},
    number = {10},
    pages = {6131--6151},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    title = {Pedestrian Models for Autonomous Driving Part I: Low-Level Models, From Sensing to Tracking},
    url = {https://doi.org/10.1109%2Ftits.2020.3006768},
    volume = {22},
    year = {2021}
}

@conference{11577_3498542_Human_Motion_Prediction,
    abstract = {Reasoning on the context of human beings is crucial for many real-world applications especially for those deploying autonomous systems (e.g. robots). In this paper, we present a new approach for context reasoning to further advance the field of human motion prediction. We therefore propose a neuro-symbolic approach for human motion prediction (NeuroSyM), which weights differently the interactions in the neighbourhood by leveraging an intuitive technique for spatial representation called Qualitative Trajectory Calculus (QTC). The proposed approach is experimentally tested on medium and long term time horizons using two architectures from the state of art, one of which is a baseline for human motion prediction and the other is a baseline for generic multivariate time-series prediction. Six datasets of challenging crowded scenarios, collected from both fixed and mobile cameras, were used for testing. Experimental results show that the NeuroSyM approach outperforms in most cases the baseline architectures in terms of prediction accuracy.},
    author = {Mghames, S. and Castri, L. and Hanheide, M. and Bellotto, N.},
    booktitle = {Proceedings of the International Joint Conference on Neural Networks},
    doi = {10.1109/IJCNN54540.2023.10191970},
    isbn = {978-1-6654-8867-9},
    pages = {1--8},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    title = {A Neuro-Symbolic Approach for Enhanced Human Motion Prediction},
    volume = {2023-},
    year = {2023}
}

@article{11577_3515923_Editorial_Swarm_Neurorobots,
    author = {Hu, C. and Arvin, F. and Bellotto, N. and Yue, S. and Li, H.},
    doi = {10.3389/fnbot.2024.1386178},
    journal = {FRONTIERS IN NEUROROBOTICS},
    keywords = {bio-inspired; environmental perception; neurorobotic; real-world deployment; swarm},
    publisher = {FRONTIERS MEDIA SA},
    title = {Editorial: Swarm neuro-robots with the bio-inspired environmental perception},
    volume = {18},
    year = {2024}
}

@article{11577_3454981_Rgbd_Video_Sequences,
    author = {Coppola, C. and Cosar, S. and Faria, D. R. and Bellotto, N.},
    doi = {10.1007/s12369-019-00541-y},
    journal = {INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS},
    number = {1},
    pages = {201--215},
    title = {Social Activity Recognition on Continuous RGB-D Video Sequences},
    url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065185036&amp;doi=10.1007/s12369-019-00541-y&amp;partnerID=40&amp;md5=232dcfd3074f5a713c6b30bcaa4effff},
    volume = {12},
    year = {2020}
}

@article{Bellotto2016_People_With_Mci,
    author = {Salatino, C. and Gower, V. and Ghrissi, M. and Tapus, A. and Wieczorowska-Tobis, K. and Suwalska, A. and Barattini, P. and Rosso, R. and Munaro, G. and Bellotto, N. and van den Heuvel, H.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {326-334},
    title = {The EnrichMe project: A robotic solution for independence and active aging of elderly people with MCI},
    volume = {9758},
    year = {2016}
}

@conference{11577_3455195_Boneconduction_Audio_Interface,
    abstract = {The ActiVis projectâ€™s aim is to build a mobile guidance aid to help people with limited vision find objects in an unknown environment. This system uses bone-conduction headphones to transmit audio signals to the user and requires an effective non-visual interface. To this end, we propose a new audio-based interface that uses a spatialised signal to convey a targetâ€™s position on the horizontal plane. The vertical position on the median plan is given by adjusting the toneâ€™s pitch to overcome the audio localisation limitations of bone-conduction headphones. This interface is validated through a set of experiments with blindfolded and visually impaired participants.},
    author = {Lock, J. C. and Gilchrist, I. D. and Cielniak, G. and Bellotto, N.},
    booktitle = {Communications in Computer and Information Science},
    doi = {10.1007/978-981-15-1301-5_43},
    isbn = {978-981-15-1300-8},
    keywords = {Bone-conduction; Human-machine interface; Spatialised sound; Varying pitch; Vision impairment},
    pages = {542--553},
    publisher = {Springer},
    title = {Bone-conduction audio interface to guide people with visual impairments},
    volume = {1122},
    year = {2019}
}

@inbook{Broughton_George_2016_Mild_Cognitive_Impairments,
    author = {Broughton George and Krajn&iacute;k Tom&aacute;&scaron; and Fernandez-Carmona Manuel and Cielniak Grzegorz and Bellotto Nicola},
    booktitle = {Intelligent Environments 2016},
    doi = {10.3233/978-1-61499-690-3-366},
    issn = {1875-4163},
    publisher = {IOS Press},
    title = {RFID-Based Object Localisation with a Mobile Robot to Assist the Elderly with Mild Cognitive Impairments},
    url = {http://dx.doi.org/10.3233/978-1-61499-690-3-366},
    year = {2016}
}

@article{Kucner_2023_Survey_Of_Maps,
    author = {Kucner, Tomasz Piotr and Magnusson, Martin and Mghames, Sariah and Palmieri, Luigi and Verdoja, Francesco and Swaminathan, Chittaranjan Srinivas and KrajnÃ­k, TomÃ¡Å¡ and Schaffernicht, Erik and Bellotto, Nicola and Hanheide, Marc and Lilienthal, Achim J},
    doi = {10.1177/02783649231190428},
    issn = {1741-3176},
    journal = {The International Journal of Robotics Research},
    month = {August},
    number = {11},
    pages = {977â€“1006},
    publisher = {SAGE Publications},
    title = {Survey of maps of dynamics for mobile robots},
    url = {http://dx.doi.org/10.1177/02783649231190428},
    volume = {42},
    year = {2023}
}

@inbook{11577_3454982_Continuous_Game_Theory,
    abstract = {Autonomous Vehicles (AVs) must interact with other road users. They must understand and adapt to complex pedestrian behaviour, especially during crossings where priority is not clearly defined. This includes feedback effects such as modelling a pedestrianâ€™s likely behaviours resulting from changes in the AVs behaviour. For example, whether a pedestrian will yield if the AV accelerates, and vice versa. To enable such automated interactions, it is necessary for the AV to possess a statistical model of the pedestrianâ€™s responses to its own actions. A previous work demonstrated a proof-ofconcept method to fit parameters to a simplified model based on data from a highly artificial discrete laboratory task with human subjects. The method was based on LIDAR-based person tracking, game theory, and Gaussian process analysis. The present study extends this method to enable analysis of more realistic continuous human experimental data. It shows for the first time how game-theoretic predictive parameters can be fit into pedestrians natural and continuous motion during road-crossings, and how predictions can be made about their interactions with AV controllers in similar real-world settings.},
    author = {Camara, F. and Cosar, S. and Bellotto, N. and Merat, N. and Fox, C. W.},
    booktitle = {Human Factors in Intelligent Vehicles},
    pages = {1--20},
    publisher = {River Publishers},
    title = {Continuous game theory pedestrian modelling method for autonomous vehicles},
    year = {2020}
}

@article{Bellotto2007_People_Tracking,
    author = {Bellotto, N. and Hu, O.},
    journal = {Proceedings of the 2007 IEEE International Conference on Mechatronics and Automation, ICMA 2007},
    pages = {3565-3570},
    title = {People tracking and identification with a mobile robot},
    year = {2007}
}

@article{Bellotto2008_Mobile_Service_Robot,
    author = {Bellotto, N. and Hu, H.},
    journal = {2008 IEEE International Conference on Robotics and Biomimetics, ROBIO 2008},
    pages = {401-406},
    title = {Multimodal perception and recognition of humans with a mobile service robot},
    year = {2008}
}

@conference{11577_3455029_Active_Object_Search,
    author = {Lock, J. C. and Cielniak, G. and Bellotto, N.},
    booktitle = {Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP 2019)},
    doi = {10.5220/0007582304760485},
    pages = {476--485},
    title = {Active object search with a mobile device for people with visual impairments},
    url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068222941&amp;doi=10.5220/0007582304760485&amp;partnerID=40&amp;md5=053e50bb98ee968d44a5c6adb5d918b5},
    volume = {4},
    year = {2019}
}

@article{Bellotto2008_Appearancebased_Localization,
    author = {Bellotto, N. and Burn, K. and Fletcher, E. and Wermter, S.},
    journal = {Robotics and Autonomous Systems},
    number = {2},
    pages = {143-156},
    title = {Appearance-based localization for mobile robots using digital zoom and visual compass},
    volume = {56},
    year = {2008}
}

@article{Bellotto2013_Humanrobot_Spatial_Interactions,
    author = {Bellotto, N. and Hanheide, M. and Van De Weghe, N.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {331-340},
    title = {Qualitative design and implementation of human-robot spatial interactions},
    volume = {8239 LNAI},
    year = {2013}
}

@article{Bellotto2012_Qualitative_Trajectory_Calculus,
    author = {Hanheide, M. and Peters, A. and Bellotto, N.},
    journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
    pages = {689-694},
    title = {Analysis of human-robot spatial behaviour applying a qualitative trajectory calculus},
    year = {2012}
}

@article{Bellotto2017_Ceiling_Rgbd_Camera,
    author = {Liciotti, D. and Frontoni, E. and Zingaretti, P. and Bellotto, N. and Duckett, T.},
    journal = {ICPRAM 2017 - Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods},
    pages = {567-574},
    title = {HMM-based activity recognition with a ceiling RGB-D camera},
    volume = {2017-January},
    year = {2017}
}

@conference{11577_3465415_Robot_Sensor_Data,
    author = {Castri, L. and Mghames, S. and Hanheide, M. and Bellotto, N.},
    booktitle = {Proceedings of the Conference on Causal Learning and Reasoning},
    title = {Enhancing Causal Discovery from Robot Sensor Data in Dynamic Scenarios},
    year = {2023}
}

@conference{11577_3471199_Industrial_Environments,
    author = {Ghidoni, Stefano and Terreran, Matteo and Evangelista, Daniele and Menegatti, Emanuele and Eitzinger, Christian and Villagrossi, Enrico and Pedrocchi, Nicola and Castaman, Nicola and Malecha, Marcin and Mghames, Sariah and Castri, Luca and Hanheide, Marc and Bellotto, Nicola},
    booktitle = {Workshop AI per l'industria},
    title = {From Human Perception and Action Recognition to Causal Understanding of Human-Robot Interaction in Industrial Environments},
    year = {2022}
}

@article{Bellotto2016_Learning_Temporal_Context,
    author = {Coppola, C. and Kraj?{\'i}k, T. and Duckett, T. and Bellotto, N.},
    journal = {Frontiers in Artificial Intelligence and Applications},
    pages = {107-115},
    title = {Learning temporal context for activity recognition},
    volume = {285},
    year = {2016}
}

@article{Bellotto2015_Wearable_Physiological_Sensors,
    author = {Sandulescu, V. and Andrews, S. and Ellis, D. and Bellotto, N. and Mozos, O.M.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {526-532},
    title = {Stress detection using wearable physiological sensors},
    volume = {9107},
    year = {2015}
}

@article{11577_3455200_Distributed_Presynaptic_Connections,
    abstract = {Collision detection is one of the most challenging tasks for unmanned aerial vehicles (UAVs). This is especially true for small or micro-UAVs due to their limited computational power. In nature, flying insects with compact and simple visual systems demonstrate their remarkable ability to navigate and avoid collision in complex environments. A good example of this is provided by locusts. They can avoid collisions in a dense swarm through the activity of a motion-based visual neuron called the Lobula giant movement detector (LGMD). The defining feature of the LGMD neuron is its preference for looming. As a flying insect's visual neuron, LGMD is considered to be an ideal basis for building UAV's collision detecting system. However, existing LGMD models cannot distinguish looming clearly from other visual cues, such as complex background movements caused by UAV agile flights. To address this issue, we proposed a new model implementing distributed spatial-temporal synaptic interactions, which is inspired by recent findings in locusts' synaptic morphology. We first introduced the locally distributed excitation to enhance the excitation caused by visual motion with preferred velocities. Then, radially extending temporal latency for inhibition is incorporated to compete with the distributed excitation and selectively suppress the nonpreferred visual motions. This spatial-temporal competition between excitation and inhibition in our model is, therefore, tuned to preferred image angular velocity representing looming rather than background movements with these distributed synaptic interactions. Systematic experiments have been conducted to verify the performance of the proposed model for UAV agile flights. The results have demonstrated that this new model enhances the looming selectivity in complex flying scenes considerably and has the potential to be implemented on embedded collision detection systems for small or micro-UAVs.},
    author = {Zhao, J. and Wang, H. and Bellotto, N. and Hu, C. and Peng, J. and Yue, S.},
    doi = {10.1109/TNNLS.2021.3106946},
    journal = {IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS},
    keywords = {Visualization; Collision avoidance; Neurons; Computational modeling; Biological system modeling; Feature extraction; Unmanned aerial vehicles; Collision detection; distributed presynaptic connection-based Lobula giant movement detector (D-LGMD); dynamic complex visual scene; presynaptic neural network; unmanned aerial vehicles (UAVs)},
    number = {5},
    pages = {2539--2553},
    publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
    title = {Enhancing LGMD's Looming Selectivity for UAV With Spatial-Temporal Distributed Presynaptic Connections},
    volume = {34},
    year = {2023}
}

@article{Bellotto2017_Detection_Fusing_Rgbd,
    author = {Fernandez-Carmona, M. and Cosar, S. and Coppola, C. and Bellotto, N.},
    journal = {IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems},
    pages = {42-48},
    title = {Entropy-based abnormal activity detection fusing RGB-D and domotic sensors},
    volume = {2017-November},
    year = {2017}
}

@conference{11577_3489820_Potato_Instance_Segmentation,
    author = {Hurst, B. and Bellotto, N. and Bosilj, P.},
    booktitle = {Proceedings of Towards Autonomous Robotic Systems (TAROS)},
    doi = {10.1007/978-3-031-43360-3_22},
    title = {An assessment of self-supervised learning for data efficient potato instance segmentation},
    year = {2023}
}

@article{Bellotto2017_Stress_Detection,
    author = {Mozos, O.M. and Sandulescu, V. and Andrews, S. and Ellis, D. and Bellotto, N. and Dobrescu, R. and Ferrandez, J.M.},
    journal = {International Journal of Neural Systems},
    number = {2},
    title = {Stress detection using wearable physiological and sociometric sensors},
    volume = {27},
    year = {2017}
}

@article{Bellotto2009_Multimodal_Robot_Perception,
    author = {Bellotto, N.},
    journal = {Robot Vision: New Research},
    pages = {161-187},
    title = {Multimodal robot perception for robust human tracking and recognition},
    year = {2009}
}

@article{Bellotto2008_University_Open_Days,
    author = {Bellotto, N. and Rowland, S. and Hu, O.},
    journal = {IEEE/ASME International Conference on Advanced Intelligent Mechatronics, AIM},
    pages = {1355-1360},
    title = {Lux - An interactive receptionist robot for university open days},
    year = {2008}
}

@conference{11577_3455203_Task_Success_Classifiers,
    abstract = {Robots learning a new manipulation task from a small amount of demonstrations are increasingly demanded in different workspaces. A classifier model assessing the quality of actions can predict the successful completion of a task, which can be used by intelligent agents for action-selection. This paper presents a novel classifier that learns to classify task completion only from a few demonstrations. We carry out a comprehensive comparison of different neural classifiers, e.g. fully connected-based, fully convolutional-based, sequence2sequence-based, and domain adaptation-based classification. We also present a new dataset including five robot manipulation tasks, which is publicly available. We compared the performances of our novel classifier and the existing models using our dataset and the MIME dataset. The results suggest domain adaptation and timing-based features improve success prediction. Our novel model, i.e. fully convolutional neural network with domain adaptation and timing features, achieves an average classification accuracy of 97.3% and 95.5% across tasks in both datasets whereas state-of-the-art classifiers without domain adaptation and timing-features only achieve 82.4% and 90.3%, respectively.},
    author = {Mohtasib, A and Ghalamzan, Ea and Bellotto, N and Cuayahuitl, H},
    booktitle = {Proceedings of International Joint Conference on Neural Networks (IJCNN)},
    doi = {10.1109/IJCNN52387.2021.9534141},
    isbn = {978-1-6654-3900-8},
    keywords = {Deep Learning; Reward Learning; Task Success; Task Timing; Domain Adaptation; Robot Skill Learning},
    pages = {1--8},
    publisher = {IEEE},
    title = {Neural Task Success Classifiers for Robotic Manipulation from Few Real Demonstrations},
    volume = {2021-July},
    year = {2021}
}

@article{Bellotto2017_Social_Activity_Classification,
    author = {Coppola, C. and Cosar, S. and Faria, D.R. and Bellotto, N.},
    journal = {RO-MAN 2017 - 26th IEEE International Symposium on Robot and Human Interactive Communication},
    pages = {871-876},
    title = {Automatic detection of human interactions from RGB-D data for social activity classification},
    volume = {2017-January},
    year = {2017}
}

@conference{11577_3537866_Autonomous_Mobile_Systems,
    abstract = {The ability to perceive traversable regions is a crucial prerequisite for truly autonomous mobile systems. In our previous work [8], we introduced P-SVM, a 3D LiDAR-based traversability analysis system that achieves real-time performance in CPU. However, by design P-SVM does not capture features other than geometric features, limiting its performance and generalizability to complex environments. In this work, we introduce an augmented point cloud descriptor that further improves the performance of P-SVM. By exploiting additional features extracted from remission and height information in the point cloud, our model is able to adapt robustly to strong scene changes. Additionally, in the proposed descriptor, remission radial distribution features are introduced to capture local information around keypoints. This addresses the limitation of P-SVM, which focuses only on global features within the cell. Our new P-SVM2 model demonstrates performance almost on par with state-of-the-art deep learning-based methods on the challenging SemanticKITTI [1] dataset in the traversability analysis task. Notably, P-SVM2 is real-time and relies solely on mobile-level CPUs. Moreover, surprising results have been obtained in robustness and generalizability experiments.},
    author = {Wm, Li and Fusaro, D and Olivastri, E and Mosco, S and Bellotto, N and Pretto, A},
    booktitle = {Proceedings of 11th IEEE International Conference on Cybernetics and Intelligent Systems (CIS) / 11th IEEE International Conference on Robotics, Automation and Mechatronics (RAM)},
    doi = {10.1109/CIS-RAM61939.2024.10672725},
    keywords = {Traversability analysis; point cloud descriptor; real-time performance; autonomous mobile systems},
    pages = {352--359},
    publisher = {IEEE},
    title = {P-SVM2: Enhancing LiDAR-based Traversability Analysis with Augmented Point Cloud Descriptor for Autonomous Mobile Systems},
    year = {2024}
}

@article{Bellotto2014_Humanrobot_Spatial_Interaction,
    author = {Dondrup, C. and Bellotto, N. and Hanheide, M.},
    journal = {IEEE RO-MAN 2014 - 23rd IEEE International Symposium on Robot and Human Interactive Communication: Human-Robot Co-Existence: Adaptive Interfaces and Systems for Daily Life, Therapy, Assistance and Socially Engaging Interactions},
    pages = {519-524},
    title = {Social distance augmented qualitative trajectory calculus for Human-Robot Spatial Interaction},
    year = {2014}
}

@article{Bellotto2015_Progressive_Coadaptation,
    author = {Gallina, P. and Bellotto, N. and Di Luca, M.},
    journal = {ICINCO 2015 - 12th International Conference on Informatics in Control, Automation and Robotics, Proceedings},
    pages = {362-368},
    title = {Progressive co-adaptation in human-machine interaction},
    volume = {2},
    year = {2015}
}

@conference{11577_3498540_Activis_Mobile_Object,
    abstract = {The ActiVis project aims to deliver a mobile system that is able to guide a person with visual impairments towards a target object or area in an unknown indoor environment. For this, it uses new developments in object detection, mobile computing, action generation and human-computer interfacing to interpret the user's surroundings and present effective guidance directions. Our approach to direction generation uses a Partially Observable Markov Decision Process (POMDP) to track the system's state and output the optimal location to be investigated. This system includes an object detector and an audio-based guidance interface to provide a complete active search pipeline. The ActiVis system was evaluated in a set of experiments showing better performance than a simpler unguided case.},
    author = {Lock, J. C. and Tramontano, A. G. and Ghidoni, S. and Bellotto, N.},
    booktitle = {Proceedings of the 20th International Conference on Image Analysis and Processing (ICIAP)},
    doi = {10.1007/978-3-030-30645-8_59},
    isbn = {978-3-030-30644-1},
    keywords = {Active vision; Vision impairment; Object detection},
    pages = {649--660},
    publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
    title = {ActiVis: Mobile object detection and active guidance for people with visual impairments},
    volume = {11752 LNCS},
    year = {2019}
}

@conference{11577_3482820_Discovery_In_Robotics,
    author = {Castri, Luca and Mghames, Sariah and Bellotto, Nicola},
    booktitle = {Proceedings of The First AAAI Bridge Program on Continual Causality},
    pages = {85--91},
    title = {From Continual Learning to Causal Discovery in Robotics},
    volume = {208},
    year = {2023}
}

@article{Bellotto2012_Robot_Control_Based,
    author = {Bellotto, N.},
    journal = {AAAI Spring Symposium - Technical Report},
    pages = {2-6},
    title = {Robot control based on qualitative representation of human trajectories},
    volume = {SS-12-02},
    year = {2012}
}

@conference{11577_3529284_Continual_Object_Detection,
    author = {Pasti, F. and Ceccon, M. and Dalle Pezze, D. and Paissan, F. and Farella, E. and Susto, G. A. and Bellotto, N.},
    booktitle = {Workshop on Computational Aspects of Deep Learning, European Conference on Computer Vision (ECCV)},
    title = {Latent Distillation for Continual Object Detection at the Edge},
    year = {2024}
}

@article{11577_3455205.2_Robot_Thermal_Camera,
    abstract = {Human re-identification is an important feature of domestic service robots, in particular for elderly monitoring and assistance, because it allows them to perform personalized tasks and human-robot interactions. However vision-based re- identification systems are subject to limitations due to human pose and poor lighting conditions. This paper presents a new re-identification method for service robots using thermal images. In robotic applications, as the number and size of thermal datasets is limited, it is hard to use approaches that require huge amount of training samples. We propose a re-identification system that can work using only a small amount of data. During training, we perform entropy-based sampling to obtain a thermal dictionary for each person. Then, a symbolic representation is produced by converting each video into sequences of dictionary elements. Finally, we train a classifier using this symbolic representation and geometric distribution within the new representation domain. The experiments are performed on a new thermal dataset for human re-identification, which includes various situations of human motion, poses and occlusion, and which is made publicly available for research purposes. The proposed approach has been tested on this dataset and its improvements over standard approaches have been demonstrated.},
    author = {Cosar, S. and Bellotto, N.},
    doi = {10.1007/s10846-019-01026-w},
    journal = {JOURNAL OF INTELLIGENT &amp; ROBOTIC SYSTEMS},
    keywords = {Service robots; Re-identification; Elderly care; Thermal camera; Occlusion; Body motion},
    number = {1},
    pages = {85--102},
    publisher = {SPRINGER},
    title = {Human Re-Identification with a Robot Thermal Camera Using Entropy-Based Sampling},
    volume = {98},
    year = {2020}
}

@article{Lock_2020_Spatialised_Audio_Interface,
    author = {Lock, Jacobus C. and Gilchrist, Iain D. and Gilchrist, Iain D. and Cielniak, Grzegorz and Bellotto, Nicola},
    doi = {10.1145/3412325},
    issn = {1936-7236},
    journal = {ACM Transactions on Accessible Computing},
    month = {October},
    number = {4},
    pages = {1â€“21},
    publisher = {Association for Computing Machinery (ACM)},
    title = {Experimental Analysis of a Spatialised Audio Interface for People with Visual Impairments},
    url = {http://dx.doi.org/10.1145/3412325},
    volume = {13},
    year = {2020}
}

@article{Bellotto2009_Multisensorbased_Human_Detection,
    author = {Bellotto, N. and Hu, H.},
    journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
    number = {1},
    pages = {167-181},
    title = {Multisensor-based human detection and tracking for mobile service robots},
    volume = {39},
    year = {2009}
}

@article{Yan_2020_Autonomous_Floor_Scrubber,
    author = {Zhi Yan and Simon Schreiberhuber and Georg Halmetschlager and Tom Duckett and Markus Vincze and Nicola Bellotto},
    doi = {10.1007/s11370-020-00324-9},
    journal = {Intelligent Service Robotics},
    month = {jun},
    number = {3},
    pages = {403--417},
    publisher = {Springer Science and Business Media {LLC}},
    title = {Robot perception of static and dynamic objects with an autonomous floor scrubber},
    url = {https://doi.org/10.1007%2Fs11370-020-00324-9},
    volume = {13},
    year = {2020}
}

@article{Bellotto2007_Multisensor_Data_Fusion,
    author = {Bellotto, N. and Hu, O.},
    journal = {2007 IEEE International Conference on Robotics and Biomimetics, ROBIO},
    pages = {1494-1499},
    title = {Multisensor data fusion for joint people tracking and identification with a service robot},
    year = {2007}
}

@article{11577_3455194.4_Enrichme_Perception,
    abstract = {Recent technological advances enabled modern robots to become part of our daily life. In particular, assistive robotics emerged as an exciting research topic that can provide solutions to improve the quality of life of elderly and vulnerable people. This paper introduces the robotic platform developed in the ENRICHME project, with particular focus on its innovative perception and interaction capabilities. The project's main goal is to enrich the day-to-day experience of elderly people at home with technologies that enable health monitoring, complementary care, and social support. The paper presents several modules created to provide cognitive stimulation services for elderly users with mild cognitive impairments. The ENRICHME robot was tested in three pilot sites around Europe (Poland, Greece, and UK) and proven to be an effective assistant for the elderly at home.},
    author = {Cosar, Serhan and Fernandez-Carmona, Manuel and Agrigoroaie, Roxana and Pages, Jordi and Ferland, Francois and Zhao, Feng and Yue, Shigang and Bellotto, Nicola and Tapus, Adriana},
    doi = {10.1007/s12369-019-00614-y},
    journal = {INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS},
    keywords = {Assistive robotics; Robot perception; Human-robot interaction},
    number = {3},
    pages = {779--805},
    publisher = {SPRINGER},
    title = {ENRICHME: Perception and Interaction of an Assistive Robot for the Elderly at Home},
    volume = {12},
    year = {2020}
}

@article{Bellotto2017_Mobile_Robots_Ecmr,
    author = {Duckett, T. and Tapus, A. and Bellotto, N.},
    journal = {Robotics and Autonomous Systems},
    pages = {348},
    title = {Special Issue on the Seventh European Conference on Mobile Robots (ECMR?15)},
    volume = {91},
    year = {2017}
}

@article{Bellotto2009_Dstributed_Camera_System,
    author = {Bellotto, N. and Sommerlade, E. and Benfold, B. and Bibby, C. and Reid, I. and Roth, D. and Fern{\'a}ndez, C. and Gool, L.V. and Gonz{\'a}lez, J.},
    journal = {2009 3rd ACM/IEEE International Conference on Distributed Smart Cameras, ICDSC 2009},
    title = {A dstributed camera system for multi-resolution surveillance},
    year = {2009}
}

@article{Isakhani_2021_Micro_Aerial_Robots,
    author = {Hamid Isakhani and Nicola Bellotto and Qinbing Fu and Shigang Yue},
    doi = {10.1093/jcde/qwab040},
    journal = {Journal of Computational Design and Engineering},
    month = {aug},
    number = {5},
    pages = {1191--1203},
    publisher = {Oxford University Press ({OUP})},
    title = {Generative design and fabrication of a locust-inspired gliding wing prototype for micro aerial robots},
    url = {https://doi.org/10.1093%2Fjcde%2Fqwab040},
    volume = {8},
    year = {2021}
}

@conference{11577_3455030_Dataset_For_Action,
    abstract = {The development of autonomous robots for agriculture depends on a successful approach to recognize user needs as well as datasets reflecting the characteristics of the domain. Available datasets for 3D Action Recognition generally feature controlled lighting and framing while recording subjects from the front. They mostly reflect good recording conditions and therefore fail to account for the highly variable conditions the robot would have to work with in the field, e.g. when providing in-field logistic support for human fruit pickers as in our scenario. Existing work on Intention Recognition mostly labels plans or actions as intentions, but neither of those fully capture the extend of human intent. In this work, we argue for a holistic view on human Intention Recognition and propose a set of recording conditions, gestures and behaviors that better reflect the environment and conditions an agricultural robot might find itself in. We demonstrate the utility of the dataset by means of evaluating two human detection methods: Bounding boxes and skeleton extraction.},
    author = {Gabriel, A. and Cosar, S. and Bellotto, N. and Baxter, P.},
    booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    doi = {10.1007/978-3-030-23807-0_30},
    isbn = {978-3-030-23806-3},
    keywords = {Action Recognition; Agricultural robotics; Dataset; Human-robot interaction; Intention recognition},
    pages = {362--374},
    publisher = {Springer Verlag},
    title = {A Dataset for Action Recognition in the Wild},
    volume = {11649},
    year = {2019}
}

@article{Bellotto2018_Camera_Based_Physiological,
    author = {Cosar, S. and Yan, Z. and Zhao, F. and Lambrou, T. and Yue, S. and Bellotto, N.},
    journal = {Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
    pages = {5010-5013},
    title = {Thermal Camera Based Physiological Monitoring with an Assistive Robot},
    volume = {2018-July},
    year = {2018}
}

@article{Co_ar_2020_Enrichme_Perception,
    author = {CoÅŸar, Serhan and Fernandez-Carmona, Manuel and Agrigoroaie, Roxana and Pages, Jordi and Ferland, FranÃ§ois and Zhao, Feng and Yue, Shigang and Bellotto, Nicola and Tapus, Adriana},
    doi = {10.1007/s12369-019-00614-y},
    issn = {1875-4805},
    journal = {International Journal of Social Robotics},
    month = {February},
    number = {3},
    pages = {779â€“805},
    publisher = {Springer Science and Business Media LLC},
    title = {ENRICHME: Perception and Interaction of an Assistive Robot for the Elderly at Home},
    url = {http://dx.doi.org/10.1007/S12369-019-00614-Y},
    volume = {12},
    year = {2020}
}

@article{Bellotto2010_Mobile_Service_Robots,
    author = {Bellotto, N. and Hu, H.},
    journal = {International Journal of Social Robotics},
    number = {2},
    pages = {121-136},
    title = {A bank of unscented kalman filters for multimodal human perception with mobile service robots},
    volume = {2},
    year = {2010}
}

@conference{11577_3498541_Efficient_Causal_Discovery,
    author = {Castri, Luca and Mghames, Sariah and Bellotto, Nicola},
    booktitle = {Proceedings of Italian Conference on Robotics and Intelligent Machines (I-RIM 3D)},
    doi = {10.48550/arXiv.2310.14925},
    title = {Efficient Causal Discovery for Robotics Applications},
    year = {2023}
}

@conference{11577_3455196_Online_Transfer_Learning,
    abstract = {Human detection and tracking is an essential task for service robots, where the combined use of multiple sensors has potential advantages that are yet to be fully exploited. In this paper, we introduce a framework allowing a robot to learn a new 3D LiDAR-based human classifier from other sensors over time, taking advantage of a multisensor tracking system. The main innovation is the use of different detectors for existing sensors (i.e. RGB-D camera, 2D LiDAR) to train, online, a new 3D LiDAR-based human classifier based on a new "trajectory probability". Our framework uses this probability to check whether new detection belongs to a human trajectory, estimated by different sensors and/or detectors, and to learn a human classifier in a semi-supervised fashion. The framework has been implemented and tested on a real-world dataset collected by a mobile robot. We present experiments illustrating that our system is able to effectively learn from different sensors and from the environment, and that the performance of the 3D LiDAR-based human classification improves with the number of sensors/detectors used.},
    author = {Yan, Z and Sun, L and Ducketi, T and Bellotto, N},
    booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
    pages = {7635--7640},
    publisher = {IEEE},
    title = {Multisensor Online Transfer Learning for 3D LiDAR-based Human Detection with a Mobile Robot},
    year = {2018}
}

@conference{11577_3515926_Causal_Analysis_Framework,
    author = {Castri, L. and Beraldo, G. and Mghames, S. and Hanheide, M. and Bellotto, N.},
    booktitle = {Workshop on Causal Learning for Human-Robot Interaction (Causal-HRI), ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
    title = {ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot Interaction Applications},
    year = {2024}
}

@article{Bellotto2017_Portable_Navigation_System,
    author = {Lock, J. and Cielniak, G. and Bellotto, N.},
    journal = {AAAI Spring Symposium - Technical Report},
    pages = {395-400},
    title = {A portable navigation system with an adaptive multimodal interface for the blind},
    volume = {SS-17-01 - SS-17-08},
    year = {2017}
}

@article{Bellotto2017_Enrichme_Integration,
    author = {Bellotto, N. and Fernandez-Carmona, M. and Cosar, S.},
    journal = {AAAI Spring Symposium - Technical Report},
    pages = {657-664},
    title = {ENRICHME integration of ambient intelligence and robotics for AAL},
    volume = {SS-17-01 - SS-17-08},
    year = {2017}
}

@article{Bellotto2012_Cognitive_Visual_Tracking,
    author = {Bellotto, N. and Benfold, B. and Harland, H. and Nagel, H.-H. and Pirlo, N. and Reid, I. and Sommerlade, E. and Zhao, C.},
    journal = {Computer Vision and Image Understanding},
    number = {3},
    pages = {457-471},
    title = {Cognitive visual tracking and camera control},
    volume = {116},
    year = {2012}
}

@article{Bellotto2017_Online_Learning,
    author = {Yan, Z. and Duckett, T. and Bellotto, N.},
    journal = {IEEE International Conference on Intelligent Robots and Systems},
    pages = {864-871},
    title = {Online learning for human classification in 3D LiDAR-based tracking},
    volume = {2017-September},
    year = {2017}
}

@article{Bellotto2014_Swarm_Aggregation_Strategies,
    author = {Arvin, F. and Turgut, A.E. and Bellotto, N. and Yue, S.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {1-8},
    title = {Comparison of different cue-based swarm aggregation strategies},
    volume = {8794},
    year = {2014}
}

@article{11577_3455204_Dnbsplidarbased_Human_Detection,
    author = {Yan, Z. and Duckett, T. and Bellotto, N.},
    doi = {10.1007/s10514-019-09883-y},
    journal = {AUTONOMOUS ROBOTS},
    pages = {147--164},
    title = {Online learning for 3D&nbsp;LiDAR-based human detection: experimental analysis of point cloud clustering and classification methods},
    volume = {44},
    year = {2020}
}

@inbook{Bellotto_2018_Detection_And_Tracking,
    author = {Bellotto, Nicola and Cosar, Serhan and Yan, Zhi},
    booktitle = {Encyclopedia of Robotics},
    doi = {10.1007/978-3-642-41610-1_34-1},
    isbn = {9783642416101},
    pages = {1â€“10},
    publisher = {Springer Berlin Heidelberg},
    title = {Human Detection and Tracking},
    url = {http://dx.doi.org/10.1007/978-3-642-41610-1_34-1},
    year = {2018}
}

@article{Castri_2024_Candoit_Causal_Discovery,
    author = {Castri, Luca and Mghames, Sariah and Hanheide, Marc and Bellotto, Nicola},
    doi = {10.1002/aisy.202400181},
    issn = {2640-4567},
    journal = {Advanced Intelligent Systems},
    month = {November},
    number = {12},
    publisher = {Wiley},
    title = {CAnDOIT: Causal Discovery with Observational and Interventional Data from Time Series},
    url = {http://dx.doi.org/10.1002/aisy.202400181},
    volume = {6},
    year = {2024}
}

@article{Bellotto2015_Qualitative_Trajectory_Calculus,
    author = {Dondrup, C. and Bellotto, N. and Hanheide, M. and Eder, K. and Leonards, U.},
    journal = {Robotics},
    number = {1},
    pages = {63-102},
    title = {A computational model of human-robot spatial interactions based on a qualitative trajectory calculus},
    volume = {4},
    year = {2015}
}

@conference{11577_3549114_Complex_Intralogistic_Environments,
    author = {Stracca, E. and Rudenko, A. and Palmieri, L. and Salaris, P. and Castri, L. and Mazzi, N. and Rakcevic, V. and Vaskevicius, N. and Linder, T. and Bellotto, N. and Schreiter, T. and Zhu, Y. and Castellano-Quero, M. and Napolitano, O. and Stefanini, E. and Heuer, L. and Magnusson, M. and Swikir, A. and Lilienthal, A. J.},
    booktitle = {European Robotics Forum 2025},
    title = {DARKO-Nav: Hierarchical Risk- and Context-aware Robot Navigation in Complex Intralogistic Environments},
    year = {2025}
}

@conference{11577_3529302_Human_Motion_Prediction,
    author = {Mghames, Sariah and Castri, Luca and Hanheide, Marc and Bellotto, Nicola},
    booktitle = {IEEE International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE International Conference on Robotics, Automation and Mechatronics (RAM)},
    doi = {10.1109/cis-ram61939.2024.10672815},
    pages = {57--62},
    title = {neuROSym: Deployment and Evaluation of a ROS-based Neuro-Symbolic Model for Human Motion Prediction},
    year = {2024}
}

@incollection{Fu_2019_Vehicle_Driving_Scenarios,
    author = {Qinbing Fu and Nicola Bellotto and Huatian Wang and F. Claire Rind and Hongxin Wang and Shigang Yue},
    booktitle = {{IFIP} Advances in Information and Communication Technology},
    doi = {10.1007/978-3-030-19823-7_5},
    pages = {67--79},
    publisher = {Springer International Publishing},
    title = {A Visual Neural Network for Robust Collision Perception in Vehicle Driving Scenarios},
    url = {https://doi.org/10.1007%2F978-3-030-19823-7_5},
    year = {2019}
}

@conference{11577_3373517_Realtime_Object_Detection,
    abstract = {Object detection plays a crucial role in the development of Electronic Travel Aids (ETAs), capable to guide a person with visual impairments towards a target object in an unknown indoor environment. In such a scenario, the object detector runs on a mobile device (e.g. smartphone) and needs to be fast, accurate, and, most importantly, lightweight. Nowadays, Deep Neural Networks (DNN) have become the state-of-the-art solution for object detection tasks, with many works improving speed and accuracy by proposing new architectures or extending existing ones. A common strategy is to use deeper networks to get higher performance, but that leads to a higher computational cost which makes it impractical to integrate them on mobile devices with limited computational power. In this work we compare different object detectors to find a suitable candidate to be implemented on ETAs, focusing on lightweight models capable of working in real-time on mobile devices with a good accuracy. In particular, we select two models: SSD Lite with Mobilenet V2 and Tiny-DSOD. Both models have been tested on the popular OpenImage dataset and a new dataset, named L-CAS Office dataset, collected to further test modelsâ€™ performance and robustness in a real scenario inspired by the actual perception challenges of a user with visual impairments.},
    author = {Terreran, Matteo and Tramontano, Andrea G. and Lock, Jacobus C. and Ghidoni, Stefano and Bellotto, Nicola},
    booktitle = {IEEE International Conference on Image Processing, Applications and Systems, IPAS},
    doi = {10.1109/IPAS50080.2020.9334933},
    isbn = {978-1-7281-7575-1},
    keywords = {Object detection, real-time, electronic travel aid},
    pages = {89--95},
    title = {Real-time Object Detection using Deep Learning for helping People with Visual Impairments},
    year = {2020}
}

@article{Bellotto2016_Markov_Logic_Network,
    author = {Fernandez-Carmona, M. and Bellotto, N.},
    journal = {Proceedings - 12th International Conference on Intelligent Environments, IE 2016},
    pages = {136-143},
    title = {On-line inference comparison with Markov logic network engines for activity recognition in AAL environments},
    year = {2016}
}

@article{Bellotto2010_Computationally_Efficient_Solutions,
    author = {Bellotto, N. and Hu, H.},
    journal = {Autonomous Robots},
    number = {4},
    pages = {425-438},
    title = {Computationally efficient solutions for tracking people with a mobile robot: An experimental evaluation of Bayesian filters},
    volume = {28},
    year = {2010}
}
