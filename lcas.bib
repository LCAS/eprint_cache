@comment{<head>
<META HTTP-EQUIV="Refresh" CONTENT="0;URL=/servlet/useragent">
</head>}

@article{Lock_2020_Spatialised_Audio_Interface,
    author = {Jacobus C. Lock and Iain D. Gilchrist and Iain D. Gilchrist and Grzegorz Cielniak and Nicola Bellotto},
    doi = {10.1145/3412325},
    journal = {{ACM} Transactions on Accessible Computing},
    month = {oct},
    number = {4},
    pages = {1--21},
    publisher = {Association for Computing Machinery ({ACM})},
    title = {Experimental Analysis of a Spatialised Audio Interface for People with Visual Impairments},
    url = {https://doi.org/10.1145%2F3412325},
    volume = {13},
    year = {2020}
}

@article{Kunze_2018_Longterm_Robot_Autonomy,
    author = {Lars Kunze and Nick Hawes and Tom Duckett and Marc Hanheide and Tomas Krajnik},
    doi = {10.1109/lra.2018.2860628},
    journal = {{IEEE} Robotics and Automation Letters},
    month = {oct},
    number = {4},
    pages = {4023--4030},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    title = {Artificial Intelligence for Long-Term Robot Autonomy: A Survey},
    url = {https://doi.org/10.1109%2Flra.2018.2860628},
    volume = {3},
    year = {2018}
}

@inproceedings{Cielniak_Person_Identification,
    author = {G. Cielniak and T. Duckett},
    booktitle = {1st International Workshop on Robotic Sensing, 2003. {ROSE}{\textquotesingle} 03.},
    doi = {10.1109/rose.2003.1218704},
    publisher = {{IEEE}},
    title = {Person identification by mobile robots in indoor environments},
    url = {https://doi.org/10.1109%2Frose.2003.1218704}
}

@inproceedings{lirolem6937_Spatial_Contextaware_Personfollowing,
    abstract = {Domestic robots are in the focus of research in
terms of service providers in households and even as robotic
companion that share the living space with humans. A major
capability of mobile domestic robots that is joint exploration
of space. One challenge to deal with this task is how could we
let the robots move in space in reasonable, socially acceptable
ways so that it will support interaction and communication
as a part of the joint exploration. As a step towards this
challenge, we have developed a context-aware following behav-
ior considering these social aspects and applied these together
with a multi-modal person-tracking method to switch between
three basic following approaches, namely direction-following,
path-following and parallel-following. These are derived from
the observation of human-human following schemes and are
activated depending on the current spatial context (e.g. free
space) and the relative position of the interacting human.
A combination of the elementary behaviors is performed in
real time with our mobile robot in different environments.
First experimental results are provided to demonstrate the
practicability of the proposed approach.},
    author = {Fang Yuan and Marc Hanheide and Gerhard Sagerer},
    booktitle = {International Workshop on Cognition for Technical Systems},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c0fd8)},
    month = {December},
    note = {Domestic robots are in the focus of research in
terms of service providers in households and even as robotic
companion that share the living space with humans. A major
capability of mobile domestic robots that is joint exploration
of space. One challenge to deal with this task is how could we
let the robots move in space in reasonable, socially acceptable
ways so that it will support interaction and communication
as a part of the joint exploration. As a step towards this
challenge, we have developed a context-aware following behav-
ior considering these social aspects and applied these together
with a multi-modal person-tracking method to switch between
three basic following approaches, namely direction-following,
path-following and parallel-following. These are derived from
the observation of human-human following schemes and are
activated depending on the current spatial context (e.g. free
space) and the relative position of the interacting human.
A combination of the elementary behaviors is performed in
real time with our mobile robot in different environments.
First experimental results are provided to demonstrate the
practicability of the proposed approach.},
    title = {Spatial context-aware person-following for a domestic robot},
    url = {http://eprints.lincoln.ac.uk/6937/},
    year = {2008}
}

@article{de_Silva_2024_Vision_Based_Crop,
    author = {de Silva, Rajitha and Cielniak, Grzegorz and Gao, Junfeng},
    doi = {10.1016/j.compag.2023.108581},
    issn = {0168-1699},
    journal = {Computers and Electronics in Agriculture},
    month = {February},
    pages = {108581},
    publisher = {Elsevier BV},
    title = {Vision based crop row navigation under varying field conditions in arable fields},
    url = {http://dx.doi.org/10.1016/j.compag.2023.108581},
    volume = {217},
    year = {2024}
}

@article{Polvara_2023_Blt_Data_Set,
    author = {Polvara, Riccardo and Molina, Sergi and Hroob, Ibrahim and Papadimitriou, Alexios and Tsiolis, Konstantinos and Giakoumis, Dimitrios and Likothanassis, Spiridon and Tzovaras, Dimitrios and Cielniak, Grzegorz and Hanheide, Marc},
    doi = {10.1002/rob.22228},
    issn = {1556-4967},
    journal = {Journal of Field Robotics},
    month = {August},
    publisher = {Wiley},
    title = {Bacchus Long‐Term (BLT) data set: Acquisition of the agricultural multimodal BLT data set with automated robot deployment},
    url = {http://dx.doi.org/10.1002/rob.22228},
    year = {2023}
}

@inproceedings{lirolem8323_Automatic_Recognition_Performances,
    abstract = {Facial expressions are one important nonverbal communication cue, as they can provide feedback in conversations between people and also in human-robot interaction. This paper presents an evaluation of three standard pattern recognition techniques (active appearance models, gabor energy filters, and raw images) for facial feedback interpretation in terms of valence (success and failure) and compares the results to the human performance. The used database contains videos of people interacting with a robot by teaching the names of several objects to it. After teaching, the robot should term the objects correctly. The subjects reacted to its answer while showing spontaneous facial expressions, which were classified in this work. One main result is that an automatic classification of facial expressions in terms of valence using simple standard pattern recognition techniques is possible with an accuracy comparable to the average human classification rate, but with a high variance between different subjects, likewise to the human performance. \^A\\copyright 2010 IEEE.},
    address = {San Francisco, CA},
    author = {C. Lang and S. Wachsmuth and H. Wersing and Marc Hanheide},
    booktitle = {Conference of 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010},
    journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010},
    keywords = {ARRAY(0x7f43493bfc80)},
    month = {June},
    note = {Conference Code: 81678},
    pages = {79--85},
    title = {Facial expressions as feedback cue in human-robot interaction: a comparison between human and automatic recognition performances},
    url = {http://eprints.lincoln.ac.uk/8323/},
    year = {2010}
}

@article{Cielniak2019_Boneconduction_Audio_Interface,
    author = {Lock, J.C. and Gilchrist, I.D. and Cielniak, G. and Bellotto, N.},
    journal = {Communications in Computer and Information Science},
    pages = {542-553},
    title = {Bone-conduction audio interface to guide people with visual impairments},
    volume = {1122 CCIS},
    year = {2019}
}

@article{lirolem6710_Integrated_Recognition_Systems,
    abstract = {Object recognition is the ability of a system to relate visual stimuli to its knowledge of the world. Although humans perform this task effortlessly and without thinking about it, a general algorithmic solution has not yet been found. Recently, a shift from devising isolated recognition techniques towards integrated systems could be observed [Y. Aloimonos, Active vision revisited, in: Y. Aloimonos (Ed.), Active Perception, Lawrence Efibaum, 1993, pp. 1?18; H. Christensen, Cognitive (vision) systems, ERCIM News (April, 2003). 17?18]. The visual active memory (VAM) perspective refines this system view towards an interactive computational framework for recognition systems in human everyday environments. VAM is in line with the recently emerged Cognitive Vision paradigm [H. Christensen, Cognitive (vision) systems, ERCIM News (April, 2003). 17?18] which is concerned with vision systems that evaluate, gather and integrate contextual knowledge for visual analysis. It consists of active processes that generate knowledge by means of a tight cooperation of perception, reasoning, learning and prior models. In addition, VAM emphasizes the dynamic representation of gathered knowledge. The memory is assumed to be structured in a hierarchy of successive memory systems that mediate the modularly defined processing components of the recognition system. Recognition and learning take place in the stress field of objects, actions, activities, scene context, and user interaction. In this paper, we exemplify the VAM perspective by means of existing demonstrator systems. Assuming three different perspectives (biological foundation, system engineering, and computer vision), we will show that the VAM concept is central to the cognitive capabilities of the system and that it leads to a more general object recognition framework.},
    author = {Christian Bauckhage and Sven Wachsmuth and Marc Hanheide and S. Wrede and Gerhard Sagerer and G. Heidemann and H. Ritter},
    journal = {Image and Vision Computing},
    keywords = {ARRAY(0x7f43493c11b8)},
    month = {January},
    note = {Object recognition is the ability of a system to relate visual stimuli to its knowledge of the world. Although humans perform this task effortlessly and without thinking about it, a general algorithmic solution has not yet been found. Recently, a shift from devising isolated recognition techniques towards integrated systems could be observed [Y. Aloimonos, Active vision revisited, in: Y. Aloimonos (Ed.), Active Perception, Lawrence Efibaum, 1993, pp. 1?18; H. Christensen, Cognitive (vision) systems, ERCIM News (April, 2003). 17?18]. The visual active memory (VAM) perspective refines this system view towards an interactive computational framework for recognition systems in human everyday environments. VAM is in line with the recently emerged Cognitive Vision paradigm [H. Christensen, Cognitive (vision) systems, ERCIM News (April, 2003). 17?18] which is concerned with vision systems that evaluate, gather and integrate contextual knowledge for visual analysis. It consists of active processes that generate knowledge by means of a tight cooperation of perception, reasoning, learning and prior models. In addition, VAM emphasizes the dynamic representation of gathered knowledge. The memory is assumed to be structured in a hierarchy of successive memory systems that mediate the modularly defined processing components of the recognition system. Recognition and learning take place in the stress field of objects, actions, activities, scene context, and user interaction. In this paper, we exemplify the VAM perspective by means of existing demonstrator systems. Assuming three different perspectives (biological foundation, system engineering, and computer vision), we will show that the VAM concept is central to the cognitive capabilities of the system and that it leads to a more general object recognition framework.},
    number = {1},
    pages = {5--14},
    publisher = {elsevier},
    title = {The visual active memory perspective on integrated recognition systems},
    url = {http://eprints.lincoln.ac.uk/6710/},
    volume = {26},
    year = {2008}
}

@inproceedings{lirolem6926_Systemic_Interaction_Analysis,
    abstract = {Recent developments in robotics enable advanced human-robot interaction. Especially interactions of novice users with robots are often unpredictable and, therefore, demand for novel methods for the analysis of the interaction in systemic ways. We propose Systemic Interaction Analysis (SInA) as a method to jointly analyze system level and interaction level in an integrated manner using one tool. The approach allows us to trace back patterns that deviate from prototypical interaction sequences to the distinct system components of our autonomous robot. In this paper, we exemplarily apply the method to the analysis of the follow behavior of our domestic robot BIRON. The analysis is the basis to achieve our goal of improving human-robot interaction iteratively.},
    author = {Manja Lohse and Marc Hanheide and Katharina J. Rohlfing and Gerhard Sagerer},
    booktitle = {4th ACM/IEEE international conference on Human robot interaction - HRI '09},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c0f78)},
    month = {March},
    note = {Recent developments in robotics enable advanced human-robot interaction. Especially interactions of novice users with robots are often unpredictable and, therefore, demand for novel methods for the analysis of the interaction in systemic ways. We propose Systemic Interaction Analysis (SInA) as a method to jointly analyze system level and interaction level in an integrated manner using one tool. The approach allows us to trace back patterns that deviate from prototypical interaction sequences to the distinct system components of our autonomous robot. In this paper, we exemplarily apply the method to the analysis of the follow behavior of our domestic robot BIRON. The analysis is the basis to achieve our goal of improving human-robot interaction iteratively.},
    pages = {93--100},
    publisher = {ACM / IEEE},
    title = {Systemic interaction analysis (SInA) in HRI},
    url = {http://eprints.lincoln.ac.uk/6926/},
    year = {2009}
}

@article{Polvara_2021_Agricultural_Environments,
    author = {Polvara, Riccardo and Del Duchetto, Francesco and Neumann, Gerhard and Hanheide, Marc},
    doi = {10.1109/lra.2021.3094557},
    issn = {2377-3774},
    journal = {IEEE Robotics and Automation Letters},
    month = {October},
    number = {4},
    pages = {6577–6584},
    publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
    title = {Navigate-and-Seek: A Robotics Framework for People Localization in Agricultural Environments},
    url = {http://dx.doi.org/10.1109/LRA.2021.3094557},
    volume = {6},
    year = {2021}
}

@article{Polvara_2020_Rfid_Tags_Discovery,
    author = {Riccardo Polvara and Manuel Fernandez-Carmona and Gerhard Neumann and Marc Hanheide},
    doi = {10.1109/lra.2020.3001539},
    journal = {{IEEE} Robotics and Automation Letters},
    month = {jul},
    number = {3},
    pages = {4477--4484},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    title = {Next-Best-Sense: A Multi-Criteria Robotic Exploration Strategy for {RFID} Tags Discovery},
    url = {https://doi.org/10.1109%2Flra.2020.3001539},
    volume = {5},
    year = {2020}
}

@inproceedings{lirolem8365_Robot_George_Interactive,
    abstract = {The video presents the robot George learning visual concepts in dialogue with a tutor},
    author = {Michael Zillich and Kai Zhou and Danijel Skocaj and Matej Kristan and Alen Vrecko and Marko Mahnic and Miroslav Janicek and Geert-Jan M. Kruijff and Thomas Keller and Marc Hanheide and Nick Hawes},
    booktitle = {Proceedings of the 8th ACM/IEEE international conference on Human-robot interaction},
    keywords = {ARRAY(0x7f43493bf920)},
    month = {March},
    pages = {425},
    publisher = {IEEE Press},
    title = {Robot George: interactive continuous learning of visual concepts},
    url = {http://eprints.lincoln.ac.uk/8365/},
    year = {2013}
}

@article{Hu_2024_Attnestedunet_Sugar_Beet,
    author = {Hu, xinzhi and Jeon, Wang-Su and Cielniak, Grezgorz and Rhee, Sang-Yong},
    doi = {10.5391/ijfis.2024.24.1.1},
    issn = {2093-744X},
    journal = {INTERNATIONAL JOURNAL of FUZZY LOGIC and INTELLIGENT SYSTEMS},
    month = {March},
    number = {1},
    pages = {1–9},
    publisher = {Korean Institute of Intelligent Systems},
    title = {ATT-NestedUnet: Sugar Beet and Weed Detection Using Semantic Segmentation},
    url = {http://dx.doi.org/10.5391/ijfis.2024.24.1.1},
    volume = {24},
    year = {2024}
}

@article{Cielniak2014_Source_Racing_Game,
    author = {Blake, J. and Cielniak, G.},
    journal = {15th International Conference on Intelligent Games and Simulation, GAME-ON 2014},
    pages = {90-94},
    title = {Procedural generation of race tracks in an open source racing game},
    year = {2014}
}

@article{Cielniak2018_Semantic_Segmentation,
    author = {Jeon, W.-S. and Cielniak, G. and Rhee, S.-Y.},
    journal = {International Journal of Fuzzy Logic and Intelligent Systems},
    number = {3},
    pages = {196-203},
    title = {Semantic segmentation using trade-off and internal ensemble},
    volume = {18},
    year = {2018}
}

@inproceedings{lirolem11637_Humanrobot_Spatial_Interactions,
    abstract = {Despite the large number of navigation algorithms available for mobile robots, in many social contexts they often exhibit inopportune motion behaviours in proximity of people, often with very \"unnatural\" movements due to the execution of segmented trajectories or the sudden activation of safety mechanisms (e.g., for obstacle avoidance). We argue that the reason of the problem is not only the difficulty of modelling human behaviours and generating opportune robot control policies, but also the way human-robot spatial interactions are represented and implemented.
In this paper we propose a new methodology based on a qualitative representation of spatial interactions, which is both flexible and compact, adopting the well-defined and coherent formalization of Qualitative Trajectory Calculus (QTC). We show the potential of a QTC-based approach to abstract and design complex robot behaviours, where the desired robot's behaviour is represented together with its actual performance in one coherent approach, focusing on spatial interactions rather than pure navigation problems.},
    author = {Nicola Bellotto and Marc Hanheide and Nico Van de Weghe},
    booktitle = {International Conference on Social Robotics (ICSR)},
    keywords = {ARRAY(0x7f43493bf080)},
    month = {October},
    publisher = {Springer},
    title = {Qualitative design and implementation of human-robot spatial interactions},
    url = {http://eprints.lincoln.ac.uk/11637/},
    year = {2013}
}

@article{Hanheide_2017_Robot_Task_Planning,
    author = {Marc Hanheide and Moritz GÃ¶belbecker and Graham S. Horn and Andrzej Pronobis and Kristoffer SjÃ¶Ã¶ and Alper Aydemir and Patric Jensfelt and Charles Gretton and Richard Dearden and Miroslav Janicek and Hendrik Zender and Geert-Jan Kruijff and Nick Hawes and Jeremy L. Wyatt},
    doi = {10.1016/j.artint.2015.08.008},
    journal = {Artificial Intelligence},
    month = {jun},
    pages = {119--150},
    publisher = {Elsevier {BV}},
    title = {Robot task planning and explanation in open and uncertain worlds},
    url = {https://doi.org/10.1016%2Fj.artint.2015.08.008},
    volume = {247},
    year = {2017}
}

@inproceedings{lirolem6933_Moving_From_Augmented,
    abstract = {Recently1 there has been a growing interest in human
augmented mapping[1, 2]. That is: a mobile robot builds
a low level spatial representation of the environment based
on its sensor readings while a human provides labels for
human concepts, such as rooms, which are then augmented
or anchored to this representation or map [3]. Given such an
augmented map the robot has the ability to communicate with
the human about spatial concepts using the labels that the
human understand. For instance, the robot could report it is
in the ?kitchen?, instead of a set Cartesian coordinates which
are probably meaningless to the human.},
    author = {Olaf Booij and Ben Kr\"ose and Julia Peltason and Thorsten P. Spexard and Marc Hanheide},
    booktitle = {Robotics: Science and Systems Workshop on Interactive Robot Learning},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c10c8)},
    month = {June},
    note = {Recently1 there has been a growing interest in human
augmented mapping[1, 2]. That is: a mobile robot builds
a low level spatial representation of the environment based
on its sensor readings while a human provides labels for
human concepts, such as rooms, which are then augmented
or anchored to this representation or map [3]. Given such an
augmented map the robot has the ability to communicate with
the human about spatial concepts using the labels that the
human understand. For instance, the robot could report it is
in the ?kitchen?, instead of a set Cartesian coordinates which
are probably meaningless to the human.},
    title = {Moving from augmented to interactive mapping},
    url = {http://eprints.lincoln.ac.uk/6933/},
    year = {2008}
}

@article{Cielniak2018_Predicting_Rhythmic_Flow,
    author = {Molina, S. and Cielniak, G. and Krajn{\'i}k, T. and Duckett, T.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {135-146},
    title = {Modelling and predicting rhythmic flow patterns in dynamic environments},
    volume = {10965 LNAI},
    year = {2018}
}

@inproceedings{lirolem6750_Qualitative_Trajectory_Calculus,
    abstract = {The analysis and understanding of human-robot joint spatial behaviour (JSB) such as guiding, approaching, departing, or coordinating movements in narrow spaces and its communicative and dynamic aspects are key requirements on the road towards more intuitive interaction, safe encounter, and appealing living with mobile robots. This endeavours demand for appropriate models and methodologies to represent JSB and facilitate its analysis. In this paper, we adopt a qualitative trajectory calculus (QTC) as a formal foundation for the analysis and representation of such spatial behaviour of a human and a robot based on a compact encoding of the relative trajectories of two interacting agents in a sequential model. We present this QTC together with a distance measure and a probabilistic behaviour model and outline its usage in an actual JSB study.We argue that the proposed QTC coding scheme and derived methodologies for analysis and modelling are flexible and extensible to be adapted for a variety of other scenarios and studies. I.},
    author = {Marc Hanheide and Annika Peters and Nicola Bellotto},
    booktitle = {21st IEEE International Symposium on Robot and Human Interactive Communication},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493bf968)},
    month = {September},
    pages = {689--694},
    publisher = {IEEE},
    title = {Analysis of human-robot spatial behaviour applying a qualitative trajectory calculus},
    url = {http://eprints.lincoln.ac.uk/6750/},
    year = {2012}
}

@article{lirolem6700_Attention_Via_Synchrony,
    abstract = {Infants learning about their environment are confronted with many stimuli of different modalities. Therefore, a crucial problem is how to discover which stimuli are related, for instance, in learning words. In making these multimodal ldquobindings,rdquo infants depend on social interaction with a caregiver to guide their attention towards relevant stimuli. The caregiver might, for example, visually highlight an object by shaking it while vocalizing the object's name. These cues are known to help structuring the continuous stream of stimuli. To detect and exploit them, we propose a model of bottom-up attention by multimodal signal-level synchrony. We focus on the guidance of visual attention from audio-visual synchrony informed by recent adult-infant interaction studies. Consequently, we demonstrate that our model is receptive to parental cues during child-directed tutoring. The findings discussed in this paper are consistent with recent results from developmental psychology but for the first time are obtained employing an objective, computational model. The presence of ldquomultimodal mothereserdquo is verified directly on the audio-visual signal. Lastly, we hypothesize how our computational model facilitates tutoring interaction and discuss its application in interactive learning scenarios, enabling social robots to benefit from adult-like tutoring.},
    author = {Matthias Rolf and Marc Hanheide and Katharina J. Rohfling},
    journal = {Autonomous Mental Development, IEEE Transactions on},
    keywords = {ARRAY(0x7f43493c0f18)},
    month = {May},
    note = {Infants learning about their environment are confronted with many stimuli of different modalities. Therefore, a crucial problem is how to discover which stimuli are related, for instance, in learning words. In making these multimodal ldquobindings,rdquo infants depend on social interaction with a caregiver to guide their attention towards relevant stimuli. The caregiver might, for example, visually highlight an object by shaking it while vocalizing the object's name. These cues are known to help structuring the continuous stream of stimuli. To detect and exploit them, we propose a model of bottom-up attention by multimodal signal-level synchrony. We focus on the guidance of visual attention from audio-visual synchrony informed by recent adult-infant interaction studies. Consequently, we demonstrate that our model is receptive to parental cues during child-directed tutoring. The findings discussed in this paper are consistent with recent results from developmental psychology but for the first time are obtained employing an objective, computational model. The presence of ldquomultimodal mothereserdquo is verified directly on the audio-visual signal. Lastly, we hypothesize how our computational model facilitates tutoring interaction and discuss its application in interactive learning scenarios, enabling social robots to benefit from adult-like tutoring.},
    number = {1},
    pages = {55--67},
    publisher = {IEEE},
    title = {Attention via synchrony: making use of multimodal cues in social learning},
    url = {http://eprints.lincoln.ac.uk/6700/},
    volume = {1},
    year = {2009}
}

@article{Molina_2022_Human_Motion_Patterns,
    author = {Molina, Sergi and Cielniak, Grzegorz and Duckett, Tom},
    doi = {10.1109/tro.2021.3101358},
    issn = {1941-0468},
    journal = {IEEE Transactions on Robotics},
    month = {April},
    number = {2},
    pages = {1304–1318},
    publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
    title = {Robotic Exploration for Learning Human Motion Patterns},
    url = {http://dx.doi.org/10.1109/TRO.2021.3101358},
    volume = {38},
    year = {2022}
}

@incollection{Barnes_2010_Identification_In_Potatoes,
    author = {Michael Barnes and Grzegorz Cielniak and Tom Duckett},
    booktitle = {Computer Vision and Graphics},
    doi = {10.1007/978-3-642-15910-7_23},
    pages = {209--216},
    publisher = {Springer Berlin Heidelberg},
    title = {Minimalist {AdaBoost} for Blemish Identification in Potatoes},
    url = {https://doi.org/10.1007%2F978-3-642-15910-7_23},
    year = {2010}
}

@inproceedings{lirolem8353_Home_Alone_Autonomous,
    abstract = {In this paper we present an account of the problems faced by a mobile robot given an incomplete tour of an unknown environment, and introduce a collection of techniques which can generate successful behaviour even in the presence of such problems. Underlying our approach is the principle that an autonomous system must be motivated to act to gather new knowledge, and to validate and correct existing knowledge. This principle is embodied in Dora, a mobile robot which features the aforementioned techniques: shared representations, non-monotonic reasoning, and goal generation and management. To demonstrate how well this collection of techniques work in real-world situations we present a comprehensive analysis of the Dora system's performance over multiple tours in an indoor environment. In this analysis Dora successfully completed 18 of 21 attempted runs, with all but 3 of these successes requiring one or more of the integrated techniques to recover from problems. \^A\\copyright 2011 IEEE.},
    address = {Shanghai},
    author = {N. Hawes and Marc Hanheide and J. Hargreaves and B. Page and H. Zender and P. Jensfelt},
    booktitle = {Robotics and Automation (ICRA), 2011 IEEE International Conference on},
    journal = {Proceedings - IEEE International Conference on Robotics and Automation},
    keywords = {ARRAY(0x7f43493bfad0)},
    month = {May},
    note = {Conference of 2011 IEEE International Conference on Robotics and Automation, ICRA 2011; Conference Date: 9 May 2011 through 13 May 2011; Conference Code: 94261},
    pages = {3907--3914},
    publisher = {IEEE},
    title = {Home alone: autonomous extension and correction of spatial representations},
    url = {http://eprints.lincoln.ac.uk/8353/},
    year = {2011}
}

@inproceedings{lirolem6756_Exploiting_Probabilistic_Knowledge,
    abstract = {Robots must perform tasks efficiently and reli- ably while acting under uncertainty. One way to achieve efficiency is to give the robot common- sense knowledge about the structure of the world. Reliable robot behaviour can be achieved by mod- elling the uncertainty in the world probabilistically. We present a robot system that combines these two approaches and demonstrate the improvements in efficiency and reliability that result. Our first con- tribution is a probabilistic relational model integrat- ing common-sense knowledge about the world in general, with observations of a particular environ- ment. Our second contribution is a continual plan- ning system which is able to plan in the large prob- lems posed by that model, by automatically switch- ing between decision-theoretic and classical proce- dures. We evaluate our system on object search tasks in two different real-world indoor environ- ments. By reasoning about the trade-offs between possible courses of action with different informa- tional effects, and exploiting the cues and general structures of those environments, our robot is able to consistently demonstrate efficient and reliable goal-directed behaviour.},
    author = {Marc Hanheide and Charles Gretton and Richard W. Dearden and Nick A. Hawes and Jeremy L. Wyatt and Moritz Goedelbecker and Andrzej Pronobis and Alper Aydemir and Hendrik Zender},
    booktitle = {Twenty-Second International Joint Conference on Artificial Intelligence},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493bfa70)},
    month = {July},
    note = {Robots must perform tasks efficiently and reli- ably while acting under uncertainty. One way to achieve efficiency is to give the robot common- sense knowledge about the structure of the world. Reliable robot behaviour can be achieved by mod- elling the uncertainty in the world probabilistically. We present a robot system that combines these two approaches and demonstrate the improvements in efficiency and reliability that result. Our first con- tribution is a probabilistic relational model integrat- ing common-sense knowledge about the world in general, with observations of a particular environ- ment. Our second contribution is a continual plan- ning system which is able to plan in the large prob- lems posed by that model, by automatically switch- ing between decision-theoretic and classical proce- dures. We evaluate our system on object search tasks in two different real-world indoor environ- ments. By reasoning about the trade-offs between possible courses of action with different informa- tional effects, and exploiting the cues and general structures of those environments, our robot is able to consistently demonstrate efficient and reliable goal-directed behaviour.},
    pages = {2442--2449},
    publisher = {International Joint Conferences on Artiicial Intelligence},
    title = {Exploiting probabilistic knowledge under uncertain sensing for efficient robot behaviour},
    url = {http://eprints.lincoln.ac.uk/6756/},
    year = {2011}
}

@inproceedings{lirolem6939_Active_Visionbased_Localization,
    abstract = {Self-Localization is a crucial task for mobile robots. It is not only a requirement
for auto navigation but also provides contextual information to support
human robot interaction (HRI). In this paper we present an active vision-based
localization method for integration in a complex robot system to work in human
interaction scenarios (e.g. home-tour) in a real world apartment. The holistic
features used are robust to illumination and structural changes in the scene. The
system uses only a single pan-tilt camera shared between different vision applications
running in parallel to reduce the number of sensors. Additional information
from other modalities (like laser scanners) can be used, profiting of an integration
into an existing system. The camera view can be actively adapted and the
evaluation showed that different rooms can be discerned.},
    author = {Falk Schubert and Thorsten P. Spexard and Marc Hanheide and Sven Wachsmuth},
    booktitle = {5th International Conference on Computer Vision Systems (ICVS 2007)},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c12d8)},
    note = {Self-Localization is a crucial task for mobile robots. It is not only a requirement
for auto navigation but also provides contextual information to support
human robot interaction (HRI). In this paper we present an active vision-based
localization method for integration in a complex robot system to work in human
interaction scenarios (e.g. home-tour) in a real world apartment. The holistic
features used are robust to illumination and structural changes in the scene. The
system uses only a single pan-tilt camera shared between different vision applications
running in parallel to reduce the number of sensors. Additional information
from other modalities (like laser scanners) can be used, profiting of an integration
into an existing system. The camera view can be actively adapted and the
evaluation showed that different rooms can be discerned.},
    publisher = {Applied Computer Science Group, Bielefeld University, Germany},
    title = {Active vision-based localization for robots in a home-tour scenario},
    url = {http://eprints.lincoln.ac.uk/6939/},
    year = {2007}
}

@phdthesis{lirolem6748_Konturmodelle_In_Intensitatsbildern,
    abstract = {Abstract},
    author = {Marc Hanheide},
    keywords = {ARRAY(0x7f43493c15d8)},
    month = {July},
    note = {Abstract},
    school = {Universitat Bielefeld},
    title = {Objektbezogene 3D-Erkennung automatisch generierter Konturmodelle in Intensit\"atsbildern},
    url = {http://eprints.lincoln.ac.uk/6748/},
    year = {2001}
}

@article{Guevara_2023_Softfruit_Harvesting_Operations,
    author = {Guevara, Leonardo and Hanheide, Marc and Parsons, Simon},
    doi = {10.1002/rob.22227},
    issn = {1556-4967},
    journal = {Journal of Field Robotics},
    month = {July},
    publisher = {Wiley},
    title = {Implementation of a human‐aware robot navigation module for cooperative soft‐fruit harvesting operations},
    url = {http://dx.doi.org/10.1002/rob.22227},
    year = {2023}
}

@inproceedings{lirolem6943_Cognitive_Vision_System,
    abstract = {In this paper, we present a case study that exemplifies
general ideas of system integration and coordination.
The application field of assistant technology provides an
ideal test bed for complex computer vision systems including
real-time components, human-computer interaction, dynamic
3-d environments, and information retrieval aspects.
In our scenario the user is wearing an augmented reality device
that supports her/him in everyday tasks by presenting
information that is triggered by perceptual and contextual
cues. The system integrates a wide variety of visual functions
like localization, object tracking and recognition, action
recognition, interactive object learning, etc. We show
how different kinds of system behavior are realized using
the Active Memory Infrastructure that provides the technical
basis for distributed computation and a data- and eventdriven
integration approach.},
    author = {Sebastian Wrede and Marc Hanheide and Sven Wachsmuth and Gerhard Sagerer},
    booktitle = {Computer Vision Systems, 2006 ICVS '06. IEEE International Conference on},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1398)},
    month = {January},
    note = {In this paper, we present a case study that exemplifies
general ideas of system integration and coordination.
The application field of assistant technology provides an
ideal test bed for complex computer vision systems including
real-time components, human-computer interaction, dynamic
3-d environments, and information retrieval aspects.
In our scenario the user is wearing an augmented reality device
that supports her/him in everyday tasks by presenting
information that is triggered by perceptual and contextual
cues. The system integrates a wide variety of visual functions
like localization, object tracking and recognition, action
recognition, interactive object learning, etc. We show
how different kinds of system behavior are realized using
the Active Memory Infrastructure that provides the technical
basis for distributed computation and a data- and eventdriven
integration approach.},
    publisher = {IEEE},
    title = {Integration and coordination in a cognitive vision system},
    url = {http://eprints.lincoln.ac.uk/6943/},
    year = {2006}
}

@article{Treptow_2006_Realtime_People_Tracking,
    author = {Andr{\'{e}} Treptow and Grzegorz Cielniak and Tom Duckett},
    doi = {10.1016/j.robot.2006.04.013},
    journal = {Robotics and Autonomous Systems},
    month = {sep},
    number = {9},
    pages = {729--739},
    publisher = {Elsevier {BV}},
    title = {Real-time people tracking for mobile robots using thermal vision},
    url = {https://doi.org/10.1016%2Fj.robot.2006.04.013},
    volume = {54},
    year = {2006}
}

@article{Del_Duchetto_2022_Job_Longterm_Behavioural,
    author = {Del Duchetto, Francesco and Hanheide, Marc},
    doi = {10.1109/lra.2022.3178807},
    issn = {2377-3774},
    journal = {IEEE Robotics and Automation Letters},
    month = {July},
    number = {3},
    pages = {6934–6941},
    publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
    title = {Learning on the Job: Long-Term Behavioural Adaptation in Human-Robot Interactions},
    url = {http://dx.doi.org/10.1109/LRA.2022.3178807},
    volume = {7},
    year = {2022}
}

@phdthesis{lirolem6743_Cognitive_Egovision_System,
    abstract = {With increasing computational power and decreasing size, computers nowadays are already wearable and mobile. They become attendant of peoples' everyday life. Personal digital assistants and mobile phones equipped with adequate software gain a lot of interest in public, although the functionality they provide in terms of assistance is little more than a mobile databases for appointments, addresses, to-do lists and photos. Compared to the assistance a human can provide, such systems are hardly to call real assistants. The motivation to construct more human-like assistance systems that develop a certain level of cognitive capabilities leads to the exploration of two central paradigms in this work. The first paradigm is termed cognitive vision systems. Such systems take human cognition as a design principle of underlying concepts and develop learning and adaptation capabilities to be more flexible in their application. They are embodied, active, and situated. Second, the ego-vision paradigm is introduced as a very tight interaction scheme between a user and a computer system that especially eases close collaboration and assistance between these two. Ego-vision systems (EVS) take a user's (visual) perspective and integrate the human in the system's processing loop by means of a shared perception and augmented reality. EVSs adopt techniques of cognitive vision to identify objects, interpret actions, and understand the user's visual perception. And they articulate their knowledge and interpretation by means of augmentations of the user's own view. These two paradigms are studied as rather general concepts, but always with the goal in mind to realize more flexible assistance systems that closely collaborate with its users. This work provides three major contributions. First, a definition and explanation of ego-vision as a novel paradigm is given. Benefits and challenges of this paradigm are discussed as well. Second, a configuration of different approaches that permit an ego-vision system to perceive its environment and its user is presented in terms of object and action recognition, head gesture recognition, and mosaicing. These account for the specific challenges identified for ego-vision systems, whose perception capabilities are based on wearable sensors only. Finally, a visual active memory (VAM) is introduced as a flexible conceptual architecture for cognitive vision systems in general, and for assistance systems in particular. It adopts principles of human cognition to develop a representation for information stored in this memory. So-called memory processes continuously analyze, modify, and extend the content of this VAM. The functionality of the integrated system emerges from their coordinated interplay of these memory processes. An integrated assistance system applying the approaches and concepts outlined before is implemented on the basis of the visual active memory. The system architecture is discussed and some exemplary processing paths in this system are presented and discussed. It assists users in object manipulation tasks and has reached a maturity level that allows to conduct user studies. Quantitative results of different integrated memory processes are as well presented as an assessment of the interactive system by means of these user studies.},
    author = {Marc Hanheide},
    keywords = {ARRAY(0x7f43493c1308)},
    month = {October},
    note = {With increasing computational power and decreasing size, computers nowadays are already wearable and mobile. They become attendant of peoples' everyday life. Personal digital assistants and mobile phones equipped with adequate software gain a lot of interest in public, although the functionality they provide in terms of assistance is little more than a mobile databases for appointments, addresses, to-do lists and photos. Compared to the assistance a human can provide, such systems are hardly to call real assistants. The motivation to construct more human-like assistance systems that develop a certain level of cognitive capabilities leads to the exploration of two central paradigms in this work. The first paradigm is termed cognitive vision systems. Such systems take human cognition as a design principle of underlying concepts and develop learning and adaptation capabilities to be more flexible in their application. They are embodied, active, and situated. Second, the ego-vision paradigm is introduced as a very tight interaction scheme between a user and a computer system that especially eases close collaboration and assistance between these two. Ego-vision systems (EVS) take a user's (visual) perspective and integrate the human in the system's processing loop by means of a shared perception and augmented reality. EVSs adopt techniques of cognitive vision to identify objects, interpret actions, and understand the user's visual perception. And they articulate their knowledge and interpretation by means of augmentations of the user's own view. These two paradigms are studied as rather general concepts, but always with the goal in mind to realize more flexible assistance systems that closely collaborate with its users. This work provides three major contributions. First, a definition and explanation of ego-vision as a novel paradigm is given. Benefits and challenges of this paradigm are discussed as well. Second, a configuration of different approaches that permit an ego-vision system to perceive its environment and its user is presented in terms of object and action recognition, head gesture recognition, and mosaicing. These account for the specific challenges identified for ego-vision systems, whose perception capabilities are based on wearable sensors only. Finally, a visual active memory (VAM) is introduced as a flexible conceptual architecture for cognitive vision systems in general, and for assistance systems in particular. It adopts principles of human cognition to develop a representation for information stored in this memory. So-called memory processes continuously analyze, modify, and extend the content of this VAM. The functionality of the integrated system emerges from their coordinated interplay of these memory processes. An integrated assistance system applying the approaches and concepts outlined before is implemented on the basis of the visual active memory. The system architecture is discussed and some exemplary processing paths in this system are presented and discussed. It assists users in object manipulation tasks and has reached a maturity level that allows to conduct user studies. Quantitative results of different integrated memory processes are as well presented as an assessment of the interactive system by means of these user studies.},
    school = {Universitat Bielefeld},
    title = {A cognitive ego-vision system for interactive assistance},
    url = {http://eprints.lincoln.ac.uk/6743/},
    year = {2006}
}

@inproceedings{lirolem8314_Longterm_Socially_Perceptive,
    abstract = {This paper gives a brief overview of the challenges for multi-model perception and generation applied to robot companions located in human social environments. It reviews the current position in both perception and generation and the immediate technical challenges and goes on to consider the extra issues raised by embodiment and social context. Finally, it briefly discusses the impact of systems that must function continually over months rather than just for a few hours. \^A\\copyright 2011 ACM.},
    address = {Alicante},
    author = {Ruth S. Aylett and Ginevra Castellano and Bogdan Raducanu and Ana Paiva and Marc Hanheide},
    booktitle = {Conference of 2011 ACM International Conference on Multimodal Interaction, ICMI'11},
    journal = {ICMI'11 - Proceedings of the 2011 ACM International Conference on Multimodal Interaction},
    keywords = {ARRAY(0x7f43493bfa10)},
    month = {November},
    note = {Conference Code: 87685},
    pages = {323--326},
    publisher = {ACM},
    title = {Long-term socially perceptive and interactive robot companions: challenges and future perspective},
    url = {http://eprints.lincoln.ac.uk/8314/},
    year = {2011}
}

@inproceedings{lirolem8321_Remembering_Interaction_Episodes,
    abstract = {In this paper we will present a new approach to give a robot the capability to recognize already seen people and to remember details about past interactions. These details are time, length, location(GPS) and involved people of one interaction. Furthermore all features of this system work unsupervised. This means that the robot itself decides e.g. when and which person is important to remember or when an interaction starts. Out of these collected data additional information can be learned. For example a social network is build up which contains how often different people were seen together in the same interaction. \^A\\copyright2010 IEEE.},
    address = {Nashville, TN},
    author = {S. Gieselmann and Marc Hanheide and B. Wrede},
    booktitle = {Conference of 2010 10th IEEE-RAS International Conference on Humanoid Robots, Humanoids 2010},
    journal = {2010 10th IEEE-RAS International Conference on Humanoid Robots, Humanoids 2010},
    keywords = {ARRAY(0x7f43493bfb30)},
    month = {December},
    note = {Conference Code: 83761},
    pages = {566--571},
    publisher = {IEEE},
    title = {Remembering interaction episodes: an unsupervised learning approach for a humanoid robot},
    url = {http://eprints.lincoln.ac.uk/8321/},
    year = {2010}
}

@article{Atas_2024_Kinodynamic_Motion_Planner,
    author = {Atas, Fetullah and Cielniak, Grzegorz and Grimstad, Lars},
    doi = {10.4173/mic.2024.1.2},
    issn = {1890-1328},
    journal = {Modeling, Identification and Control: A Norwegian Research Bulletin},
    number = {1},
    pages = {15–28},
    publisher = {Norwegian Society of Automatic Control},
    title = {CostTrust: A Fast-Exploring, Iteratively Expanding Frontier-Based Kinodynamic Motion Planner},
    url = {http://dx.doi.org/10.4173/mic.2024.1.2},
    volume = {45},
    year = {2024}
}

@inproceedings{lirolem6936_Planar_Surface_Extraction,
    abstract = {This paper presents a new method for detecting
and merging redundant points in registered range data.
Given a global representation from sequences of 3D
points, the points are projected onto a virtual image
plane computed from the intrinsic parameters of the
sensor. Candidates for redundancy are collected per
pixel which then are clustered locally via region growing and replaced by the cluster?s mean value. As data is
provided in a certain manner defined by camera characteristics, this processing step preserves the structural
information of the data. For evaluation, our approach is
compared to two other algorithms. Applied to two dif-
ferent sequences, it is shown that the presented method
gives smooth results within planar regions of the point
clouds by successfully reducing noise and redundancy
and thus improves registered range data.},
    author = {Agnes Swadzba and Anna-Lisa Vollmer and Marc Hanheide and Sven Wachsmuth},
    booktitle = {19th International Conference on Pattern Recognition},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1008)},
    month = {December},
    note = {This paper presents a new method for detecting
and merging redundant points in registered range data.
Given a global representation from sequences of 3D
points, the points are projected onto a virtual image
plane computed from the intrinsic parameters of the
sensor. Candidates for redundancy are collected per
pixel which then are clustered locally via region growing and replaced by the cluster?s mean value. As data is
provided in a certain manner defined by camera characteristics, this processing step preserves the structural
information of the data. For evaluation, our approach is
compared to two other algorithms. Applied to two dif-
ferent sequences, it is shown that the presented method
gives smooth results within planar regions of the point
clouds by successfully reducing noise and redundancy
and thus improves registered range data.},
    publisher = {IEEE / The International Association for Pattern Recognition (IAPR)},
    title = {Reducing noise and redundancy in registered range data for planar surface extraction},
    url = {http://eprints.lincoln.ac.uk/6936/},
    year = {2008}
}

@article{Herrero_2018_Customers_Automated_Analysis,
    author = {Roberto Pinillos Herrero and Jaime Pulido Fentanes and Marc Hanheide},
    doi = {10.1109/lra.2018.2856264},
    journal = {{IEEE} Robotics and Automation Letters},
    month = {oct},
    number = {4},
    pages = {3733--3740},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    title = {Getting to Know Your Robot Customers: Automated Analysis of User Identity and Demographics for Robots in the Wild},
    url = {https://doi.org/10.1109%2Flra.2018.2856264},
    volume = {3},
    year = {2018}
}

@inproceedings{lirolem8325_Multimodal_Emotion_Recognition,
    abstract = {Recognition of emotions from multimodal cues is of basic interest for the design of many adaptive interfaces in human-machine and human-robot interaction. It provides a means to incorporate non-verbal feedback in the interactional course. Humans express their emotional state rather unconsciously exploiting their different natural communication modalities. In this paper, we present a first study on multimodal recognition of emotions from auditive and visual cues for interaction interfaces. We recognize seven classes of basic emotions by means of visual analysis of talking faces. In parallel, the audio signal is analyzed on the basis of the intonation of the verbal articulation. We compare the performance of state of the art recognition systems on the DaFEx database for both complement modalities and discuss these results with regard to the theoretical background and possible fusion schemes in real-world multimodal interfaces. \^A\\copyright 2009 IEEE.},
    address = {Dubai},
    author = {A. Rabie and B. Wrede and T. Vogt and M. Hanheide},
    booktitle = {ICCEE '09. Second International Conference on Computer and Electrical Engineering},
    journal = {2009 International Conference on Computer and Electrical Engineering, ICCEE 2009},
    keywords = {ARRAY(0x7f43493c0fa8)},
    note = {Conference Code: 79725},
    pages = {598--602},
    publisher = {IEEE},
    title = {Evaluation and discussion of multi-modal emotion recognition},
    url = {http://eprints.lincoln.ac.uk/8325/},
    volume = {1},
    year = {2009}
}

@inproceedings{lirolem6931_Evaluating_Extrovert,
    abstract = {Human-robot interaction (HRI) research is here presented into social robots that have to be able to interact with inexperienced users. In the design of these robots many research findings of human-human interaction and human-computer interaction are adopted but the direct applicability of these theories is limited because a robot is different from both humans and computers. Therefore, new methods have to be developed in HRI in order to build robots that are suitable for inexperienced users. In this paper we present a video study we conducted employing our robot BIRON (Bielefeld robot companion) which is designed for use in domestic environments. Subjects watched the system during the interaction with a human and rated two different robot behaviours (extrovert and introvert). The behaviours differed regarding verbal output and person following of the robot. Aiming to improve human-robot interaction, participantspsila ratings of the behaviours were evaluated and compared.},
    author = {Manja Lohse and Marc Hanheide and Britta Wrede and Michael L. Walters and Kheng Lee Koay and Dag Sverre Syrdal and Anders Green and Helge Huttenrauch and Kerstin Dautenhahn and Gerhard Sagerer and Kerstin Severinson-Eklundh},
    booktitle = {The 17th IEEE International Symposium on Robot and Human Interactive Communication},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1098)},
    month = {August},
    note = {Human-robot interaction (HRI) research is here presented into social robots that have to be able to interact with inexperienced users. In the design of these robots many research findings of human-human interaction and human-computer interaction are adopted but the direct applicability of these theories is limited because a robot is different from both humans and computers. Therefore, new methods have to be developed in HRI in order to build robots that are suitable for inexperienced users. In this paper we present a video study we conducted employing our robot BIRON (Bielefeld robot companion) which is designed for use in domestic environments. Subjects watched the system during the interaction with a human and rated two different robot behaviours (extrovert and introvert). The behaviours differed regarding verbal output and person following of the robot. Aiming to improve human-robot interaction, participantspsila ratings of the behaviours were evaluated and compared.},
    pages = {488--493},
    publisher = {IEEE},
    title = {Evaluating extrovert and introvert behaviour of a domestic robot -- a video study},
    url = {http://eprints.lincoln.ac.uk/6931/},
    year = {2008}
}

@article{Hroob_2024_Stable_Points_Segmentation,
    author = {Hroob, Ibrahim and Mersch, Benedikt and Stachniss, Cyrill and Hanheide, Marc},
    doi = {10.1109/lra.2024.3368236},
    issn = {2377-3774},
    journal = {IEEE Robotics and Automation Letters},
    month = {April},
    number = {4},
    pages = {3546–3553},
    publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
    title = {Generalizable Stable Points Segmentation for 3D LiDAR Scan-to-Map Long-Term Localization},
    url = {http://dx.doi.org/10.1109/LRA.2024.3368236},
    volume = {9},
    year = {2024}
}

@article{Cielniak_2010_Visionbased_People_Tracking,
    author = {Grzegorz Cielniak and Tom Duckett and Achim J. Lilienthal},
    doi = {10.1016/j.robot.2010.02.004},
    journal = {Robotics and Autonomous Systems},
    month = {may},
    number = {5},
    pages = {435--443},
    publisher = {Elsevier {BV}},
    title = {Data association and occlusion handling for vision-based people tracking by mobile robots},
    url = {https://doi.org/10.1016%2Fj.robot.2010.02.004},
    volume = {58},
    year = {2010}
}

@article{lirolem6562_Expectations_Intentions,
    abstract = {From the issue entitled \"Expectations, Intentions \& Actions\"
Human-robot interaction is becoming increasingly complex
through the growing number of abilities, both cognitive and
physical, available to today?s robots. At the same time, interaction is still often dif?cult because the users do not understand the robots? internal states, expectations, intentions, and
actions. Vice versa, robots lack understanding of the users?
expectations, intentions, actions, and social signals.},
    author = {M. Hanheide and M. Lohse and H. Zender},
    journal = {Internation Journal of Social Robotics},
    keywords = {ARRAY(0x7f43493bf9b0)},
    month = {April},
    note = {From the issue entitled \"Expectations, Intentions \& Actions\"
Human-robot interaction is becoming increasingly complex
through the growing number of abilities, both cognitive and
physical, available to today?s robots. At the same time, interaction is still often dif?cult because the users do not understand the robots? internal states, expectations, intentions, and
actions. Vice versa, robots lack understanding of the users?
expectations, intentions, actions, and social signals.},
    number = {2},
    pages = {107--108},
    publisher = {Springer},
    title = {Expectations, intentions, and actions in human-robot interaction},
    url = {http://eprints.lincoln.ac.uk/6562/},
    volume = {4},
    year = {2012}
}

@inproceedings{Treptow_2005_Active_People_Recognition,
    author = {A. Treptow and G. Cielniak and T. Duckett},
    booktitle = {2005 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
    doi = {10.1109/iros.2005.1545530},
    publisher = {{IEEE}},
    title = {Active people recognition using thermal and grey images on a mobile security robot},
    url = {https://doi.org/10.1109%2Firos.2005.1545530},
    year = {2005}
}

@article{Cielniak2003_Utilizing_Motion_Patterns,
    author = {Cielniak, G. and Bennewitz, M. and Burgard, W.},
    journal = {IJCAI International Joint Conference on Artificial Intelligence},
    pages = {909-914},
    title = {Where is...? Learning and utilizing motion patterns of persons with mobile robots},
    year = {2003}
}

@inproceedings{lirolem6925_System_Integration_Supporting,
    abstract = {Abstract With robotic systems entering our daily life, they have to become more flexible and subsuming a multitude of abilities in one single integrated system. Subsequently an increased extensibility of the robots? system architectures is needed. 
The goal is to facilitate a long-time evolution of the integrated system in-line with the scientific progress on the algorithmic level. In this paper we present an approach developed for an event-driven robot architecture, focussing on the coordination and interplay of new abilities and components. Appropriate timing, sequencing strategies, execution guaranties, and process flow synchronization are taken into account to allow appropriate arbitration and interaction between components as well as between the integrated system and the user. The presented approach features dynamic reconfiguration and global coordination based on simple production rules. These are applied fist time in conjunction with flexible representations in global memory spaces and an event-driven architecture. As a result a highly adaptive robot control compared to alternative approaches is achieved, allowing system modification during runtime even within complex interactive human-robot scenarios},
    author = {Thorsten P. Spexard and Marc Hanheide},
    booktitle = {Conference on Human Centered Robotic Systems},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493bfce0)},
    month = {November},
    note = {Abstract With robotic systems entering our daily life, they have to become more flexible and subsuming a multitude of abilities in one single integrated system. Subsequently an increased extensibility of the robots? system architectures is needed. 
The goal is to facilitate a long-time evolution of the integrated system in-line with the scientific progress on the algorithmic level. In this paper we present an approach developed for an event-driven robot architecture, focussing on the coordination and interplay of new abilities and components. Appropriate timing, sequencing strategies, execution guaranties, and process flow synchronization are taken into account to allow appropriate arbitration and interaction between components as well as between the integrated system and the user. The presented approach features dynamic reconfiguration and global coordination based on simple production rules. These are applied fist time in conjunction with flexible representations in global memory spaces and an event-driven architecture. As a result a highly adaptive robot control compared to alternative approaches is achieved, allowing system modification during runtime even within complex interactive human-robot scenarios},
    pages = {1--9},
    publisher = {Springer},
    title = {System integration supporting evolutionary development and design},
    url = {http://eprints.lincoln.ac.uk/6925/},
    year = {2009}
}

@article{Cielniak2013_Preface,
    author = {Cherino, C. and Cielniak, G. and Dickinson, P.},
    journal = {19th European Concurrent Engineering Conference 2013, ECEC 2013 - 9th Future Business Technology Conference, FUBUTEC 2013},
    pages = {XI},
    title = {Preface},
    year = {2013}
}

@inbook{Vayakkattil_2023_Dynamic_Environment,
    author = {Vayakkattil, Srikishan and Cielniak, Grzegorz and Calisti, Marcello},
    booktitle = {Lecture Notes in Computer Science},
    doi = {10.1007/978-3-031-43360-3_1},
    isbn = {9783031433603},
    issn = {1611-3349},
    pages = {3–14},
    publisher = {Springer Nature Switzerland},
    title = {Plant Phenotyping Using DLT Method: Towards Retrieving the Delicate Features in a Dynamic Environment},
    url = {http://dx.doi.org/10.1007/978-3-031-43360-3_1},
    year = {2023}
}

@article{Del_Duchetto_2018_Learning_Local_Recovery,
    author = {Del Duchetto, Francesco and Kucukyilmaz, Ayse and Iocchi, Luca and Hanheide, Marc},
    doi = {10.1109/lra.2018.2861080},
    issn = {2377-3774},
    journal = {IEEE Robotics and Automation Letters},
    month = {October},
    number = {4},
    pages = {4084–4091},
    publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
    title = {Do Not Make the Same Mistakes Again and Again: Learning Local Recovery Policies for Navigation From Human Demonstrations},
    url = {http://dx.doi.org/10.1109/LRA.2018.2861080},
    volume = {3},
    year = {2018}
}

@article{Badiee_2021_Soil_Moisture_Measurement,
    author = {Amir Badiee and John R. Wallbank and Jaime Pulido Fentanes and Emily Trill and Peter Scarlet and Yongchao Zhu and Grzegorz Cielniak and Hollie Cooper and James R. Blake and Jonathan G. Evans and Marek Zreda and Markus KÃ¶hli and Simon Pearson},
    doi = {10.1029/2020wr028478},
    journal = {Water Resources Research},
    month = {jun},
    number = {6},
    publisher = {American Geophysical Union ({AGU})},
    title = {Using Additional Moderator to Control the Footprint of a {COSMOS} Rover for Soil Moisture Measurement},
    url = {https://doi.org/10.1029%2F2020wr028478},
    volume = {57},
    year = {2021}
}

@article{Cielniak2019_Selective_Broccoli_Harvesting,
    author = {Montes, H.A. and Cielniak, G. and Duckett, T.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {448-459},
    title = {Model-Based 3D Point Cloud Segmentation for Automated Selective Broccoli Harvesting},
    volume = {11649 LNAI},
    year = {2019}
}

@inproceedings{lirolem6940_Joint_Environment_Exploration,
    abstract = {An important goal for research on service robots
is the cooperation of a human and a robot as team. A
service robot in a domestic environment needs to build a
representation of its future workspace that corresponds to
the human user's understanding of these surroundings. But
it also needs to apply this model about the \"where\" and
\"what\" in its current interaction to allow communication about
objects and places in a human-adequate way. In this paper
we present the integration of a hierarchical robotic mapping
system into an interactive framework controlled by a dialog
system. The goal is to use interactively acquired environment
models to implement a robot with interaction aware behaviors.
A major contribution of this work is a three-level hierarchy of
spatial representation affecting three different communication
dimensions. This hierarchy is consequently applied in the design
of the grounding-based dialog, laser-based topological mapping,
and an objects attention system. We demonstrate the benefits
of this integration for learning and tour guiding in a humancomprehensible
interaction between a robot and its user in
a home-tour scenario. The enhanced interaction capabilities
are crucial for developing a new generation of robots that
will be accepted not only as service robots but also as robot
companions.},
    author = {Thorsten Spexard and Shuyin Li and Britta Wrede and Marc Hanheide and Elin A. Topp and Helge Huttenrauch},
    booktitle = {RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c12a8)},
    month = {August},
    note = {An important goal for research on service robots
is the cooperation of a human and a robot as team. A
service robot in a domestic environment needs to build a
representation of its future workspace that corresponds to
the human user's understanding of these surroundings. But
it also needs to apply this model about the \"where\" and
\"what\" in its current interaction to allow communication about
objects and places in a human-adequate way. In this paper
we present the integration of a hierarchical robotic mapping
system into an interactive framework controlled by a dialog
system. The goal is to use interactively acquired environment
models to implement a robot with interaction aware behaviors.
A major contribution of this work is a three-level hierarchy of
spatial representation affecting three different communication
dimensions. This hierarchy is consequently applied in the design
of the grounding-based dialog, laser-based topological mapping,
and an objects attention system. We demonstrate the benefits
of this integration for learning and tour guiding in a humancomprehensible
interaction between a robot and its user in
a home-tour scenario. The enhanced interaction capabilities
are crucial for developing a new generation of robots that
will be accepted not only as service robots but also as robot
companions.},
    pages = {546--551},
    title = {Interaction awareness for joint environment exploration},
    url = {http://eprints.lincoln.ac.uk/6940/},
    year = {2007}
}

@inbook{Rogers_2023_Spraying_Evaluation_System,
    author = {Rogers, Harry and De La Iglesia, Beatriz and Zebin, Tahmina and Cielniak, Grzegorz and Magri, Ben},
    booktitle = {Lecture Notes in Computer Science},
    doi = {10.1007/978-3-031-43360-3_3},
    isbn = {9783031433603},
    issn = {1611-3349},
    pages = {26–37},
    publisher = {Springer Nature Switzerland},
    title = {An Automated Precision Spraying Evaluation System},
    url = {http://dx.doi.org/10.1007/978-3-031-43360-3_3},
    year = {2023}
}

@article{Cielniak2016_Dvision_Based_Detection,
    author = {Kusumam, K. and Krajn{\'i}k, T. and Pearson, S. and Cielniak, G. and Duckett, T.},
    journal = {IEEE International Conference on Intelligent Robots and Systems},
    pages = {646-651},
    title = {Can you pick a broccoli? 3D-vision based detection and localisation of broccoli heads in the field},
    volume = {2016-November},
    year = {2016}
}

@inproceedings{lirolem13462_Facial_Communicative_Signal,
    abstract = {Facial communicative signals (FCSs) such as head gestures, eye gaze, and facial expressions can provide useful feedback in conversations between people and also in human-robot interaction. This paper presents a pattern recognition approach for the interpretation of FCSs in terms of valence, based on the selection of discriminative subsequences in video data. These subsequences capture important temporal dynamics and are used as prototypical reference subsequences in a classification procedure based on dynamic time warping and feature extraction with active appearance models. Using this valence classification, the robot can discriminate positive from negative interaction situations and react accordingly. The approach is evaluated on a database containing videos of people interacting with a robot by teaching the names of several objects to it. The verbal answer of the robot is expected to elicit the display of spontaneous FCSs by the human tutor, which were classified in this work. The achieved classification accuracies are comparable to the average human recognition performance and outperformed our previous results on this task. \^A\\copyright 2013 IEEE.},
    address = {Karlsruhe},
    author = {C. Lang and S. Wachsmuth and M. Hanheide and H. Wersing},
    booktitle = {IEEE International Conference on Robotics and Automation, ICRA 2013},
    keywords = {ARRAY(0x7f43493bf938)},
    note = {Conference Code:100673},
    pages = {170--177},
    publisher = {IEEE},
    title = {Facial communicative signal interpretation in human-robot interaction by discriminative video subsequence selection},
    url = {http://eprints.lincoln.ac.uk/13462/},
    year = {2013}
}

@inproceedings{Dickinson_2016_Bluetooth_Low_Energy,
    author = {Patrick Dickinson and Gregorz Cielniak and Olivier Szymanezyk and Mike Mannion},
    booktitle = {2016 International Conference on Indoor Positioning and Indoor Navigation ({IPIN})},
    doi = {10.1109/ipin.2016.7743684},
    month = {oct},
    publisher = {{IEEE}},
    title = {Indoor positioning of shoppers using a network of Bluetooth Low Energy beacons},
    url = {https://doi.org/10.1109%2Fipin.2016.7743684},
    year = {2016}
}

@article{lirolem6741_Humanoriented_Interaction,
    abstract = {A very important aspect in developing robots capable of human-robot interaction (HRI) is the research in natural, human-like communication, and subsequently, the development of a research platform with multiple HRI capabilities for evaluation. Besides a flexible dialog system and speech understanding, an anthropomorphic appearance has the potential to support intuitive usage and understanding of a robot, e.g., human-like facial expressions and deictic gestures can as well be produced and also understood by the robot. As a consequence of our effort in creating an anthropomorphic appearance and to come close to a human- human interaction model for a robot, we decided to use human-like sensors, i.e., two cameras and two microphones only, in analogy to human perceptual capabilities too. Despite the challenges resulting from these limits with respect to perception, a robust attention system for tracking and interacting with multiple persons simultaneously in real time is presented. The tracking approach is sufficiently generic to work on robots with varying hardware, as long as stereo audio data and images of a video camera are available. To easily implement different interaction capabilities like deictic gestures, natural adaptive dialogs, and emotion awareness on the robot, we apply a modular integration approach utilizing XML-based data exchange. The paper focuses on our efforts to bring together different interaction concepts and perception capabilities integrated on a humanoid robot to achieve comprehending human-oriented interaction.},
    author = {Thorsten P. Spexard and Marc Hanheide and Gerhard Sagerer},
    journal = {Robotics, IEEE Transactions on},
    keywords = {ARRAY(0x7f43493c1278)},
    month = {October},
    note = {A very important aspect in developing robots capable of human-robot interaction (HRI) is the research in natural, human-like communication, and subsequently, the development of a research platform with multiple HRI capabilities for evaluation. Besides a flexible dialog system and speech understanding, an anthropomorphic appearance has the potential to support intuitive usage and understanding of a robot, e.g., human-like facial expressions and deictic gestures can as well be produced and also understood by the robot. As a consequence of our effort in creating an anthropomorphic appearance and to come close to a human- human interaction model for a robot, we decided to use human-like sensors, i.e., two cameras and two microphones only, in analogy to human perceptual capabilities too. Despite the challenges resulting from these limits with respect to perception, a robust attention system for tracking and interacting with multiple persons simultaneously in real time is presented. The tracking approach is sufficiently generic to work on robots with varying hardware, as long as stereo audio data and images of a video camera are available. To easily implement different interaction capabilities like deictic gestures, natural adaptive dialogs, and emotion awareness on the robot, we apply a modular integration approach utilizing XML-based data exchange. The paper focuses on our efforts to bring together different interaction concepts and perception capabilities integrated on a humanoid robot to achieve comprehending human-oriented interaction.},
    number = {5},
    pages = {852--862},
    publisher = {IEEE},
    title = {Human-oriented interaction with an anthropomorphic robot},
    url = {http://eprints.lincoln.ac.uk/6741/},
    volume = {23},
    year = {2007}
}

@inproceedings{lirolem6935_Wrong_Error_Detection,
    abstract = {A matter of course for the researchers and developers of state-of-the-art technology for human-computer- or human-robot-interaction is to create not only systems that can precisely fulfill a certain task. They must provide a strong robustness against internal and external errors or user-dependent application errors. Especially when creating service robots for a variety of applications or robots for accompanying humans in everyday situations sufficient error robustness is crucial for acceptance by users. But experience unveils that operating such systems under real world conditions with unexperienced users is an extremely challenging task which still is not solved satisfactorily. In this paper we will present an approach for handling both internal errors and application errors within an integrated system capable of performing extended HRI on different robotic platforms and in unspecified surroundings like a real world apartment. Based on the gathered experience from user studies and evaluating integrated systems in the real world, we implemented several ways to generalize and handle unexpected situations. Adding such a kind of error awareness to HRI systems in cooperation with the interaction partner avoids to get stuck in an unexpected situation or state and handle mode confusion. Instead of shouldering the enormous effort to account for all possible problems, this paper proposes a more general solution and underpins this with findings from naive user studies. This enhancement is crucial for the development of a new generation of robots as despite diligent preparations might be made, no one can predict how an interaction with a robotic system will develop and which kind of environment it has to cope with.},
    author = {Thorsten P. Spexard and Marc Hanheide and Shuyin Li and Britta Wrede},
    booktitle = {ICRA Workshop on Social Interaction with Intelligent Indoor Robots (2008)},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c10f8)},
    month = {May},
    note = {A matter of course for the researchers and developers of state-of-the-art technology for human-computer- or human-robot-interaction is to create not only systems that can precisely fulfill a certain task. They must provide a strong robustness against internal and external errors or user-dependent application errors. Especially when creating service robots for a variety of applications or robots for accompanying humans in everyday situations sufficient error robustness is crucial for acceptance by users. But experience unveils that operating such systems under real world conditions with unexperienced users is an extremely challenging task which still is not solved satisfactorily. In this paper we will present an approach for handling both internal errors and application errors within an integrated system capable of performing extended HRI on different robotic platforms and in unspecified surroundings like a real world apartment. Based on the gathered experience from user studies and evaluating integrated systems in the real world, we implemented several ways to generalize and handle unexpected situations. Adding such a kind of error awareness to HRI systems in cooperation with the interaction partner avoids to get stuck in an unexpected situation or state and handle mode confusion. Instead of shouldering the enormous effort to account for all possible problems, this paper proposes a more general solution and underpins this with findings from naive user studies. This enhancement is crucial for the development of a new generation of robots as despite diligent preparations might be made, no one can predict how an interaction with a robotic system will develop and which kind of environment it has to cope with.},
    title = {Oops, something is wrong - error detection and recovery for advanced human-robot-interaction},
    url = {http://eprints.lincoln.ac.uk/6935/},
    year = {2008}
}

@inproceedings{Zhu_2023_Autonomous_Topological_Optimisation,
    author = {Zhu, Zuyuan and Das, Gautham and Hanheide, Marc},
    booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
    collection = {SAC ’23},
    doi = {10.1145/3555776.3577666},
    month = {March},
    publisher = {ACM},
    series = {SAC ’23},
    title = {Autonomous Topological Optimisation for Multi-robot Systems in Logistics},
    url = {http://dx.doi.org/10.1145/3555776.3577666},
    year = {2023}
}

@inproceedings{lirolem6946_Cognitive_Vision_System,
    abstract = {The emerging cognitive vision paradigm is concerned
with vision systems that evaluate, gather and integrate con-
textual knowledge for visual analysis. In reasoning about
events and structures, cognitive vision systems should rely
on multiple computations in order to perform robustly even
in noisy domains. Action recognition in an unconstrained
office environment thus provides an excellent testbed for re-
search on cognitive computer vision. In this contribution,
we present a system that consists of several computational
modules for object and action recognition. It applies atten-
tion mechanisms, visual learning and contextual as well as
probabilistic reasoning to fuse individual results and verify
their consistency. Database technologies are used for infor-
mation storage and an XML based communication frame-
work integrates all modules into a consistent architecture.},
    author = {Christian Bauckhage and Marc Hanheide and Sebastian Wrede and Gerhard Sagerer},
    booktitle = {2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c15a8)},
    month = {April},
    note = {The emerging cognitive vision paradigm is concerned
with vision systems that evaluate, gather and integrate con-
textual knowledge for visual analysis. In reasoning about
events and structures, cognitive vision systems should rely
on multiple computations in order to perform robustly even
in noisy domains. Action recognition in an unconstrained
office environment thus provides an excellent testbed for re-
search on cognitive computer vision. In this contribution,
we present a system that consists of several computational
modules for object and action recognition. It applies atten-
tion mechanisms, visual learning and contextual as well as
probabilistic reasoning to fuse individual results and verify
their consistency. Database technologies are used for infor-
mation storage and an XML based communication frame-
work integrates all modules into a consistent architecture.},
    pages = {827--833},
    publisher = {IEEE},
    title = {A cognitive vision system for action recognition in office environments},
    url = {http://eprints.lincoln.ac.uk/6946/},
    year = {2004}
}

@article{Cielniak2019_Pedestrian_Flow_Patterns,
    author = {Molina, S. and Cielniak, G. and Duckett, T.},
    journal = {Proceedings - IEEE International Conference on Robotics and Automation},
    pages = {9725-9731},
    title = {Go with the flow: Exploration and mapping of pedestrian flow patterns from partial observations},
    volume = {2019-May},
    year = {2019}
}

@article{Pulido_Fentanes_2019_Krigingbased_Robotic_Exploration,
    author = {Pulido Fentanes, Jaime and Badiee, Amir and Duckett, Tom and Evans, Jonathan and Pearson, Simon and Cielniak, Grzegorz},
    doi = {10.1002/rob.21914},
    issn = {1556-4967},
    journal = {Journal of Field Robotics},
    month = {September},
    number = {1},
    pages = {122–136},
    publisher = {Wiley},
    title = {Kriging‐based robotic exploration for soil moisture mapping using a cosmic‐ray sensor},
    url = {http://dx.doi.org/10.1002/rob.21914},
    volume = {37},
    year = {2019}
}

@incollection{Moreno_2014_Agricultural_Spraying_Vehicles,
    author = {Francisco-Angel Moreno and Grzegorz Cielniak and Tom Duckett},
    booktitle = {Towards Autonomous Robotic Systems},
    doi = {10.1007/978-3-662-43645-5_22},
    pages = {210--221},
    publisher = {Springer Berlin Heidelberg},
    title = {Evaluation of Laser Range-Finder Mapping for Agricultural Spraying Vehicles},
    url = {https://doi.org/10.1007%2F978-3-662-43645-5_22},
    year = {2014}
}

@incollection{lirolem6718_Industrial_Car_Assembly,
    abstract = {Abstract. Quality assurance programs of today?s car manufacturers show increasing demand for automated visual inspection tasks. A typical example is just-in-time checking of assemblies along production lines. Since high throughput must be achieved, object recognition and pose estimation heavily rely on offline preprocessing stages of available CAD data. In this paper, we propose a complete, universal framework for CAD model feature extraction and entropy index based viewpoint selection that is developed in cooperation with a major german car manufacturer.},
    author = {Dirk Stoessel and Marc Hanheide and Gerhard Sagerer and Lars Kruger},
    booktitle = {Pattern recognition},
    editor = {Carl Edward Rasmussen and Heinrich H. B\"ulthoff and Bernhard Sch\"olkopf and Martin A. Giese},
    keywords = {ARRAY(0x7f43493c14e8)},
    month = {December},
    note = {Abstract. Quality assurance programs of today?s car manufacturers show increasing demand for automated visual inspection tasks. A typical example is just-in-time checking of assemblies along production lines. Since high throughput must be achieved, object recognition and pose estimation heavily rely on offline preprocessing stages of available CAD data. In this paper, we propose a complete, universal framework for CAD model feature extraction and entropy index based viewpoint selection that is developed in cooperation with a major german car manufacturer.},
    number = {3175},
    pages = {528--535},
    publisher = {Springer},
    series = {Lecture Notes in Computer Science},
    title = {Feature and viewpoint selection for industrial car assembly},
    url = {http://eprints.lincoln.ac.uk/6718/},
    year = {2004}
}

@inproceedings{lirolem13523_Qualitative_Trajectory_Calculus,
    abstract = {In this paper we propose a probabilistic model for Human-Robot Spatial Interaction (HRSI) using a Qualitative Trajectory Calculus (QTC). In particular, we will build on previous work representing HRSI as a Markov chain of QTC states and evolve this to an approach using a Hidden Markov Model representation. Our model accounts for the invalidity of certain transitions within the QTC to reduce the complexity of the probabilistic model and to ensure state sequences in accordance to this representational framework. We show the appropriateness of our approach by using the probabilistic model to encode different HRSI behaviours observed in a human-robot interaction study and show how the models can be used to classify these behaviours reliably. Copyright \^A\\copyright 2014, Association for the Advancement of Artificial Intelligence. All rights reserved.},
    author = {Christian Dondrup and Marc Hanheide and Nicola Bellotto},
    booktitle = {AAAI Spring Symposium: \"Qualitative Representations for Robots\"},
    keywords = {ARRAY(0x7f43493b2878)},
    month = {March},
    publisher = {AAAI / AI Access Foundation},
    title = {A probabilistic model of human-robot spatial interaction using a qualitative trajectory calculus
},
    url = {http://eprints.lincoln.ac.uk/13523/},
    year = {2014}
}

@article{Cielniak2018_Rasberry_Robotic,
    author = {From, P.J. and Grimstad, L. and Hanheide, M. and Pearson, S. and Cielniak, G.},
    journal = {Mechanical Engineering},
    number = {6},
    pages = {14-18},
    title = {RASberry: Robotic and autonomous systems: For berry production},
    volume = {140},
    year = {2018}
}

@inproceedings{lirolem6945_Interactive_Model_Acquisition,
    abstract = {Systems that perform in real environments need to bind the internal state to externally
perceived objects, events, or complete scenes. How to learn this correspondence has been a long
standing problem in computer vision as well as artificial intelligence. Augmented Reality provides
an interesting perspective on this problem because a human user can directly relate displayed
system results to real environments. In the following we present a system that is able to bootstrap
internal models from user-system interactions. Starting from pictorial representations it learns
symbolic object labels that provide the basis for storing observed episodes. In a second step, more
complex relational information is extracted from stored episodes that enables the system to react
on specific scene contexts.},
    author = {Sven Wachsmuth and Marc Hanheide and Sebastian Wrede and Christian Bauckhage},
    booktitle = {KI 2005 Workshop on Mixed-reality as a Challenge to Image Understanding and Artificial Intelligence},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1428)},
    month = {September},
    note = {Systems that perform in real environments need to bind the internal state to externally
perceived objects, events, or complete scenes. How to learn this correspondence has been a long
standing problem in computer vision as well as artificial intelligence. Augmented Reality provides
an interesting perspective on this problem because a human user can directly relate displayed
system results to real environments. In the following we present a system that is able to bootstrap
internal models from user-system interactions. Starting from pictorial representations it learns
symbolic object labels that provide the basis for storing observed episodes. In a second step, more
complex relational information is extracted from stored episodes that enables the system to react
on specific scene contexts.},
    pages = {41--46},
    title = {From images via symbols to contexts: using augmented reality for interactive model acquisition},
    url = {http://eprints.lincoln.ac.uk/6945/},
    year = {2005}
}

@inproceedings{lirolem6929_Automatic_Initialization,
    abstract = {The human face plays an important role in communication as it allows to discern different interaction partners and provides non-verbal feedback. In this paper, we present a soft real-time vision system that enables an interactive robot to analyze faces of interaction partners not only to identify them, but also to recognize their respective facial expressions as a dialog-controlling non-verbal cue. In order to assure applicability in real world environments, a robust detection scheme is presented which detects faces and basic facial features such as the position of the mouth, nose, and eyes. Based on these detected features, facial parameters are extracted using active appearance models (AAMs) and conveyed to support vector machine (SVM) classifiers to identify both persons and facial expressions. This paper focuses on four different initialization methods for determining the initial shape for the AAM algorithm and their particular performance in two different classification tasks with respect to either the facial expression DaFEx database and to the real world data obtained from a robot?s point of view.},
    author = {Ahmad Rabie and Christian Lang and Marc Hanheide and Modesto Castrillon-Santana and Gerhard Sagerer},
    booktitle = {6th International Conference, ICVS 2008},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1158)},
    month = {May},
    note = {The human face plays an important role in communication as it allows to discern different interaction partners and provides non-verbal feedback. In this paper, we present a soft real-time vision system that enables an interactive robot to analyze faces of interaction partners not only to identify them, but also to recognize their respective facial expressions as a dialog-controlling non-verbal cue. In order to assure applicability in real world environments, a robust detection scheme is presented which detects faces and basic facial features such as the position of the mouth, nose, and eyes. Based on these detected features, facial parameters are extracted using active appearance models (AAMs) and conveyed to support vector machine (SVM) classifiers to identify both persons and facial expressions. This paper focuses on four different initialization methods for determining the initial shape for the AAM algorithm and their particular performance in two different classification tasks with respect to either the facial expression DaFEx database and to the real world data obtained from a robot?s point of view.},
    pages = {517--526},
    publisher = {Springer},
    title = {Automatic initialization for facial analysis in interactive robotics},
    url = {http://eprints.lincoln.ac.uk/6929/},
    year = {2008}
}

@inproceedings{lirolem6932_Realtime_Object_Tracking,
    abstract = {This paper introduces a generic architecture for the fusion of perceptual processes and its application in real-time object tracking. In this architecture, the well known anchoring approach is, by integrating techniques from information fusion, extended to multi-modal anchoring so as to be applicable in a multi-process environment. The system architecture is designed to be applicable in a generic way, independent of specific application domains and of the characteristics of the underlying sensory processes. It is shown that, by combining multiple independent video-based detection methods, the generic multi-modal anchoring approach can be successfully employed for real-time person tracking in difficult environments},
    author = {Kai J\"ungling and Michael Arens and Marc Hanheide and Gerhard Sagerer},
    booktitle = {11th International Conference on Information Fusion},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1188)},
    month = {April},
    note = {This paper introduces a generic architecture for the fusion of perceptual processes and its application in real-time object tracking. In this architecture, the well known anchoring approach is, by integrating techniques from information fusion, extended to multi-modal anchoring so as to be applicable in a multi-process environment. The system architecture is designed to be applicable in a generic way, independent of specific application domains and of the characteristics of the underlying sensory processes. It is shown that, by combining multiple independent video-based detection methods, the generic multi-modal anchoring approach can be successfully employed for real-time person tracking in difficult environments},
    pages = {1--8},
    publisher = {IEEE},
    title = {Fusion of perceptual processes for real-time object tracking},
    url = {http://eprints.lincoln.ac.uk/6932/},
    year = {2008}
}

@article{Cielniak2010_Group_Emotion_Modelling,
    author = {Szymanezyk, O. and Cielniak, G.},
    journal = {Proceedings of the 3rd International Symposium on AI and Games - A Symposium at the AISB 2010 Convention},
    pages = {25-30},
    title = {Group emotion modelling and the use of middleware for virtual crowds in video-games},
    year = {2010}
}

@article{Bosilj_2019_Crops_Versus_Weeds,
    author = {Bosilj, Petra and Aptoula, Erchan and Duckett, Tom and Cielniak, Grzegorz},
    doi = {10.1002/rob.21869},
    issn = {1556-4967},
    journal = {Journal of Field Robotics},
    month = {March},
    number = {1},
    pages = {7–19},
    publisher = {Wiley},
    title = {Transfer learning between crop types for semantic segmentation of crops versus weeds in precision agriculture},
    url = {http://dx.doi.org/10.1002/rob.21869},
    volume = {37},
    year = {2019}
}

@article{Bennewitz_2005_Compliant_Robot_Motion,
    author = {Maren Bennewitz and Wolfram Burgard and Grzegorz Cielniak and Sebastian Thrun},
    doi = {10.1177/0278364904048962},
    journal = {The International Journal of Robotics Research},
    month = {jan},
    number = {1},
    pages = {31--48},
    publisher = {{SAGE} Publications},
    title = {Learning Motion Patterns of People for Compliant Robot Motion},
    url = {https://doi.org/10.1177%2F0278364904048962},
    volume = {24},
    year = {2005}
}

@article{Barnes_2010_Minimalist_Boosted_Classifiers,
    author = {Michael Barnes and Tom Duckett and Grzegorz Cielniak and Graeme Stroud and Glyn Harper},
    doi = {10.1016/j.jfoodeng.2010.01.010},
    journal = {Journal of Food Engineering},
    month = {jun},
    number = {3},
    pages = {339--346},
    publisher = {Elsevier {BV}},
    title = {Visual detection of blemishes in potatoes using minimalist boosted classifiers},
    url = {https://doi.org/10.1016%2Fj.jfoodeng.2010.01.010},
    volume = {98},
    year = {2010}
}

@inproceedings{lirolem6764_Home_Alone_Autonomous,
    abstract = {In this paper we present an account
of the problems faced by a mobile robot given
an incomplete tour of an unknown environment,
and introduce a collection of techniques which can
generate successful behaviour even in the presence
of such problems. Underlying our approach is the
principle that an autonomous system must be motivated
to act to gather new knowledge, and to validate
and correct existing knowledge. This principle is
embodied in Dora, a mobile robot which features
the aforementioned techniques: shared representations,
non-monotonic reasoning, and goal generation
and management. To demonstrate how well this
collection of techniques work in real-world situations
we present a comprehensive analysis of the Dora
system?s performance over multiple tours in an indoor
environment. In this analysis Dora successfully
completed 18 of 21 attempted runs, with all but
3 of these successes requiring one or more of the
integrated techniques to recover from problems.},
    author = {Nick Hawes and Marc Hanheide and Jack Hargreaves and Ben Page and Hendrik Zender and Patric Jensfelt},
    booktitle = {2011 IEEE International Conference on Robotics and Automation (ICRA)},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493bfaa0)},
    month = {May},
    note = {In this paper we present an account
of the problems faced by a mobile robot given
an incomplete tour of an unknown environment,
and introduce a collection of techniques which can
generate successful behaviour even in the presence
of such problems. Underlying our approach is the
principle that an autonomous system must be motivated
to act to gather new knowledge, and to validate
and correct existing knowledge. This principle is
embodied in Dora, a mobile robot which features
the aforementioned techniques: shared representations,
non-monotonic reasoning, and goal generation
and management. To demonstrate how well this
collection of techniques work in real-world situations
we present a comprehensive analysis of the Dora
system?s performance over multiple tours in an indoor
environment. In this analysis Dora successfully
completed 18 of 21 attempted runs, with all but
3 of these successes requiring one or more of the
integrated techniques to recover from problems.},
    pages = {3907--3914},
    publisher = {IEEE},
    title = {Home alone: autonomous extension and correction of spatial
representations},
    url = {http://eprints.lincoln.ac.uk/6764/},
    year = {2011}
}

@incollection{Gyebi_2017_Integrating_Educational_Robotic,
    author = {Ernest B. B. Gyebi and Marc Hanheide and Grzegorz Cielniak},
    booktitle = {Educational Robotics in the Makers Era},
    doi = {10.1007/978-3-319-55553-9_6},
    pages = {73--87},
    publisher = {Springer International Publishing},
    title = {The Effectiveness of Integrating Educational Robotic Activities into Higher Education Computer Science Curricula: A Case Study in a Developing Country},
    url = {https://doi.org/10.1007%2F978-3-319-55553-9_6},
    year = {2017}
}

@article{Cielniak_2013_Undergraduate_Computer_Science,
    author = {Grzegorz Cielniak and Nicola Bellotto and Tom Duckett},
    doi = {10.1109/te.2012.2213822},
    journal = {{IEEE} Transactions on Education},
    month = {feb},
    number = {1},
    pages = {48--53},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    title = {Integrating Mobile Robotics and Vision With Undergraduate Computer Science},
    url = {https://doi.org/10.1109%2Fte.2012.2213822},
    volume = {56},
    year = {2013}
}

@inbook{Kirk_2021_Occlusions_With_Reidentification,
    author = {Kirk, Raymond and Mangan, Michael and Cielniak, Grzegorz},
    booktitle = {Computer Vision Systems},
    doi = {10.1007/978-3-030-87156-7_17},
    isbn = {9783030871567},
    issn = {1611-3349},
    pages = {211–222},
    publisher = {Springer International Publishing},
    title = {Robust Counting of Soft Fruit Through Occlusions with Re-identification},
    url = {http://dx.doi.org/10.1007/978-3-030-87156-7_17},
    year = {2021}
}

@inproceedings{lirolem14893_Spatiotemporal_Representation,
    abstract = {The FP-7 Integrated Project STRANDS [1] is aimed at producing intelligent mobile robots that are able to operate robustly for months in dynamic human environments. To achieve long-term autonomy, the robots would need to understand the environment and how it changes over time. For that, we will have to develop novel approaches to extract 3D shapes, objects, people, and models of activity from sensor data gathered during months of autonomous operation.
So far, the environment models used in mobile robotics have been tailored to capture static scenes and environment variations are largely treated as noise. Therefore, utilization of the static models in ever-changing, real world environments is difficult. We propose to represent the environment?s spatio-temporal dynamics by its frequency spectrum.},
    author = {Tom Duckett and Marc Hanheide and Tomas Krajnik and Jaime Pulido Fentanes and Christian Dondrup},
    booktitle = {International IEEE/EPSRC Workshop on Autonomous Cognitive Robotics},
    keywords = {ARRAY(0x7f43493bf038)},
    month = {March},
    title = {Spatio-temporal representation for cognitive control in long-term scenarios},
    url = {http://eprints.lincoln.ac.uk/14893/},
    year = {2013}
}

@article{de_Silva_2023_Deep_Learningbased_Crop,
    author = {de Silva, Rajitha and Cielniak, Grzegorz and Wang, Gang and Gao, Junfeng},
    doi = {10.1002/rob.22238},
    issn = {1556-4967},
    journal = {Journal of Field Robotics},
    month = {August},
    publisher = {Wiley},
    title = {Deep learning‐based crop row detection for infield navigation of agri‐robots},
    url = {http://dx.doi.org/10.1002/rob.22238},
    year = {2023}
}

@inproceedings{Zaganidis_2017_Normal_Distributions_Transform,
    author = {Anestis Zaganidis and Martin Magnusson and Tom Duckett and Grzegorz Cielniak},
    booktitle = {2017 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
    doi = {10.1109/iros.2017.8206262},
    month = {sep},
    publisher = {{IEEE}},
    title = {Semantic-assisted 3D normal distributions transform for scan registration in environments with limited structure},
    url = {https://doi.org/10.1109%2Firos.2017.8206262},
    year = {2017}
}

@article{lirolem8317_Spatial_Attention_System,
    abstract = {Social interaction between humans takes place in the spatial dimension on a daily basis. We occupy space for ourselves and respect the dynamics of spaces that are occupied by others. In human-robot interaction, the focus has been on other topics so far. Therefore, this work applies a spatial model to a humanoid robot and implements an attention system that is connected to it. The resulting behaviors have been verified in an on-line video study. The questionnaire revealed that these behaviors are applicable and result in a robot that has been perceived as more interested in the human and shows its attention and intentions to a higher degree. \^A\\copyright 2010 Springer-Verlag.},
    address = {Singapore},
    author = {Patrick Holthaus and Ingo Lutkebohle and Marc Hanheide and Sven Wachsmuth},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    keywords = {ARRAY(0x7f43493bfb90)},
    month = {November},
    note = {Second International Conference on Social Robotics, ICSR 2010, Singapore, November 23-24, 2010. Conference Date: 23 November 2010 through 24 November 2010; Conference Code: 82714},
    pages = {325--334},
    publisher = {Springer},
    title = {Can I help you? A spatial attention system for a receptionist robot},
    url = {http://eprints.lincoln.ac.uk/8317/},
    volume = {6414 L},
    year = {2010}
}

@inproceedings{Onoufriou_2020_Time_Series_Forecasting,
    author = {George Onoufriou and   and Marc Hanheide and Georgios Leontidis and},
    booktitle = {{UKRAS}20 Conference: {\textquotedblleft}Robots into the real world{\textquotedblright} Proceedings},
    doi = {10.31256/qm1fu7l},
    month = {may},
    publisher = {{EPSRC} {UK}-{RAS} Network},
    title = {The Augmented Agronomist Pipeline and Time Series Forecasting},
    url = {https://doi.org/10.31256%2Fqm1fu7l},
    year = {2020}
}

@article{lirolem8339_Cognitive_Vision_System,
    abstract = {The European Cognitive Vision project VAMPIRE uses mobile AR-kits to interact with a visual active memory for teaching and retrieval purposes. This paper describes concept and technical realization of the used mobile AR-kits and discusses interactive learning and retrieval in office environments, and the active memory infrastructure. The focus is on 3D interaction for pointing in a scene coordinate system. This is achieved by 3D augmented pointing, which combines inside-out tracking for head pose recovery and 3D stereo human-computer interaction. Experimental evaluation shows that the accuracy of this 3D cursor is within a few centimeters, which is sufficient to point at an object in an office. Finally, an application of the cursor in VAMPIRE is presented, where in addition to the mobile system, at least one stationary active camera is used to obtain different views of an object. There are many potential applications, for example an improved view-based object recognition. \^A\\copyright 2006 Elsevier B.V. All rights reserved.},
    author = {H. Siegl and Marc Hanheide and S. Wrede and A. Pinz},
    journal = {Image and Vision Computing},
    keywords = {ARRAY(0x7f43493c11e8)},
    month = {December},
    number = {12},
    pages = {1895--1903},
    publisher = {Elsevier},
    title = {An augmented reality human-computer interface for object localization in a cognitive vision system},
    url = {http://eprints.lincoln.ac.uk/8339/},
    volume = {25},
    year = {2007}
}

@inproceedings{Barnes_2009_Boosting_Minimalist_Classifiers,
    author = {Michael Barnes and Tom Duckett and Grzegorz Cielniak},
    booktitle = {2009 24th International Conference Image and Vision Computing New Zealand},
    doi = {10.1109/ivcnz.2009.5378372},
    month = {nov},
    publisher = {{IEEE}},
    title = {Boosting minimalist classifiers for blemish detection in potatoes},
    url = {https://doi.org/10.1109%2Fivcnz.2009.5378372},
    year = {2009}
}

@inproceedings{lirolem6921_Concept_In_Hri,
    abstract = {Mobile robots are already applied in factories and hospitals, merely to do a distinct task. It is envisioned that robots assist in households, soon. Those service robots will have to cope with several situations and tasks and of course with sophisticated Human-Robot Interaction (HRI)},
    author = {Annika Peters and Thorsten P. Spexard and Petra Wei\ss and Marc Hanheide},
    booktitle = {Workshop on Behavior Monitoring and Interpretation - Well Being},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c0e28)},
    month = {September},
    note = {Mobile robots are already applied in factories and hospitals, merely to do a distinct task. It is envisioned that robots assist in households, soon. Those service robots will have to cope with several situations and tasks and of course with sophisticated Human-Robot Interaction (HRI)},
    title = {Make room for me: a spatial and situational movement concept in HRI},
    url = {http://eprints.lincoln.ac.uk/6921/},
    year = {2009}
}

@incollection{lirolem11964_System_Integration_Supporting,
    abstract = {With robotic systems entering our daily life, they have to become more flexible and subsuming a multitude of abilities in one single integrated system. Sub- sequently an increased extensibility of the robots? system architectures is needed. The goal is to facilitate a long-time evolution of the integrated system in-line with the scientific progress on the algorithmic level. In this paper we present an approach developed for an event-driven robot architecture, focussing on the coordination and interplay of new abilities and components. Appropriate timing, sequencing strategies, execution guaranties, and process flow synchronisation are taken into account to allow appropriate arbitration and interaction between components as well as between the integrated system and the user. The presented approach features dynamic reconfiguration and global coordination based on simple production rules. These are applied first time in conjunction with flexible representations in global memory spaces and an event-driven architecture. As a result a highly adaptive robot control compared to alternative approaches is achieved, allowing system modification during runtime even within complex interactive human-robot scenarios. },
    author = {Thorsten P. Spexard and Marc Hanheide},
    booktitle = {Human centered robot systems: cognition, interaction, technology },
    keywords = {ARRAY(0x7f43493c0d38)},
    month = {November},
    number = {6},
    pages = {1--9},
    publisher = {Springer Berlin Heidelberg},
    series = {Cognitive Systems Monographs},
    title = {System integration supporting evolutionary development and design},
    url = {http://eprints.lincoln.ac.uk/11964/},
    year = {2009}
}

@article{lirolem6744_Vision_Systems,
    abstract = {The emerging cognitive vision paradigm deals with vision systems that apply machine learning and automatic reasoning in order to learn from what they perceive. Cognitive vision systems can rate the relevance and consistency of newly acquired knowledge, they can adapt to their environment and thus will exhibit high robustness. This contribution presents vision systems that aim at flexibility and robustness. One is tailored for content-based image retrieval, the others are cognitive vision systems that constitute prototypes of visual active memories which evaluate, gather, and integrate contextual knowledge for visual analysis. All three systems are designed to interact with human users. After we will have discussed adaptive content-based image retrieval and object and action recognition in an office environment, the issue of assessing cognitive systems will be raised. Experiences from psychologically evaluated human-machine interactions will be reported and the promising potential of psychologically-based usability experiments will be stressed.},
    author = {Christian Bauckhage and Marc Hanheide and Sebastian Wrede and Thomas Kaster and Michael Pfeiffer and Gerhard Sagerer},
    journal = {EURASIP Journal on Applied Signal Processing},
    keywords = {ARRAY(0x7f43493c1458)},
    month = {August},
    note = {The emerging cognitive vision paradigm deals with vision systems that apply machine learning and automatic reasoning in order to learn from what they perceive. Cognitive vision systems can rate the relevance and consistency of newly acquired knowledge, they can adapt to their environment and thus will exhibit high robustness. This contribution presents vision systems that aim at flexibility and robustness. One is tailored for content-based image retrieval, the others are cognitive vision systems that constitute prototypes of visual active memories which evaluate, gather, and integrate contextual knowledge for visual analysis. All three systems are designed to interact with human users. After we will have discussed adaptive content-based image retrieval and object and action recognition in an office environment, the issue of assessing cognitive systems will be raised. Experiences from psychologically evaluated human-machine interactions will be reported and the promising potential of psychologically-based usability experiments will be stressed.},
    pages = {2375--2390},
    publisher = {Hindawi Publishing Corp},
    title = {Vision systems with the human in the loop},
    url = {http://eprints.lincoln.ac.uk/6744/},
    volume = {14},
    year = {2005}
}

@incollection{Ghidoni_2013_Texturebased_Crowd_Detection,
    author = {Stefano Ghidoni and Grzegorz Cielniak and Emanuele Menegatti},
    booktitle = {Advances in Intelligent Systems and Computing},
    doi = {10.1007/978-3-642-33926-4_69},
    pages = {725--736},
    publisher = {Springer Berlin Heidelberg},
    title = {Texture-Based Crowd Detection and Localisation},
    url = {https://doi.org/10.1007%2F978-3-642-33926-4_69},
    year = {2013}
}

@article{lirolem6699_Selfunderstanding_And_Selfextension,
    abstract = {There are many different approaches to building a system that can engage in autonomous mental development. In this paper we present an approach based on what we term em self-understanding, by which we mean the use of explicit representation of and reasoning about what a system does and doesn't know, and how that understanding changes under action. We present a coherent architecture and a set of representations used in two robot systems that exhibit a limited degree of autonomous mental development, what we term em self-extension. The contributions include: representations of gaps and uncertainty for specific kinds of knowledge, and a motivational and planning system for setting and achieving learning goals},
    author = {Jeremy L. Wyatt and Alper Aydemir and Michael Brenner and Marc Hanheide and Nick Hawes and Patric Jensfelt and Matej Kristan and Geert-Jan M. Kruijff and Pierre Lison and Andrzej Pronobis and Kristoffer Sjoo and Alen Vrecko and Hendrik Zender and Michael Zillich and Danijel Skocaj},
    journal = {Autonomous Mental Development, IEEE Transactions on},
    keywords = {ARRAY(0x7f43493bfb60)},
    month = {December},
    note = {There are many different approaches to building a system that can engage in autonomous mental development. In this paper we present an approach based on what we term em self-understanding, by which we mean the use of explicit representation of and reasoning about what a system does and doesn't know, and how that understanding changes under action. We present a coherent architecture and a set of representations used in two robot systems that exhibit a limited degree of autonomous mental development, what we term em self-extension. The contributions include: representations of gaps and uncertainty for specific kinds of knowledge, and a motivational and planning system for setting and achieving learning goals},
    number = {4},
    pages = {282--303},
    publisher = {IEEE},
    title = {Self-understanding and self-extension: a systems and representational approach},
    url = {http://eprints.lincoln.ac.uk/6699/},
    volume = {2},
    year = {2010}
}

@inproceedings{lirolem6924_Human_Augmented_Mapping,
    abstract = {In scenarios that require a close collaboration and
knowledge transfer between inexperienced users and robots,
the ?learning by interacting? paradigm goes hand in hand
with appropriate representations and learning methods. In this paper we discuss a mixed initiative strategy for robotic learning by interacting with a user in a joint map acquisition process.
We propose the integration of an environment representation
approach into our interactive learning framework. The environment representation and mapping system supports both
user driven and data driven strategies for the acquisition of spatial information, so that a mixed initiative strategy for the learning process is realised. We evaluate our system with test runs according to the scenario of a guided tour, extending the area of operation from structured laboratory environment to
less predictable domestic settings},
    author = {Julia Peltason and F. H. K. Siepmann and T. P. Spexard and Britta Wrede and Marc Hanheide and E. A. Topp},
    booktitle = {IEEE International Conference on Robotics and Automation.},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c0ee8)},
    month = {May},
    note = {In scenarios that require a close collaboration and
knowledge transfer between inexperienced users and robots,
the ?learning by interacting? paradigm goes hand in hand
with appropriate representations and learning methods. In this paper we discuss a mixed initiative strategy for robotic learning by interacting with a user in a joint map acquisition process.
We propose the integration of an environment representation
approach into our interactive learning framework. The environment representation and mapping system supports both
user driven and data driven strategies for the acquisition of spatial information, so that a mixed initiative strategy for the learning process is realised. We evaluate our system with test runs according to the scenario of a guided tour, extending the area of operation from structured laboratory environment to
less predictable domestic settings},
    pages = {2146--2153},
    publisher = {IEEE},
    title = {Mixed-initiative in human augmented mapping},
    url = {http://eprints.lincoln.ac.uk/6924/},
    year = {2009}
}

@article{Kirk_2020_Deep_Learning_Networks,
    author = {Raymond Kirk and Grzegorz Cielniak and Michael Mangan},
    doi = {10.3390/s20010275},
    journal = {Sensors},
    month = {jan},
    number = {1},
    pages = {275},
    publisher = {{MDPI} {AG}},
    title = {L{\ast}a{\ast}b{\ast}Fruits: A Rapid and Robust Outdoor Fruit Detection System Combining Bio-Inspired Features with One-Stage Deep Learning Networks},
    url = {https://doi.org/10.3390%2Fs20010275},
    volume = {20},
    year = {2020}
}

@inproceedings{lirolem7218_Laserbased_Navigation_Enhanced,
    abstract = {Navigation and obstacle avoidance in robotics using planar laser scans has matured over the last decades. They basically enable robots to penetrate highly dynamic and populated spaces, such as people's home, and move around smoothly. However, in an unconstrained environment the twodimensional perceptual space of a fixed mounted laser is not sufficient to ensure safe navigation. In this paper, we present an approach that pools a fast and reliable motion generation approach with modern 3D capturing techniques using a Timeof-Flight camera. Instead of attempting to implement full 3D motion control, which is computationally more expensive and simply not needed for the targeted scenario of a domestic robot, we introduce a \&quot;virtual laser\&quot;. For the originally solely laserbased motion generation the technique of fusing real laser measurements and 3D point clouds into a continuous data stream is 100\% compatible and transparent. The paper covers the general concept, the necessary extrinsic calibration of two very different types of sensors, and exemplarily illustrates the benefit which is to avoid obstacles not being perceivable in the original laser scan. \^A\\copyright 2009 IEEE.},
    address = {Kobe},
    author = {Fang Yuan and Agnes Swadzba and Roland Philippsen and Orhan Engin and Marc Hanheide and Sven Wachsmuth},
    booktitle = {Conference of 2009 IEEE International Conference on Robotics and Automation, ICRA '09},
    journal = {Proceedings - IEEE International Conference on Robotics and Automation},
    keywords = {ARRAY(0x7f43493c0eb8)},
    month = {May},
    note = {Navigation and obstacle avoidance in robotics using planar laser scans has matured over the last decades. They basically enable robots to penetrate highly dynamic and populated spaces, such as people's home, and move around smoothly. However, in an unconstrained environment the twodimensional perceptual space of a fixed mounted laser is not sufficient to ensure safe navigation. In this paper, we present an approach that pools a fast and reliable motion generation approach with modern 3D capturing techniques using a Timeof-Flight camera. Instead of attempting to implement full 3D motion control, which is computationally more expensive and simply not needed for the targeted scenario of a domestic robot, we introduce a \&quot;virtual laser\&quot;. For the originally solely laserbased motion generation the technique of fusing real laser measurements and 3D point clouds into a continuous data stream is 100\% compatible and transparent. The paper covers the general concept, the necessary extrinsic calibration of two very different types of sensors, and exemplarily illustrates the benefit which is to avoid obstacles not being perceivable in the original laser scan. \^A\\copyright 2009 IEEE.},
    pages = {2844--2850},
    publisher = {IEEE},
    title = {Laser-based navigation enhanced with 3D time-of-flight data},
    url = {http://eprints.lincoln.ac.uk/7218/},
    year = {2009}
}

@inproceedings{lirolem6948_Memory_Consistency_Validation,
    abstract = {Information fusion is a mandatory prerequisite for
cognitive vision systems. These are vision systems that apply reasoning and learning on different levels of abstraction and correspondingly have to deal with hypotheses from different categorical domains. Following some principles of human cognition, we
present an approach to information fusion that closely couples
reasoning and representation. We will discuss how processes like
probabilistic contextual reasoning as well as functional and nonfunctional requirements in storing data from different sources can
be integrated by a uni?ed XML based data representation. Due
to the interaction between active processes and data storage, we
call our approach an active memory. Performance results of an
implemented system as well as an evaluation of data fusion from
contextual inference will be presented},
    author = {Marc Hanheide and Christian Bauckhage and Gerhard Sagerer},
    booktitle = {17th International Conference on Pattern Recognition},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1548)},
    month = {August},
    note = {Information fusion is a mandatory prerequisite for
cognitive vision systems. These are vision systems that apply reasoning and learning on different levels of abstraction and correspondingly have to deal with hypotheses from different categorical domains. Following some principles of human cognition, we
present an approach to information fusion that closely couples
reasoning and representation. We will discuss how processes like
probabilistic contextual reasoning as well as functional and nonfunctional requirements in storing data from different sources can
be integrated by a uni?ed XML based data representation. Due
to the interaction between active processes and data storage, we
call our approach an active memory. Performance results of an
implemented system as well as an evaluation of data fusion from
contextual inference will be presented},
    pages = {459--462},
    publisher = {IEEE},
    title = {Memory consistency validation in a cognitive vision system},
    url = {http://eprints.lincoln.ac.uk/6948/},
    year = {2004}
}

@inproceedings{lirolem8313_Online_Datadriven_Fault,
    abstract = {In this paper we demonstrate the online applicability of the fault detection and diagnosis approach which we previously developed and published in 1. In our former work we showed that a purely data driven fault detection approach can be successfully built based on monitored inter-component communication data of a robotic system and used for a-posteriori fault detection. Here we propose an extension to this approach which is capable of online learning of the fault model as well as for online fault detection. We evaluate the application of our approach in the context of a RoboCup task executed by our service robot BIRON in corporation with an expert user. \^A\\copyright 2011 IEEE.},
    address = {San Francisco, CA},
    author = {R. Golombek and S. Wrede and Marc Hanheide and M. Heckmann},
    booktitle = {Conference of 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems: Celebrating 50 Years of Robotics, IROS'11},
    journal = {IEEE International Conference on Intelligent Robots and Systems},
    keywords = {ARRAY(0x7f43493bfa40)},
    month = {September},
    note = {Conference Code: 87712},
    pages = {3011--3016},
    publisher = {IEEE},
    title = {Online data-driven fault detection for robotic systems},
    url = {http://eprints.lincoln.ac.uk/8313/},
    year = {2011}
}

@inproceedings{lirolem6947_Information_Fusion,
    abstract = {Information fusion is a mandatory prerequisite for
cognitive vision systems. These are vision systems that apply rea-
soning and learning on different levels of abstraction and corre-
spondingly have to deal with hypotheses from different categori-
cal domains. Following some principles of human cognition, we
present an approach to information fusion that closely couples
reasoning and representation. We will discuss how processes like
probabilistic contextual reasoning as well as functional and non-
functional requirements in storing data from different sources can
be integrated by a unified XML based data representation. Due
to the interaction between active processes and data storage, we
call our approach an active memory. Performance results of an
implemented system as well as an evaluation of data fusion from
contextual inference will be presented.},
    author = {Sebastian Wrede and Marc Hanheide and Christian Bauckhage and Gerhard Sagerer},
    booktitle = {International Conference on Information Fusion},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1578)},
    month = {June},
    note = {Information fusion is a mandatory prerequisite for
cognitive vision systems. These are vision systems that apply rea-
soning and learning on different levels of abstraction and corre-
spondingly have to deal with hypotheses from different categori-
cal domains. Following some principles of human cognition, we
present an approach to information fusion that closely couples
reasoning and representation. We will discuss how processes like
probabilistic contextual reasoning as well as functional and non-
functional requirements in storing data from different sources can
be integrated by a unified XML based data representation. Due
to the interaction between active processes and data storage, we
call our approach an active memory. Performance results of an
implemented system as well as an evaluation of data fusion from
contextual inference will be presented.},
    pages = {198--205},
    title = {An active memory as a model for information fusion},
    url = {http://eprints.lincoln.ac.uk/6947/},
    year = {2004}
}

@inproceedings{lirolem8320_Dynamic_Path_Planning,
    abstract = {Mobile robots that are employed in people's homes need to safely navigate their environment. And natural human-inhabited environments still pose significant challenges for robots despite the impressive progress that has been achieved in the field of path planning and obstacle avoidance. These challenges mostly arise from the fact that (i) the perceptual abilities of a robot are limited, thus sometimes impeding its ability to see relevant obstacles (e.g. transparent objects), and (ii) the environment is highly dynamic being populated by humans. In this contribution we are making a case for an integrated solution to these challenges that builds upon the analysis and use of implicit human knowledge in path planning and a cascade of replanning approaches. We combine state of the art path planning and obstacle avoidance algorithms with the knowledge about how humans navigate in their very own environment. The approach results in a more robust and predictable navigation ability for domestic robots as is demonstrated in a number of experimental runs. \^A\\copyright2010 IEEE.},
    address = {Taipei},
    author = {F. Yuan and L. Twardon and Marc Hanheide},
    booktitle = {Conference of 23rd IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010},
    journal = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
    keywords = {ARRAY(0x7f43493bfbc0)},
    month = {October},
    note = {Conference Code: 83389},
    pages = {3275--3281},
    publisher = {IEEE},
    title = {Dynamic path planning adopting human navigation strategies for a domestic mobile robot},
    url = {http://eprints.lincoln.ac.uk/8320/},
    year = {2010}
}

@article{lirolem6704_Improving_Hri_Design,
    abstract = {Social robots are designed to interact with humans. That is why they need interaction models that take social behaviors into account. These usually influence many of a robot's abilities simultaneously. Hence, when designing robots that users will want to interact with, all components need to be tested in the system context, with real users and real tasks in real interactions. This requires methods that link the analysis of the robot's internal computations within and between components (system level) with the interplay between robot and user (interaction level). This article presents Systemic Interaction Analysis (SInA) as an integrated method to (a) derive prototypical courses of interaction based on system and interaction level, (b) identify deviations from these, (c) infer the causes of deviations by analyzing the system's operational sequences, and (d) improve the robot iteratively by adjusting models and implementations.},
    author = {Manja Lohse and Marc Hanheide and Karola Pitsch and Katharina J. Rohlfing and Gerhard Sagerer},
    journal = {Interaction Studies},
    keywords = {ARRAY(0x7f43493c0d98)},
    month = {October},
    note = {.},
    number = {3},
    pages = {298--303},
    publisher = {John Benjamins Publishing },
    title = {Improving HRI design by applying systemic interaction analysis (SInA)},
    url = {http://eprints.lincoln.ac.uk/6704/},
    volume = {10},
    year = {2009}
}

@article{Cielniak2004_People_Recognition,
    author = {Cielniak, G. and Duckett, T.},
    journal = {Journal of Intelligent and Fuzzy Systems},
    number = {1},
    pages = {21-27},
    title = {People recognition by mobile robots},
    volume = {15},
    year = {2004}
}

@article{lirolem6701_Coordinating_Interactive_Vision,
    abstract = {Most of the research conducted in human-computer interaction (HCI) focuses on a seamless interface between a user and an application that is separated from the user in terms of working space and/or control, like navigation in image databases, instruction of robots, or information retrieval systems. The interaction paradigm of cognitive assistance goes one step further in that the application consists of assisting the user performing everyday tasks in his or her own environment and in that the user and the system share the control of such tasks. This kind of tight bidirectional interaction in realistic environments demands cognitive system skills like context awareness, attention, learning, and reasoning about the external environment. Therefore, the system needs to integrate a wide variety of visual functions, like localization, object tracking and recognition, action recognition, interactive object learning, etc. In this paper we show how different kinds of system behaviors are realized using the Active Memory Infrastructure that provides the technical basis for distributed computation and a data- and event-driven integration approach. A running augmented reality system for cognitive assistance is presented that supports users in mixing beverages. The flexibility and generality of the system framework provides an ideal testbed for studying visual cues in human-computer interaction. We report about results from first user studies.},
    author = {Sven Wachsmuth and Sebastian Wrede and Marc Hanheide},
    journal = {Computer Vision and Image Understanding},
    keywords = {ARRAY(0x7f43493c1248)},
    month = {October},
    note = {Most of the research conducted in human-computer interaction (HCI) focuses on a seamless interface between a user and an application that is separated from the user in terms of working space and/or control, like navigation in image databases, instruction of robots, or information retrieval systems. The interaction paradigm of cognitive assistance goes one step further in that the application consists of assisting the user performing everyday tasks in his or her own environment and in that the user and the system share the control of such tasks. This kind of tight bidirectional interaction in realistic environments demands cognitive system skills like context awareness, attention, learning, and reasoning about the external environment. Therefore, the system needs to integrate a wide variety of visual functions, like localization, object tracking and recognition, action recognition, interactive object learning, etc. In this paper we show how different kinds of system behaviors are realized using the Active Memory Infrastructure that provides the technical basis for distributed computation and a data- and event-driven integration approach. A running augmented reality system for cognitive assistance is presented that supports users in mixing beverages. The flexibility and generality of the system framework provides an ideal testbed for studying visual cues in human-computer interaction. We report about results from first user studies.},
    number = {1-2},
    pages = {135--149},
    publisher = {Springer},
    title = {Coordinating interactive vision behaviors for cognitive assistance},
    url = {http://eprints.lincoln.ac.uk/6701/},
    volume = {108},
    year = {2007}
}

@article{Cielniak2017_Portable_Navigation_System,
    author = {Lock, J. and Cielniak, G. and Bellotto, N.},
    journal = {AAAI Spring Symposium - Technical Report},
    pages = {395-400},
    title = {A portable navigation system with an adaptive multimodal interface for the blind},
    volume = {SS-17-01 - SS-17-08},
    year = {2017}
}

@inproceedings{lirolem8322_Probabilistic_Selfawareness_Model,
    abstract = {In order to address the problem of failure detection in the robotics domain, we present in this contribution a so-called self-awareness model, based on the system's internal data exchange and the inherent dynamics of inter-component communication. The model is strongly data driven and provides an anomaly detector for robotics systems both applicable in-situ at runtime as well as a-posteriori in post-mortem analysis. Current architectures or methods for failure detection in autonomous robots are either implementations of watch dog concepts or are based on excessive amounts of domain-specific error detection code. The approach presented in this contribution provides an avenue for the detection of more subtle anomalies originating from external sources such as the environment itself or system failures such as resource starvation. Additionally, developers are alleviated from explicitly modeling and foreseeing every exceptional situation, instead training the presented probabilistic model with the known normal modes within the specification of the robot system. As we developed and evaluated the self-awareness model on a mobile robot platform featuring an event-driven software architecture, the presented method can easily be applied in other current robotics software architectures. \^A\\copyright2010 IEEE.},
    address = {Taipei},
    author = {R. Golombek and S. Wrede and M. Hanheide and M. Heckmann},
    booktitle = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
    journal = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
    keywords = {ARRAY(0x7f43493bfbf0)},
    month = {October},
    note = {cited By (since 1996) 0; Conference of 23rd IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010; Conference Date: 18 October 2010 through 22 October 2010; Conference Code: 83389},
    pages = {2745--2750},
    title = {Learning a probabilistic self-awareness model for robotic systems},
    url = {http://eprints.lincoln.ac.uk/8322/},
    year = {2010}
}

@article{Zaganidis_2018_Point_Cloud_Registration,
    author = {Anestis Zaganidis and Li Sun and Tom Duckett and Grzegorz Cielniak},
    doi = {10.1109/lra.2018.2848308},
    journal = {{IEEE} Robotics and Automation Letters},
    month = {oct},
    number = {4},
    pages = {2942--2949},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    title = {Integrating Deep Semantic Segmentation Into 3-D Point Cloud Registration},
    url = {https://doi.org/10.1109%2Flra.2018.2848308},
    volume = {3},
    year = {2018}
}

@article{Mayoral_Ba_os_2023_Risk_Management_Method,
    author = {Mayoral Baños, José Carlos and From, Pål Johan and Cielniak, Grzegorz},
    doi = {10.3390/robotics12030063},
    issn = {2218-6581},
    journal = {Robotics},
    month = {April},
    number = {3},
    pages = {63},
    publisher = {MDPI AG},
    title = {Towards Safe Robotic Agricultural Applications: Safe Navigation System Design for a Robotic Grass-Mowing Application through the Risk Management Method},
    url = {http://dx.doi.org/10.3390/robotics12030063},
    volume = {12},
    year = {2023}
}

@inproceedings{Feltwell_2015_Game_Design_Tool,
    author = {Tom Feltwell and Grzegorz Cielniak and Patrick Dickinson and Ben J. Kirman and Shaun Lawson},
    booktitle = {Proceedings of the 2015 Annual Symposium on Computer-Human Interaction in Play - {CHI} {PLAY} {\textquotesingle}15},
    doi = {10.1145/2793107.2810284},
    publisher = {{ACM} Press},
    title = {Dendrogram Visualization as a Game Design Tool},
    url = {https://doi.org/10.1145%2F2793107.2810284},
    year = {2015}
}

@inproceedings{lirolem6928_Active_Memorybased_Interaction,
    abstract = {Despite increasing efforts in the field of social
robotics and interactive systems integrated and fully autonomous
robots which are capable of learning from interaction
with inexperienced and non-expert users are still a rarity.
However, in order to tackle the challenge of learning by
interaction robots need to be equipped with a set of basic
behaviors and abilities which have to be coupled and combined
in a flexible manner. This paper presents how a recently
proposed information-driven integration concept termed ?active
memory? is adopted to realize learning-enabling behaviors for
a domestic robot. These behaviors enable it to (i) learn about its
environment, (ii) interact with several humans simultaneously,
and (iii) couple learning and interaction tightly. The basic
interaction strategies on the basis of information exchange
through the active memory are presented. A brief discussion
of results obtained from live user trials with inexperienced
users in a home tour scenario underpin the relevance and
appropriateness of the described concepts.},
    author = {Marc Hanheide and Gerhard Sagerer},
    booktitle = {RO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1038)},
    month = {August},
    note = {Despite increasing efforts in the field of social
robotics and interactive systems integrated and fully autonomous
robots which are capable of learning from interaction
with inexperienced and non-expert users are still a rarity.
However, in order to tackle the challenge of learning by
interaction robots need to be equipped with a set of basic
behaviors and abilities which have to be coupled and combined
in a flexible manner. This paper presents how a recently
proposed information-driven integration concept termed ?active
memory? is adopted to realize learning-enabling behaviors for
a domestic robot. These behaviors enable it to (i) learn about its
environment, (ii) interact with several humans simultaneously,
and (iii) couple learning and interaction tightly. The basic
interaction strategies on the basis of information exchange
through the active memory are presented. A brief discussion
of results obtained from live user trials with inexperienced
users in a home tour scenario underpin the relevance and
appropriateness of the described concepts.},
    pages = {101--106},
    publisher = {IEEE},
    title = {Active memory-based interaction strategies for learning-enabling behaviors},
    url = {http://eprints.lincoln.ac.uk/6928/},
    year = {2008}
}

@inproceedings{Cielniak_2007_Improved_Data_Association,
    author = {Grzegorz Cielniak and Tom Duckett and Achim J. Lilienthal},
    booktitle = {2007 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
    doi = {10.1109/iros.2007.4399507},
    month = {oct},
    publisher = {{IEEE}},
    title = {Improved data association and occlusion handling for vision-based people tracking by mobile robots},
    url = {https://doi.org/10.1109%2Firos.2007.4399507},
    year = {2007}
}

@inproceedings{Cielniak_2003_Omnidirectional_Vision_Sensor,
    author = {Grzegorz Cielniak and Mihajlo Miladinovic and Daniel Hammarin and Linus Goranson and Achim Lilienthal and Tom Duckett},
    booktitle = {2003 Conference on Computer Vision and Pattern Recognition Workshop},
    doi = {10.1109/cvprw.2003.10072},
    month = {jun},
    publisher = {{IEEE}},
    title = {Appearance-based Tracking of Persons with an Omnidirectional Vision Sensor},
    url = {https://doi.org/10.1109%2Fcvprw.2003.10072},
    year = {2003}
}

@inproceedings{lirolem6930_Robot_Applying_Heuristics,
    abstract = {In a society that keeps getting closer in touch with social robots it is very important to include potential users throughout the design of the systems. This is an important rationale to build robots that provide services and assistance in a socially acceptable way and influence societies in a positive way. In the process, methods are needed to rate the robot interaction performance. We present a multimodal corpus of na\"ive users interacting with an autonomously operating system. It comprises data that, to our conviction, reveal a lot about human-robot interaction (HRI) in general and social acceptance, in particular. In both, the evaluation and the design process we took into account Clarkson and Arkin's heuristics for HRI (developed by adapting Nielsen's and Scholtz' heuristics to robotics) 1. We discuss exemplary results to show the use of heuristics in the design of socially acceptable robots.},
    author = {Manja Lohse and Marc Hanheide},
    booktitle = {Robots as Social Actors Workshop: International Symposium on Robot and Human Interactive Communication (RO-MAN 08)},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1068)},
    month = {August},
    note = {In a society that keeps getting closer in touch with social robots it is very important to include potential users throughout the design of the systems. This is an important rationale to build robots that provide services and assistance in a socially acceptable way and influence societies in a positive way. In the process, methods are needed to rate the robot interaction performance. We present a multimodal corpus of na\"ive users interacting with an autonomously operating system. It comprises data that, to our conviction, reveal a lot about human-robot interaction (HRI) in general and social acceptance, in particular. In both, the evaluation and the design process we took into account Clarkson and Arkin's heuristics for HRI (developed by adapting Nielsen's and Scholtz' heuristics to robotics) 1. We discuss exemplary results to show the use of heuristics in the design of socially acceptable robots.},
    pages = {1584--1589},
    title = {Evaluating a social home tour robot applying heuristics},
    url = {http://eprints.lincoln.ac.uk/6930/},
    year = {2008}
}

@inproceedings{lirolem8346_Versatile_Modelbased_Visibility,
    abstract = {In this paper, we introduce a novel model-based visibility measure for geometric primitives called visibility map. It is simple to calculate, memory efficient, accurate for viewpoints outside the convex hull of the object and versatile in terms of possible applications. Several useful properties of visibility maps that show their superiority to existing visibility measures are derived. Various example applications from the automotive industry where the presented measure is used successfully conclude the paper. \^A\\copyright Springer-Verlag Berlin Heidelberg 2005.},
    address = {Joensuu},
    author = {M. M. Ellenrieder and L. Kr\"uger and D.  St\~A\\P\~A?e and M. Hanheide},
    booktitle = {14th Scandinavian Conference on Image Analysis, SCIA 2005},
    journal = {Lecture Notes in Computer Science},
    keywords = {ARRAY(0x7f43493c1488)},
    month = {June},
    note = {Conference Code: 65718},
    pages = {669--678},
    publisher = {Springer Verlag},
    title = {A versatile model-based visibility measure for geometric primitive},
    url = {http://eprints.lincoln.ac.uk/8346/},
    volume = {3540},
    year = {2005}
}

@inproceedings{lirolem7880_Facial_Communicative_Signal,
    abstract = {Facial communicative signals (FCSs) such as head gestures, eye gaze, and facial expressions can provide useful feedback in conversations between people and also in humanrobot interaction. This paper presents a pattern recognition approach for the interpretation of FCSs in terms of valence, based on the selection of discriminative subsequences in video data. These subsequences capture important temporal dynamics and are used as prototypical reference subsequences in a classi?cation procedure based on dynamic time warping and feature extraction with active appearance models. Using this valence classi?cation, the robot can discriminate positive from negative interaction situations and react accordingly. The approach is evaluated on a database containing videos of people interacting with a robot by teaching the names of several objects to it. The verbal answer of the robot is expected to elicit the display of spontaneous FCSs by the human tutor, which were classi?ed in this work. The achieved classi?cation accuracies are comparable to the average human recognition performance and outperformed our previous results on this task.},
    author = {Christian Lang and Sven Wachsmuth and Marc Hanheide and Heiko Wersing},
    booktitle = {International Conference on Robotics and Automation (ICRA)},
    keywords = {ARRAY(0x7f43493bf020)},
    month = {May},
    note = {Facial communicative signals (FCSs) such as head gestures, eye gaze, and facial expressions can provide useful feedback in conversations between people and also in humanrobot interaction. This paper presents a pattern recognition approach for the interpretation of FCSs in terms of valence, based on the selection of discriminative subsequences in video data. These subsequences capture important temporal dynamics and are used as prototypical reference subsequences in a classi?cation procedure based on dynamic time warping and feature extraction with active appearance models. Using this valence classi?cation, the robot can discriminate positive from negative interaction situations and react accordingly. The approach is evaluated on a database containing videos of people interacting with a robot by teaching the names of several objects to it. The verbal answer of the robot is expected to elicit the display of spontaneous FCSs by the human tutor, which were classi?ed in this work. The achieved classi?cation accuracies are comparable to the average human recognition performance and outperformed our previous results on this task.},
    pages = {170--177},
    publisher = {IEEE},
    title = {Facial communicative signal interpretation in human-robot interaction by discriminative video subsequence selection},
    url = {http://eprints.lincoln.ac.uk/7880/},
    year = {2013}
}

@article{Polvara_2020_Simtoreal_Quadrotor_Landing,
    author = {Riccardo Polvara and Massimiliano Patacchiola and Marc Hanheide and Gerhard Neumann},
    doi = {10.3390/robotics9010008},
    journal = {Robotics},
    month = {feb},
    number = {1},
    pages = {8},
    publisher = {{MDPI} {AG}},
    title = {Sim-to-Real Quadrotor Landing via Sequential Deep Q-Networks and Domain Randomization},
    url = {https://doi.org/10.3390%2Frobotics9010008},
    volume = {9},
    year = {2020}
}

@article{lirolem6702_Spatial_Movement_Concept,
    abstract = {In humanhuman interaction, social signals or unconscious cues are sent and received by interaction partners. These signals and cues influence the interaction partner wanted and unwantedsometimes to achieve a distinct goal. To be aware of those signals and especially of implicit cues is crucial when interaction between a robot and a human is modelled. This project aims to use implicit body and machine movements to make HRI smoother and simpler. A robot should not only consider social rules with respect to proxemics in communication or in encounter people. It should also be able to signal and understand certain spatial constraints. A first spatial and situational constraint, which this research project currently focuses on, is avoiding each other. This includes not only passing by but also especially making room for each other. Consider a narrow place, e.g. hallways, door frames or a small kitchen. A robot might block the way or drives towards you, pursuing its own goal like you. Humans do not even speak to each other in order to pass by and avoid bumping into each other even if the space is narrow. A first study is currently conducted to find out which behaviour is the most appropriate avoiding strategy and how participants express their wish to pass by. Therefore, a variety of defensive and more offensive avoiding strategies of the robot are applied in experiments. The results of the study will be used to equip the robot with spatial concepts to make interaction faster and more appropriate.},
    author = {Annika Peters and Petra Weiss and Marc Hanheide},
    journal = {Cognitive Processing},
    keywords = {ARRAY(0x7f43493c0e58)},
    month = {September},
    note = {In humanhuman interaction, social signals or unconscious cues are sent and received by interaction partners. These signals and cues influence the interaction partner wanted and unwantedsometimes to achieve a distinct goal. To be aware of those signals and especially of implicit cues is crucial when interaction between a robot and a human is modelled. This project aims to use implicit body and machine movements to make HRI smoother and simpler. A robot should not only consider social rules with respect to proxemics in communication or in encounter people. It should also be able to signal and understand certain spatial constraints. A first spatial and situational constraint, which this research project currently focuses on, is avoiding each other. This includes not only passing by but also especially making room for each other. Consider a narrow place, e.g. hallways, door frames or a small kitchen. A robot might block the way or drives towards you, pursuing its own goal like you. Humans do not even speak to each other in order to pass by and avoid bumping into each other even if the space is narrow. A first study is currently conducted to find out which behaviour is the most appropriate avoiding strategy and how participants express their wish to pass by. Therefore, a variety of defensive and more offensive avoiding strategies of the robot are applied in experiments. The results of the study will be used to equip the robot with spatial concepts to make interaction faster and more appropriate.},
    number = {S2},
    pages = {177--178},
    publisher = {Springer},
    title = {Avoid me: a spatial movement concept in human-robot interaction},
    url = {http://eprints.lincoln.ac.uk/6702/},
    volume = {10},
    year = {2009}
}

@inbook{Le_Lou_dec_2023_Robotic_Fruit_Picking,
    author = {Le Louëdec, Justin and Cielniak, Grzegorz},
    booktitle = {Computer Vision Systems},
    doi = {10.1007/978-3-031-44137-0_13},
    isbn = {9783031441370},
    issn = {1611-3349},
    pages = {148–158},
    publisher = {Springer Nature Switzerland},
    title = {Key Point-Based Orientation Estimation of Strawberries for Robotic Fruit Picking},
    url = {http://dx.doi.org/10.1007/978-3-031-44137-0_13},
    year = {2023}
}

@article{Fentanes_2018_Soil_Compaction_Mapping,
    author = {Jaime Pulido Fentanes and Iain Gould and Tom Duckett and Simon Pearson and Grzegorz Cielniak},
    doi = {10.1109/lra.2018.2849567},
    journal = {{IEEE} Robotics and Automation Letters},
    month = {oct},
    number = {4},
    pages = {3066--3072},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    title = {3-D Soil Compaction Mapping Through Kriging-Based Exploration With a Mobile Robot},
    url = {https://doi.org/10.1109%2Flra.2018.2849567},
    volume = {3},
    year = {2018}
}

@inproceedings{Krajnik_2014_Longterm_Robotic_Mapping,
    author = {Tomas Krajnik and Jaime Pulido Fentanes and Grzegorz Cielniak and Christian Dondrup and Tom Duckett},
    booktitle = {2014 {IEEE} International Conference on Robotics and Automation ({ICRA})},
    doi = {10.1109/icra.2014.6907396},
    month = {may},
    publisher = {{IEEE}},
    title = {Spectral analysis for long-term robotic mapping},
    url = {https://doi.org/10.1109%2Ficra.2014.6907396},
    year = {2014}
}

@inbook{Bosilj_2019_Soil_Size_Distribution,
    author = {Bosilj, Petra and Gould, Iain and Duckett, Tom and Cielniak, Grzegorz},
    booktitle = {Mathematical Morphology and Its Applications to Signal and Image Processing},
    doi = {10.1007/978-3-030-20867-7_32},
    isbn = {9783030208677},
    issn = {1611-3349},
    pages = {415–427},
    publisher = {Springer International Publishing},
    title = {Pattern Spectra from Different Component Trees for Estimating Soil Size Distribution},
    url = {http://dx.doi.org/10.1007/978-3-030-20867-7_32},
    year = {2019}
}

@incollection{lirolem6719_Stereo_Video_Sequences,
    abstract = {lthough mosaics are well established as a compact and non-redundant representation of image sequences, their application still suffers from restrictions of the camera motion or has to deal with parallax errors. We present an approach that allows construction of mosaics from arbitrary motion of a head-mounted camera pair. As there are no parallax errors when creating mosaics from planar objects, our approach first decomposes the scene into planar sub-scenes from stereo vision and creates a mosaic for each plane individually. The power of the presented mosaicing technique is evaluated in an office scenario, including the analysis of the parallax error.},
    author = {Nicholas Gorges and Marc Hanheide and William Christmas and Christian Bauckhage and Gerhard Sagerer and Joseph Kittler},
    booktitle = {Pattern recognition},
    editor = {Carl Edward Rasmussen and Heinrich H. Buelthoff and Bernhard Schoelkopf and Martin Giese},
    keywords = {ARRAY(0x7f43493c1518)},
    month = {December},
    note = {lthough mosaics are well established as a compact and non-redundant representation of image sequences, their application still suffers from restrictions of the camera motion or has to deal with parallax errors. We present an approach that allows construction of mosaics from arbitrary motion of a head-mounted camera pair. As there are no parallax errors when creating mosaics from planar objects, our approach first decomposes the scene into planar sub-scenes from stereo vision and creates a mosaic for each plane individually. The power of the presented mosaicing technique is evaluated in an office scenario, including the analysis of the parallax error.},
    number = {3175},
    pages = {342--349},
    publisher = {Springer},
    series = {Lecture Notes in Computer Science},
    title = {Mosaics from arbitrary stereo video sequences},
    url = {http://eprints.lincoln.ac.uk/6719/},
    year = {2004}
}

@article{lirolem6560_Videobased_Studies,
    abstract = {Robots are increasingly being used in domestic environments and should be able to interact with inexperienced users. Human-human interaction and human-computer interaction research findings are relevant, but often limited because robots are different from both humans and computers. Therefore, new human-robot interaction (HRI) research methods can inform the design of robots suitable for inexperienced users. A video-based HRI (VHRI) methodology was here used to carry out a multi-national HRI user study for the prototype domestic robot BIRON (BIelefeld RObot companioN). Previously, the VHRI methodology was used in constrained HRI situations, while in this study HRIs involved a series of events as part of a 'hometour' scenario. Thus, the present work is the first study of this methodology in extended HRI contexts with a multi-national approach. Participants watched videos of the robot interacting with a human actor and rated two robot behaviors (Extrovert and Introvert). Participants' perceptions and ratings of the robot's behaviors differed with regard to both verbal interactions and person following by the robot. The study also confirms that the VHRI methodology provides a valuable means to obtain early user feedback, even before fully working prototypes are available. This can usefully guide the future design work on robots, and associated verbal and non-verbal behaviors.},
    author = {Michael L. Walters and Manja Lohse and Marc Hanheide and Britte Wrede and Dag Sverre Syrdal and Kheng Lee Koay and Anders Green and Helge Huttenrauch and Kerstin Dautenhahn and Gerhard Sagerer and Kerstin Severinson-Eklundh},
    journal = {Advanced Robotics},
    keywords = {ARRAY(0x7f43493bf9e0)},
    month = {December},
    note = {Robots are increasingly being used in domestic environments and should be able to interact with inexperienced users. Human-human interaction and human-computer interaction research findings are relevant, but often limited because robots are different from both humans and computers. Therefore, new human-robot interaction (HRI) research methods can inform the design of robots suitable for inexperienced users. A video-based HRI (VHRI) methodology was here used to carry out a multi-national HRI user study for the prototype domestic robot BIRON (BIelefeld RObot companioN). Previously, the VHRI methodology was used in constrained HRI situations, while in this study HRIs involved a series of events as part of a 'hometour' scenario. Thus, the present work is the first study of this methodology in extended HRI contexts with a multi-national approach. Participants watched videos of the robot interacting with a human actor and rated two robot behaviors (Extrovert and Introvert). Participants' perceptions and ratings of the robot's behaviors differed with regard to both verbal interactions and person following by the robot. The study also confirms that the VHRI methodology provides a valuable means to obtain early user feedback, even before fully working prototypes are available. This can usefully guide the future design work on robots, and associated verbal and non-verbal behaviors.},
    number = {18},
    pages = {2233--2254},
    publisher = {Taylor \& Francis},
    title = {Evaluating the robot personality and verbal behavior of domestic robots using video-based studies},
    url = {http://eprints.lincoln.ac.uk/6560/},
    volume = {25},
    year = {2011}
}

@article{Cielniak2012_Usergenerated_Spatial_Data,
    author = {Feltwell, T. and Dickinson, P. and Cielniak, G.},
    journal = {13th International Conference on Intelligent Games and Simulation, GAME-ON 2012},
    pages = {17-24},
    title = {A framework for quantitative analysis of user-generated spatial data},
    year = {2012}
}

@inproceedings{lirolem6927_Synchrony_In_Parentchild,
    abstract = {In our approach, we aim at an objective measurement of
synchrony in multimodal tutoring behavior. The use of signal
correlation provides a well formalized method that yields
gradual information about the degree of synchrony. For our
analysis, we used and extended an algorithm proposed by
Hershey \& Movellan (2000) that correlates single-pixel
values of a video signal with the loudness of the
corresponding audio track over time. The results of all pixels are integrated over the video to achieve a scalar estimate of synchrony.},
    author = {Matthias Rolf and Marc Hanheide and Katharina J. Rohlfing},
    booktitle = {SRCD 2009 Biennial Meeting},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c0f48)},
    month = {April},
    note = {In our approach, we aim at an objective measurement of
synchrony in multimodal tutoring behavior. The use of signal
correlation provides a well formalized method that yields
gradual information about the degree of synchrony. For our
analysis, we used and extended an algorithm proposed by
Hershey \& Movellan (2000) that correlates single-pixel
values of a video signal with the loudness of the
corresponding audio track over time. The results of all pixels are integrated over the video to achieve a scalar estimate of synchrony.},
    publisher = {Society for Research in Child Development},
    title = {The use of synchrony in parent-child interaction can be measured on a signal-level},
    url = {http://eprints.lincoln.ac.uk/6927/},
    year = {2009}
}

@article{Pal_2022_Agricultural_Event_Prediction,
    author = {Pal, Abhishesh and Das, Gautham and Hanheide, Marc and Candea Leite, Antonio and From, Pål Johan},
    doi = {10.3390/agronomy12061299},
    issn = {2073-4395},
    journal = {Agronomy},
    month = {May},
    number = {6},
    pages = {1299},
    publisher = {MDPI AG},
    title = {An Agricultural Event Prediction Framework towards Anticipatory Scheduling of Robot Fleets: General Concepts and Case Studies},
    url = {http://dx.doi.org/10.3390/agronomy12061299},
    volume = {12},
    year = {2022}
}

@incollection{lirolem6717_Versatile_Modelbased_Visibility,
    abstract = {In this paper, we introduce a novel model-based visibility measure for geometric primitives called visibility map. It is simple to calculate, memory efficient, accurate for viewpoints outside the convex hull of the object and versatile in terms of possible applications. Several useful properties of visibility maps that show their superiority to existing visibility measures are derived. Various example applications from the automotive industry where the presented measure is used successfully conclude the paper.},
    author = {Marc M. Ellenrieder and Lars Kruger and Dirk Stoessel and Marc Hanheide},
    booktitle = {Image Analysis},
    editor = {Heikki Kalviainen and Jussi Parkinnen and Arto Kaarna},
    keywords = {ARRAY(0x7f43493c13c8)},
    month = {December},
    note = {In this paper, we introduce a novel model-based visibility measure for geometric primitives called visibility map. It is simple to calculate, memory efficient, accurate for viewpoints outside the convex hull of the object and versatile in terms of possible applications. Several useful properties of visibility maps that show their superiority to existing visibility measures are derived. Various example applications from the automotive industry where the presented measure is used successfully conclude the paper.},
    number = {3540},
    pages = {669--678},
    publisher = {Springer},
    series = {Lecture Notes in Computer Science},
    title = {A versatile model-based visibility measure for geometric primitives},
    url = {http://eprints.lincoln.ac.uk/6717/},
    year = {2005}
}

@article{Bosilj_2020_Estimating_Soil_Aggregate,
    author = {Petra Bosilj and Iain Gould and Tom Duckett and Grzegorz Cielniak},
    doi = {10.1016/j.biosystemseng.2020.07.012},
    journal = {Biosystems Engineering},
    month = {oct},
    pages = {63--77},
    publisher = {Elsevier {BV}},
    title = {Estimating soil aggregate size distribution from images using pattern spectra},
    url = {https://doi.org/10.1016%2Fj.biosystemseng.2020.07.012},
    volume = {198},
    year = {2020}
}

@article{Cielniak2019_Active_Object_Search,
    author = {Lock, J.C. and Cielniak, G. and Bellotto, N.},
    journal = {VISIGRAPP 2019 - Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
    pages = {476-485},
    title = {Active object search with a mobile device for people with visual impairments},
    volume = {4},
    year = {2019}
}

@article{Krajnik_2019_Warped_Hypertime_Representations,
    author = {Tomas Krajnik and Tomas Vintr and Sergi Molina and Jaime Pulido Fentanes and Grzegorz Cielniak and Oscar Martinez Mozos and George Broughton and Tom Duckett},
    doi = {10.1109/lra.2019.2926682},
    journal = {{IEEE} Robotics and Automation Letters},
    month = {oct},
    number = {4},
    pages = {3310--3317},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    title = {Warped Hypertime Representations for Long-Term Autonomy of Mobile Robots},
    url = {https://doi.org/10.1109%2Flra.2019.2926682},
    volume = {4},
    year = {2019}
}

@article{Cielniak2018_Connected_Attribute_Morphology,
    author = {Bosilj, P. and Duckett, T. and Cielniak, G.},
    journal = {Computers in Industry},
    pages = {226-240},
    title = {Connected attribute morphology for unified vegetation segmentation and classification in precision agriculture},
    volume = {98},
    year = {2018}
}

@article{Le_Lou_dec_2021_Deep_Learningbased_Segmentation,
    author = {Justin Le Louëdec and Grzegorz Cielniak},
    doi = {10.1016/j.compag.2021.106374},
    journal = {Computers and Electronics in Agriculture},
    month = {nov},
    pages = {106374},
    publisher = {Elsevier {BV}},
    title = {3D shape sensing and deep learning-based segmentation of strawberries},
    url = {https://doi.org/10.1016%2Fj.compag.2021.106374},
    volume = {190},
    year = {2021}
}

@article{Cielniak2012_Realtime_Adaptive_Track,
    author = {Bird, J. and Feltwell, T. and Cielniak, G.},
    journal = {13th International Conference on Intelligent Games and Simulation, GAME-ON 2012},
    pages = {48-52},
    title = {Real-time adaptive track generation in racing games},
    year = {2012}
}

@inproceedings{lirolem6942_Building_Modular_Vision,
    abstract = {With the increasing interest in computer vision for interactive
systems, the challenges of the development process
involving many researchers are becoming more prominent.
Issues like reuse of algorithms, modularity, and distributed
processing are getting more important in the endeavor of
building complex vision systems. We present a framework
that allows independent development of enclosed components
and supports interactive optimization of algorithmic
parameters in an online fashion. The communication between
components is performed nearly without any slow
down compared to a monolithic system. Through the modular
concept, all components can be flexibly distributed and
reused in other application domains. The suitability of the
approach is demonstrated with an example system.},
    author = {Frank Lomker and Sebastian Wrede and Marc Hanheide and Jannik Fritsch},
    booktitle = {Fourth IEEE International Conference on Computer Vision Systems (ICVS'06)},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1368)},
    month = {January},
    note = {With the increasing interest in computer vision for interactive
systems, the challenges of the development process
involving many researchers are becoming more prominent.
Issues like reuse of algorithms, modularity, and distributed
processing are getting more important in the endeavor of
building complex vision systems. We present a framework
that allows independent development of enclosed components
and supports interactive optimization of algorithmic
parameters in an online fashion. The communication between
components is performed nearly without any slow
down compared to a monolithic system. Through the modular
concept, all components can be flexibly distributed and
reused in other application domains. The suitability of the
approach is demonstrated with an example system.},
    publisher = {IEEE Computer Society},
    title = {Building modular vision systems with a graphical plugin environment},
    url = {http://eprints.lincoln.ac.uk/6942/},
    year = {2006}
}

@inproceedings{Baxter_2018_Safe_Humanrobot_Interaction,
    author = {Paul Baxter and Grzegorz Cielniak and Marc Hanheide and P{\aa}l From},
    booktitle = {Companion of the 2018 {ACM}/{IEEE} International Conference on Human-Robot Interaction  - {HRI} {\textquotesingle}18},
    doi = {10.1145/3173386.3177072},
    publisher = {{ACM} Press},
    title = {Safe Human-Robot Interaction in Agriculture},
    url = {https://doi.org/10.1145%2F3173386.3177072},
    year = {2018}
}

@article{Dayoub_2011_Adaptive_Spherical_View,
    author = {Feras Dayoub and Grzegorz Cielniak and Tom Duckett},
    doi = {10.1016/j.robot.2011.02.013},
    journal = {Robotics and Autonomous Systems},
    month = {may},
    number = {5},
    pages = {285--295},
    publisher = {Elsevier {BV}},
    title = {Long-term experiments with an adaptive spherical view representation for navigation in changing environments},
    url = {https://doi.org/10.1016%2Fj.robot.2011.02.013},
    volume = {59},
    year = {2011}
}

@inproceedings{lirolem15832_Social_Distance_Augmented,
    abstract = {In this paper we propose to augment a wellestablished Qualitative Trajectory Calculus (QTC) by incorporating social distances into the model to facilitate a richer and more powerful representation of Human-Robot Spatial Interaction (HRSI). By combining two variants of QTC that implement different resolutions and switching between them based on distance thresholds we show that we are able to both reduce the complexity of the representation and at the same time enrich QTC with one of the core HRSI concepts: proxemics. Building on this novel integrated QTC model, we propose to represent the joint spatial behaviour of a human and a robot employing a probabilistic representation based on Hidden Markov Models. We show the appropriateness of our approach by encoding different HRSI behaviours observed in a human-robot interaction study and show how the models can be used to represent and classify these behaviours using  social distance-augmented QTC.},
    author = {Christian Dondrup and Nicola Bellotto and Marc Hanheide},
    booktitle = {Robot and Human Interactive Communication, 2014 RO-MAN},
    keywords = {ARRAY(0x7f43493b2890)},
    month = {October},
    pages = {519--524},
    publisher = {IEEE},
    title = {Social distance augmented qualitative trajectory calculus for human-robot spatial interaction},
    url = {http://eprints.lincoln.ac.uk/15832/},
    year = {2014}
}

@inproceedings{lirolem6923_Interactive_Robotic_Learning,
    abstract = {In learning tasks, interaction is mostly about the exchange
of knowledge. The interaction process shall be governed on the one hand by the knowledge the tutor wants to convey and on the other by the lacks of knowledge of the learner. In human-robot interaction (HRI), it is usually the human demonstrating or explicitly verbalizing her knowl-
edge and the robot acquiring a respective representation. The ultimate goal in interactive robot learning is thus to enable inexperienced, un- trained users to tutor robots in a most natural and intuitive manner.
This goal is often impeded by a lack of knowledge of the human about the internal processing and expectations of the robot and by the inflexibility of the robot to understand open-ended, unconstrained tutoring or demonstration. Hence, we propose mixed-initiative strategies to allow both to mutually contribute to the interactive learning process as
a bi-directional negotiation about knowledge. Along this line this paper discusses two initially different case studies on object manipulation and learning of spatial environments. We present different styles of mixed-
initiative in these scenarios and discuss the merits in each case.},
    author = {Julia Peltason and Ingo L\"utkebohle and Britta Wrede and Marc Hanheide},
    booktitle = {Mixed Initiative Workshop on Improving Human-Robot Communication with Mixed-Initiative and Context-Awareness at the 18th IEEE International Symposium on Robot and Human Interactive Communication},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c0df8)},
    month = {September},
    note = {In learning tasks, interaction is mostly about the exchange
of knowledge. The interaction process shall be governed on the one hand by the knowledge the tutor wants to convey and on the other by the lacks of knowledge of the learner. In human-robot interaction (HRI), it is usually the human demonstrating or explicitly verbalizing her knowl-
edge and the robot acquiring a respective representation. The ultimate goal in interactive robot learning is thus to enable inexperienced, un- trained users to tutor robots in a most natural and intuitive manner.
This goal is often impeded by a lack of knowledge of the human about the internal processing and expectations of the robot and by the inflexibility of the robot to understand open-ended, unconstrained tutoring or demonstration. Hence, we propose mixed-initiative strategies to allow both to mutually contribute to the interactive learning process as
a bi-directional negotiation about knowledge. Along this line this paper discusses two initially different case studies on object manipulation and learning of spatial environments. We present different styles of mixed-
initiative in these scenarios and discuss the merits in each case.},
    publisher = {IEEE},
    title = {Mixed initiative in interactive robotic learning},
    url = {http://eprints.lincoln.ac.uk/6923/},
    year = {2009}
}

@article{Kusumam_2017_Dvision_Based_Detection,
    author = {Keerthy Kusumam and Tom{\'{a}}{\v{s}} Krajn{\'{\i}}k and Simon Pearson and Tom Duckett and Grzegorz Cielniak},
    doi = {10.1002/rob.21726},
    journal = {Journal of Field Robotics},
    month = {jun},
    number = {8},
    pages = {1505--1518},
    publisher = {Wiley-Blackwell},
    title = {3D-vision based detection, localization, and sizing of broccoli heads in the field},
    url = {https://doi.org/10.1002%2Frob.21726},
    volume = {34},
    year = {2017}
}

@inproceedings{lirolem6918_Enhancing_Human_Cooperation,
    abstract = {Humans naturally use an impressive variety of ways to com-
municate. In this work, we investigate the possibilities of complementing these natural communication channels with articial ones. For this, augmented reality is used as a technique to add synthetic visual and auditory stimuli to people's perception. A system for the mutual display
of the gaze direction of two interactants is presented and its acceptance is shown through a study. Finally, future possibilities of promoting this novel concept of articial communication channels are explored},
    author = {Christian Mertes and Angelika Dierker and Thomas Hermann and Marc Hanheide and Gerhard Sagerer},
    booktitle = {Proceedings of the 13th International Conference on Human-Computer Interaction},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c0e88)},
    month = {July},
    note = {Humans naturally use an impressive variety of ways to com-
municate. In this work, we investigate the possibilities of complementing these natural communication channels with articial ones. For this, augmented reality is used as a technique to add synthetic visual and auditory stimuli to people's perception. A system for the mutual display
of the gaze direction of two interactants is presented and its acceptance is shown through a study. Finally, future possibilities of promoting this novel concept of articial communication channels are explored},
    pages = {447--451},
    publisher = {Springer},
    title = {Enhancing human cooperation with multimodal augmented reality},
    url = {http://eprints.lincoln.ac.uk/6918/},
    year = {2009}
}

@inproceedings{lirolem14423_Longterm_Topological_Localisation,
    abstract = {This paper presents a new approach for topological localisation of service robots in dynamic indoor environments. In contrast to typical localisation approaches that rely mainly on static parts of the environment, our approach makes explicit use of information about changes by learning and modelling the spatio-temporal dynamics of the environment where the robot is acting.  The proposed spatio-temporal world model is able to predict environmental changes in time, allowing the robot to improve its localisation capabilities during long-term operations in populated environments. To investigate the proposed approach, we have enabled a mobile robot to autonomously patrol a populated environment over a period of one week while building the proposed model representation. We demonstrate that the experience learned during one week is applicable for topological localization even after a hiatus of three months by showing that the localization error rate is significantly lower compared to static environment representations.},
    author = {Tomas Krajnik and Jaime Pulido Fentanes and Oscar Martinez Mozos and Tom Duckett and Johan Ekekrantz and Marc Hanheide},
    booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
    keywords = {ARRAY(0x7f43493bf110)},
    month = {September},
    publisher = {IEEE},
    title = {Long-term topological localisation for service robots in dynamic environments using spectral maps},
    url = {http://eprints.lincoln.ac.uk/14423/},
    year = {2014}
}

@article{lirolem6712_Cognitive_Vision_System,
    abstract = {The European Cognitive Vision project VAMPIRE uses mobile AR-kits to interact with a visual active memory for teaching and retrieval purposes. This paper describes concept and technical realization of the used mobile AR-kits and discusses interactive learning and retrieval in office environments, and the active memory infrastructure. The focus is on 3D interaction for pointing in a scene coordinate system. This is achieved by 3D augmented pointing, which combines inside-out tracking for head pose recovery and 3D stereo human?computer interaction. Experimental evaluation shows that the accuracy of this 3D cursor is within a few centimeters, which is sufficient to point at an object in an office. Finally, an application of the cursor in VAMPIRE is presented, where in addition to the mobile system, at least one stationary active camera is used to obtain different views of an object. There are many potential applications, for example an improved view-based object recognition.},
    author = {H. Siegl and Marc Hanheide and S. Wrede and A. Pinz},
    journal = {Image and Vision Computing},
    keywords = {ARRAY(0x7f43493c1218)},
    month = {December},
    note = {The European Cognitive Vision project VAMPIRE uses mobile AR-kits to interact with a visual active memory for teaching and retrieval purposes. This paper describes concept and technical realization of the used mobile AR-kits and discusses interactive learning and retrieval in office environments, and the active memory infrastructure. The focus is on 3D interaction for pointing in a scene coordinate system. This is achieved by 3D augmented pointing, which combines inside-out tracking for head pose recovery and 3D stereo human?computer interaction. Experimental evaluation shows that the accuracy of this 3D cursor is within a few centimeters, which is sufficient to point at an object in an office. Finally, an application of the cursor in VAMPIRE is presented, where in addition to the mobile system, at least one stationary active camera is used to obtain different views of an object. There are many potential applications, for example an improved view-based object recognition.},
    number = {12},
    pages = {1895--1903},
    publisher = {Elsevier},
    title = {An augmented reality human?computer interface for object localization in a cognitive vision system},
    url = {http://eprints.lincoln.ac.uk/6712/},
    volume = {25},
    year = {2007}
}

@article{lirolem16987_Qualitative_Trajectory_Calculus,
    abstract = {In this paper we propose a probabilistic sequential model of Human-Robot Spatial Interaction (HRSI) using a well-established Qualitative Trajectory Calculus (QTC) to encode HRSI between a human and a mobile robot in a meaningful, tractable, and systematic manner. Our key contribution is to utilise QTC as a state descriptor and model HRSI as a probabilistic sequence of such states. Apart from the sole direction of movements of human and robot modelled by QTC, attributes of HRSI like proxemics and velocity profiles play vital roles for the modelling and generation of HRSI behaviour. In this paper, we particularly present how the concept of proxemics can be embedded in QTC to facilitate richer models. To facilitate reasoning on HRSI with qualitative representations, we show how we can combine the representational power of QTC with the concept of proxemics in a concise framework, enriching our probabilistic representation by implicitly modelling distances. We show the appropriateness of our sequential model of QTC by encoding different HRSI behaviours observed in two spatial interaction experiments. We classify these encounters, creating a comparative measurement, showing the representational capabilities of the model.},
    author = {Christian Dondrup and Nicola Bellotto and Marc Hanheide and Kerstin Eder and Ute Leonards},
    journal = {Robotics},
    keywords = {ARRAY(0x7f43493bf008)},
    month = {March},
    note = {This article belongs to the Special Issue Representations and Reasoning for Robotics},
    number = {1},
    pages = {63--102},
    publisher = {MDPI},
    title = {A computational model of human-robot spatial interactions based on a qualitative trajectory calculus},
    url = {http://eprints.lincoln.ac.uk/16987/},
    volume = {4},
    year = {2015}
}

@article{lirolem6742_Computer_Vision_Systems,
    abstract = {Computer vision is becoming an integral part in human-machine interfaces as research increasingly aims at a seamless
and natural interaction between a user and an application system. Gesture recognition, context awareness, and grounding
concepts in the commonly perceived environment as well as in the interaction history are key abilities of such systems.
Simultaneously, recent computer vision research has indicated that integrated systems which are embedded in the world
and interact with their environment seem a prerequisite for solving more general vision tasks. Cognitive computer vision
systems which enable the generation of knowledge on the basis of perception, reasoning, and extension of prior models
are a major step towards this goal. For these, the integration, interaction and organization of memory becomes a key
issue in system design. In this article we will present a computational framework for integrated vision systems that is
centered around an active memory component. It supports a fast integration and substitution of system components,
various means of interaction patterns, and enables a system to reason about its own memory content. This framework
will be exemplified by means of a cognitive human-machine interface in an Augmented Reality scenario. The system is
able to acquire new concepts from interaction and provides a context aware scene augmentation for the user.},
    author = {Sven Wachsmuth and Sebastian Wrede and Marc Hanheide and Christian Bauckhage},
    journal = {KI - K\"unstliche Intelligenz},
    keywords = {ARRAY(0x7f43493c14b8)},
    month = {March},
    note = {Computer vision is becoming an integral part in human-machine interfaces as research increasingly aims at a seamless
and natural interaction between a user and an application system. Gesture recognition, context awareness, and grounding
concepts in the commonly perceived environment as well as in the interaction history are key abilities of such systems.
Simultaneously, recent computer vision research has indicated that integrated systems which are embedded in the world
and interact with their environment seem a prerequisite for solving more general vision tasks. Cognitive computer vision
systems which enable the generation of knowledge on the basis of perception, reasoning, and extension of prior models
are a major step towards this goal. For these, the integration, interaction and organization of memory becomes a key
issue in system design. In this article we will present a computational framework for integrated vision systems that is
centered around an active memory component. It supports a fast integration and substitution of system components,
various means of interaction patterns, and enables a system to reason about its own memory content. This framework
will be exemplified by means of a cognitive human-machine interface in an Augmented Reality scenario. The system is
able to acquire new concepts from interaction and provides a context aware scene augmentation for the user.},
    number = {2},
    pages = {25--31},
    publisher = {Springer for Fachbereiches KI in der Gesellschaft f\"ur Informatik},
    title = {An active memory model for cognitive computer vision systems},
    url = {http://eprints.lincoln.ac.uk/6742/},
    volume = {19},
    year = {2005}
}

@inproceedings{lirolem6919_Feedback_Interpretation_Based,
    abstract = {In everyday conversation besides speech people also communicate by means of nonverbal cues. Facial expressions are one important cue, as they can provide useful information about the conversation, for instance, whether the interlocutor seems to understand or appears to be puzzled. Similarly, in human-robot interaction facial expressions also give feedback about the interaction situation. We present a Wizard of Oz user study in an object-teaching scenario where subjects showed several objects to a robot and taught the objects' names. Afterward, the robot should term the objects correctly. In a first evaluation, we let other people watch short video sequences of this study. They decided by looking at the face of the human whether the answer of the robot was correct (unproblematic situation) or incorrect (problematic situation). We conducted the experiments under specific conditions by varying the amount of temporal and visual context information and compare the results with related experiments described in the literature.},
    author = {Christian Lang and Marc Hanheide and Manja Lohse and Heiko Wersing and Gerhard Sagerer},
    booktitle = {The 18th IEEE International Symposium on Robot and Human Interactive Communication},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c0dc8)},
    month = {September},
    note = {In everyday conversation besides speech people also communicate by means of nonverbal cues. Facial expressions are one important cue, as they can provide useful information about the conversation, for instance, whether the interlocutor seems to understand or appears to be puzzled. Similarly, in human-robot interaction facial expressions also give feedback about the interaction situation. We present a Wizard of Oz user study in an object-teaching scenario where subjects showed several objects to a robot and taught the objects' names. Afterward, the robot should term the objects correctly. In a first evaluation, we let other people watch short video sequences of this study. They decided by looking at the face of the human whether the answer of the robot was correct (unproblematic situation) or incorrect (problematic situation). We conducted the experiments under specific conditions by varying the amount of temporal and visual context information and compare the results with related experiments described in the literature.},
    pages = {189--194},
    publisher = {IEEE},
    title = {Feedback interpretation based on facial expressions in human-robot interaction},
    url = {http://eprints.lincoln.ac.uk/6919/},
    year = {2009}
}

@incollection{lirolem6714_Concept_In_Hri,
    abstract = {Mobile robots are already applied in factories and hospitals, merely to do a distinct task. It is envisioned that robots assist in households soon. Those service robots will have to cope with several situations and tasks and of course with sophisticated human-robot interactions (HRI). Therefore, a robot has not only to consider social rules with respect to proxemics, it must detect in which (interaction) situation it is in and act accordingly. With respect to spatial HRI, we concentrate on the use of non-verbal communication. This chapter stresses the meaning of both, machine movements as signals towards a human and human body language. Considering these aspects will make interaction simpler and smoother. An observational study is presented to acquire a concept of spatial prompting by a robot and by a human. When a person and robot meet in a narrow hallway in order to pass by, they have to make room for each other. But how can a robot make sure that both really want to pass by instead of starting interaction? This especially concerns narrow, non-artificial surroundings. Which social signals are expected by the user and on the other side, can be generated or processed by a robot? The results will show what an appropriate passing behaviour is and how to distinguish between passage situations and others. The results shed light upon the readability of signals in spatial HRI.},
    author = {Annika Peters and Thorsten P. Spexard and Marc Hanheide and Petra Weiss},
    booktitle = {Behaviour Monitoring and Interpretation - BMI Well-being},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493bfb00)},
    month = {April},
    note = {Mobile robots are already applied in factories and hospitals, merely to do a distinct task. It is envisioned that robots assist in households soon. Those service robots will have to cope with several situations and tasks and of course with sophisticated human-robot interactions (HRI). Therefore, a robot has not only to consider social rules with respect to proxemics, it must detect in which (interaction) situation it is in and act accordingly. With respect to spatial HRI, we concentrate on the use of non-verbal communication. This chapter stresses the meaning of both, machine movements as signals towards a human and human body language. Considering these aspects will make interaction simpler and smoother. An observational study is presented to acquire a concept of spatial prompting by a robot and by a human. When a person and robot meet in a narrow hallway in order to pass by, they have to make room for each other. But how can a robot make sure that both really want to pass by instead of starting interaction? This especially concerns narrow, non-artificial surroundings. Which social signals are expected by the user and on the other side, can be generated or processed by a robot? The results will show what an appropriate passing behaviour is and how to distinguish between passage situations and others. The results shed light upon the readability of signals in spatial HRI.},
    publisher = {IOS Press},
    title = {Hey robot, get out of my way: survey on a spatial and situational movement concept in HRI},
    url = {http://eprints.lincoln.ac.uk/6714/},
    year = {2011}
}

@incollection{Roberts_Elliott_2020_Humanrobot_Spatial_Interaction,
    author = {Laurence Roberts-Elliott and Manuel Fernandez-Carmona and Marc Hanheide},
    booktitle = {Towards Autonomous Robotic Systems},
    doi = {10.1007/978-3-030-63486-5_27},
    pages = {249--260},
    publisher = {Springer International Publishing},
    title = {Towards Safer Robot Motion: Using a Qualitative Motion Model to Classify Human-Robot Spatial Interaction},
    url = {https://doi.org/10.1007%2F978-3-030-63486-5_27},
    year = {2020}
}

@article{lirolem6561_Facial_Communicative_Signals,
    abstract = {From the issue entitled \"Measuring Human-Robots Interactions\"
This paper investigates facial communicative signals (head gestures, eye gaze, and facial expressions) as nonverbal feedback in human-robot interaction. Motivated by a discussion of the literature, we suggest scenario-specific investigations due to the complex nature of these signals and present an object-teaching scenario where subjects teach the names of objects to a robot, which in turn shall term these objects correctly afterwards. The robot?s verbal answers are to elicit facial communicative signals of its interaction partners. We investigated the human ability to recognize this spontaneous facial feedback and also the performance of two automatic recognition approaches. The first one is a static approach yielding baseline results, whereas the second considers the temporal dynamics and achieved classification rates},
    author = {Christian Lang and Sven Wachsmuth and Marc Hanheide and Heiko Wersing},
    journal = {International Journal of Social Robotics},
    keywords = {ARRAY(0x7f43493bf980)},
    month = {August},
    note = {From the issue entitled \"Measuring Human-Robots Interactions\"
This paper investigates facial communicative signals (head gestures, eye gaze, and facial expressions) as nonverbal feedback in human-robot interaction. Motivated by a discussion of the literature, we suggest scenario-specific investigations due to the complex nature of these signals and present an object-teaching scenario where subjects teach the names of objects to a robot, which in turn shall term these objects correctly afterwards. The robot?s verbal answers are to elicit facial communicative signals of its interaction partners. We investigated the human ability to recognize this spontaneous facial feedback and also the performance of two automatic recognition approaches. The first one is a static approach yielding baseline results, whereas the second considers the temporal dynamics and achieved classification rates},
    number = {3},
    pages = {249--262},
    publisher = {Springer},
    title = {Facial communicative signals: valence recognition in task-oriented human-robot interaction},
    url = {http://eprints.lincoln.ac.uk/6561/},
    volume = {4},
    year = {2012}
}

@incollection{Dayoub_2015_Adaptive_Spherical_Views,
    author = {Feras Dayoub and Grzegorz Cielniak and Tom Duckett},
    booktitle = {Springer Tracts in Advanced Robotics},
    doi = {10.1007/978-3-319-07488-7_26},
    pages = {379--392},
    publisher = {Springer International Publishing},
    title = {Eight Weeks of Episodic Visual Navigation Inside a Non-stationary Environment Using Adaptive Spherical Views},
    url = {https://doi.org/10.1007%2F978-3-319-07488-7_26},
    year = {2015}
}

@article{Cielniak2010_Visual_Place_Memories,
    author = {Dayoub, F. and Duckett, T. and Cielniak, G.},
    journal = {Proceedings of the International Symposium on Remembering Who We Are - Human Memory for Artificial Agents - A Symposium at the AISB 2010 Convention},
    pages = {21-26},
    title = {Short- and long-term adaptation of visual place memories for mobile robots},
    year = {2010}
}

@article{James_2022_Strawberry_Trait_Automation,
    author = {Katherine Margaret Frances James and Daniel James Sargent and Adam Whitehouse and Grzegorz Cielniak},
    doi = {10.1002/ppp3.10275},
    journal = {{PLANTS}, {PEOPLE}, {PLANET}},
    month = {jun},
    publisher = {Wiley},
    title = {High-throughput phenotyping for breeding targets{\textemdash}Current status and future directions of strawberry trait automation},
    url = {https://doi.org/10.1002%2Fppp3.10275},
    year = {2022}
}

@inproceedings{lirolem6938_Social_Robots,
    abstract = {In order to provide personalized services and to
develop human-like interaction capabilities robots need to rec-
ognize their human partner. Face recognition has been studied
in the past decade exhaustively in the context of security systems
and with significant progress on huge datasets. However, these
capabilities are not in focus when it comes to social interaction
situations. Humans are able to remember people seen for a
short moment in time and apply this knowledge directly in
their engagement in conversation. In order to equip a robot with
capabilities to recall human interlocutors and to provide user-
aware services, we adopt human-human interaction schemes to
propose a face memory on the basis of active appearance models
integrated with the active memory architecture. This paper
presents the concept of the interactive face memory, the applied
recognition algorithms, and their embedding into the robot?s
system architecture. Performance measures are discussed for
general face databases as well as scenario-specific datasets.},
    author = {Marc Hanheide and Sebastian Wrede and Christian Lang and Gerhard Sagerer},
    booktitle = {IEEE International Conference on Robotics and Automation},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1128)},
    month = {May},
    note = {In order to provide personalized services and to
develop human-like interaction capabilities robots need to rec-
ognize their human partner. Face recognition has been studied
in the past decade exhaustively in the context of security systems
and with significant progress on huge datasets. However, these
capabilities are not in focus when it comes to social interaction
situations. Humans are able to remember people seen for a
short moment in time and apply this knowledge directly in
their engagement in conversation. In order to equip a robot with
capabilities to recall human interlocutors and to provide user-
aware services, we adopt human-human interaction schemes to
propose a face memory on the basis of active appearance models
integrated with the active memory architecture. This paper
presents the concept of the interactive face memory, the applied
recognition algorithms, and their embedding into the robot?s
system architecture. Performance measures are discussed for
general face databases as well as scenario-specific datasets.},
    pages = {3660--3665},
    publisher = {IEEE},
    title = {Who am I talking with? A face memory for social robots},
    url = {http://eprints.lincoln.ac.uk/6938/},
    year = {2008}
}

@inproceedings{lirolem6944_Combining_Environmental_Cues,
    abstract = {As wearable sensors and computing hardware are becoming a reality,
new and unorthodox approaches to seamless human-computer
interaction can be explored. This paper presents the prototype of a
wearable, head-mounted device for advanced human-machine interaction
that integrates speech recognition and computer vision
with head gesture analysis based on inertial sensor data. We will
focus on the innovative idea of integrating visual and inertial data
processing for interaction. Fusing head gestures with results from
visual analysis of the environment provides rich vocabularies for
human-machine communication because it renders the environment
into an interface: if objects or items in the surroundings are
being associated with system activities, head gestures can trigger
commands if the corresponding object is being looked at. We will
explain the algorithmic approaches applied in our prototype and
present experiments that highlight its potential for assistive technology.
Apart from pointing out a new direction for seamless interaction
in general, our approach provides a new and easy to use
interface for disabled and paralyzed users in particular.},
    author = {Marc Hanheide and Christian Bauckhage and Gerhard Sagerer},
    booktitle = {7th international conference on Multimodal interfaces},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c13f8)},
    month = {October},
    note = {As wearable sensors and computing hardware are becoming a reality,
new and unorthodox approaches to seamless human-computer
interaction can be explored. This paper presents the prototype of a
wearable, head-mounted device for advanced human-machine interaction
that integrates speech recognition and computer vision
with head gesture analysis based on inertial sensor data. We will
focus on the innovative idea of integrating visual and inertial data
processing for interaction. Fusing head gestures with results from
visual analysis of the environment provides rich vocabularies for
human-machine communication because it renders the environment
into an interface: if objects or items in the surroundings are
being associated with system activities, head gestures can trigger
commands if the corresponding object is being looked at. We will
explain the algorithmic approaches applied in our prototype and
present experiments that highlight its potential for assistive technology.
Apart from pointing out a new direction for seamless interaction
in general, our approach provides a new and easy to use
interface for disabled and paralyzed users in particular.},
    pages = {25--31},
    publisher = {ACM Association of Computing Machinery},
    title = {Combining environmental cues \& head gestures to interact with wearable devices},
    url = {http://eprints.lincoln.ac.uk/6944/},
    year = {2005}
}

@article{Das_2023_Unified_Topological_Representation,
    author = {Das, Gautham and Cielniak, Grzegorz and Heselden, James and Pearson, Simon and Duchetto, Francesco Del and Zhu, Zuyuan and Dichtl, Johann and Hanheide, Marc and Fentanes, Jaime Pulido and Binch, Adam and Hutchinson, Michael and From, Pal},
    doi = {10.22541/au.169357512.24867804/v1},
    month = {September},
    publisher = {Authorea, Inc.},
    title = {A Unified Topological Representation for Robotic Fleets in Agricultural Applications},
    url = {http://dx.doi.org/10.22541/au.169357512.24867804/v1},
    year = {2023}
}

@inproceedings{lirolem13775_Facial_Communicative_Signal,
    abstract = {Facial communicative signals (FCSs) such as head gestures, eye gaze, and facial expressions can provide useful feedback in conversations between people and also in human-robot interaction. This paper presents a pattern recognition approach for the interpretation of FCSs in terms of valence, based on the selection of discriminative subsequences in video data. These subsequences capture important temporal dynamics and are used as prototypical reference subsequences in a classification procedure based on dynamic time warping and feature extraction with active appearance models. Using this valence classification, the robot can discriminate positive from negative interaction situations and react accordingly. The approach is evaluated on a database containing videos of people interacting with a robot by teaching the names of several objects to it. The verbal answer of the robot is expected to elicit the display of spontaneous FCSs by the human tutor, which were classified in this work. The achieved classification accuracies are comparable to the average human recognition performance and outperformed our previous results on this task. \^A\\copyright 2013 IEEE.},
    address = {Karlsruhe},
    author = {C. Lang and S. Wachsmuth and M. Hanheide and H. Wersing},
    booktitle = {IEEE International Conference on Robotics and Automation (ICRA) },
    keywords = {ARRAY(0x7f43493bf8f0)},
    month = {May},
    note = {Conference Code:100673},
    pages = {170--177},
    publisher = {IEEE},
    title = {Facial communicative signal interpretation in human-robot interaction by discriminative video subsequence selection},
    url = {http://eprints.lincoln.ac.uk/13775/},
    year = {2013}
}

@inproceedings{lirolem6941_Wearable_Assistance_System,
    abstract = {Enabling artificial systems to recognize human actions
is a requisite to develop intelligent assistance systems that
are able to instruct and supervise users in accomplishing
tasks. In order to enable an assistance system to be wearable,
head-mounted cameras allow to perceive a scene visually
from a user?s perspective. But realizing action recognition
without any static sensors causes special challenges.
The movement of the camera is directly related to the user?s
head motion and not controlled by the system. In this paper
we present how a trajectory-based action recognition can
be combined with object recognition, visual tracking, and a
background motion compensation to be applicable in such
a wearable assistance system. The suitability of our approach
is proved by user studies in an object manipulation
scenario.},
    author = {Marc Hanheide and Nils Hofemann and Gerhard Sagerer},
    booktitle = {18th International Conference on Pattern Recognition (ICPR'06)},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c1338)},
    month = {August},
    note = {Enabling artificial systems to recognize human actions
is a requisite to develop intelligent assistance systems that
are able to instruct and supervise users in accomplishing
tasks. In order to enable an assistance system to be wearable,
head-mounted cameras allow to perceive a scene visually
from a user?s perspective. But realizing action recognition
without any static sensors causes special challenges.
The movement of the camera is directly related to the user?s
head motion and not controlled by the system. In this paper
we present how a trajectory-based action recognition can
be combined with object recognition, visual tracking, and a
background motion compensation to be applicable in such
a wearable assistance system. The suitability of our approach
is proved by user studies in an object manipulation
scenario.},
    pages = {1254--1258},
    publisher = {IEEE Computer Society},
    title = {Action recognition in a wearable assistance system},
    url = {http://eprints.lincoln.ac.uk/6941/},
    year = {2006}
}

@inbook{Baxter_2019_Dialogue_Interactivity_Development,
    author = {Baxter, Paul and Del Duchetto, Francesco and Hanheide, Marc},
    booktitle = {Educational Robotics in the Context of the Maker Movement},
    doi = {10.1007/978-3-030-18141-3_12},
    isbn = {9783030181413},
    issn = {2194-5365},
    month = {December},
    pages = {147–160},
    publisher = {Springer International Publishing},
    title = {Engaging Learners in Dialogue Interactivity Development for Mobile Robots},
    url = {http://dx.doi.org/10.1007/978-3-030-18141-3_12},
    year = {2019}
}

@article{Cielniak2014_Procedural_Story_Generation,
    author = {Wagg, K. and Cielniak, G.},
    journal = {15th International Conference on Intelligent Games and Simulation, GAME-ON 2014},
    pages = {97-103},
    title = {Procedural story generation in games},
    year = {2014}
}

@inproceedings{lirolem13570_Humanrobot_Headon_Encounters,
    abstract = {The motivation for this research stems from the future vision of being able to buy a mobile service robot for your own household, unpack it, switch it on, and have it behave in an intelligent way; but of course it also has to adapt to your personal preferences over time. My work is focusing on the spatial aspect of the robot?s behaviours, which means when it is moving in a confined, shared space with a human it will also take the communicative character of these movements into account. This adaptation to the users preferences should come from experience which the robot gathers throughout several days or months of interaction and not from a programmer hard-coding certain behaviours},
    author = {Christian Dondrup and Christina Lichtenthaeler and Marc Hanheide},
    booktitle = {9th ACM/IEEE International Conference on Human Robot Interaction},
    keywords = {ARRAY(0x7f43493bf0c8)},
    month = {March},
    pages = {154--155},
    publisher = {IEEE},
    title = {Hesitation signals in human-robot head-on encounters: a pilot study},
    url = {http://eprints.lincoln.ac.uk/13570/},
    year = {2014}
}

@inproceedings{lirolem6922_Multimodal_Augmented_Reality,
    abstract = {We present an Augmented Reality (AR) system to support
collaborative tasks in a shared real-world interaction space
by facilitating joint attention. The users are assisted by information about their interaction partner's field of view both visually and acoustically. In our study, the audiovisual improvements are compared with an AR system without these support mechanisms in terms of the participants' reaction times and error rates. The participants performed a simple object-choice task we call the gaze game to ensure controlled experimental conditions. Additionally, we asked the subjects to fill in a questionnaire to gain subjective feedback from them. We were able to show an improvement for both dependent variables as well as positive feedback for the visual augmentation in the questionnaire.},
    author = {Angelika Dierker and Christian Mertes and Thomas Hermann and Marc Hanheide and Gerhard Sagerer},
    booktitle = {International Conference on Multimodal interfaces - ICMI-MLMI '09},
    editor = {B. Gottfried and H. Aghajan},
    keywords = {ARRAY(0x7f43493c0d68)},
    month = {November},
    title = {Mediated attention with multimodal augmented reality},
    url = {http://eprints.lincoln.ac.uk/6922/},
    year = {2009}
}

@incollection{Dayoub_2011_Adaptive_Appearancebased_Map,
    author = {Feras Dayoub and Grzegorz Cielniak and Tom Duckett},
    booktitle = {Towards Autonomous Robotic Systems},
    doi = {10.1007/978-3-642-23232-9_47},
    pages = {400--401},
    publisher = {Springer Berlin Heidelberg},
    title = {Long-Term Experiment Using an Adaptive Appearance-Based Map for Visual Navigation by Mobile Robots},
    url = {https://doi.org/10.1007%2F978-3-642-23232-9_47},
    year = {2011}
}

@inbook{Kirk_2021_Soft_Fruit_Mass,
    author = {Kirk, Raymond and Mangan, Michael and Cielniak, Grzegorz},
    booktitle = {Computer Vision Systems},
    doi = {10.1007/978-3-030-87156-7_18},
    isbn = {9783030871567},
    issn = {1611-3349},
    pages = {223–233},
    publisher = {Springer International Publishing},
    title = {Non-destructive Soft Fruit Mass and Volume Estimation for Phenotyping in Horticulture},
    url = {http://dx.doi.org/10.1007/978-3-030-87156-7_18},
    year = {2021}
}

@article{Bosilj_2018_Precision_Agriculture,
    author = {Petra Bosilj and Tom Duckett and Grzegorz Cielniak},
    doi = {10.1109/lra.2018.2848305},
    journal = {{IEEE} Robotics and Automation Letters},
    month = {oct},
    number = {4},
    pages = {2950--2956},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    title = {Analysis of Morphology-Based Features for Classification of Crop and Weeds in Precision Agriculture},
    url = {https://doi.org/10.1109%2Flra.2018.2848305},
    volume = {3},
    year = {2018}
}

@incollection{lirolem6715_Learning_By_Interacting,
    abstract = {Abstract},
    author = {Britte Wrede and Katharina J. Rohlfing and Marc Hanheide and Gerhard Sagerer},
    booktitle = {Creating brain-like intelligence: from basic principles to complex intelligent systems},
    editor = {Bernhard Sendhoff and Edgar Korner and Olaf Sporns and Helge Ritter and Kenji Doya},
    keywords = {ARRAY(0x7f43493bfcb0)},
    month = {November},
    note = {Abstract},
    number = {5436},
    publisher = {Springer},
    series = {Lecture Notes in Computer Science},
    title = {Towards learning by interacting},
    url = {http://eprints.lincoln.ac.uk/6715/},
    year = {2009}
}

@article{Zahidi_2024_Optimising_Robotic_Operation,
    author = {Zahidi, Usman A. and Khan, Arshad and Zhivkov, Tsvetan and Dichtl, Johann and Li, Dom and Parsa, Soran and Hanheide, Marc and Cielniak, Grzegorz and Sklar, Elizabeth I. and Pearson, Simon and Ghalamzan‐E., Amir},
    doi = {10.1002/rob.22384},
    issn = {1556-4967},
    journal = {Journal of Field Robotics},
    month = {July},
    publisher = {Wiley},
    title = {Optimising robotic operation speed with edge computing via 5G network: Insights from selective harvesting robots},
    url = {http://dx.doi.org/10.1002/rob.22384},
    year = {2024}
}
