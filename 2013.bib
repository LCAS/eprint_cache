@article{lincoln12817,
          volume = {2013},
           month = {December},
          author = {Pin Shen Teh and Andrew Beng Jin Teoh and Shigang Yue},
           title = {A survey of keystroke dynamics biometrics},
       publisher = {Hindawi Publishing Corporation / Scientific World},
            year = {2013},
         journal = {The Scientific World Journal},
             doi = {10.1155/2013/408280},
           pages = {408280},
        keywords = {ARRAY(0x55a24bc3ca20)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/12817/},
        abstract = {Research on keystroke dynamics biometrics has been increasing, especially in the last decade. The main motivation behind this effort is due to the fact that keystroke dynamics biometrics is economical and can be easily integrated into the existing computer security systems with minimal alteration and user intervention. Numerous studies have been conducted in terms of data acquisition devices, feature representations, classification methods, experimental protocols, and evaluations. However, an up-to-date extensive survey and evaluation is not yet available. The objective of this paper is to provide an insightful survey and comparison on keystroke dynamics biometrics research performed throughout the last three decades, as well as offering suggestions and possible future research directions.}
}

@inproceedings{lincoln25785,
       booktitle = {Advances in Neural Information Processing Systems, (NIPS)},
           month = {December},
           title = {Probabilistic movement primitives},
          author = {A. Paraschos and C. Daniel and J. Peters and G. Neumann},
            year = {2013},
         journal = {Advances in Neural Information Processing Systems},
        keywords = {ARRAY(0x55a24bb3f1c0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/25785/},
        abstract = {Movement Primitives (MP) are a well-established approach for representing modular
and re-usable robot movement generators. Many state-of-the-art robot learning
successes are based MPs, due to their compact representation of the inherently
continuous and high dimensional robot movements. A major goal in robot learning
is to combine multiple MPs as building blocks in a modular control architecture
to solve complex tasks. To this effect, a MP representation has to allow for
blending between motions, adapting to altered task variables, and co-activating
multiple MPs in parallel. We present a probabilistic formulation of the MP concept
that maintains a distribution over trajectories. Our probabilistic approach
allows for the derivation of new operations which are essential for implementing
all aforementioned properties in one framework. In order to use such a trajectory
distribution for robot movement control, we analytically derive a stochastic feedback
controller which reproduces the given trajectory distribution. We evaluate
and compare our approach to existing methods on several simulated as well as
real robot scenarios.}
}

@inproceedings{lincoln12670,
       booktitle = {16th International Conference on Advanced Robotics (ICAR 2013)},
           month = {November},
           title = {External localization system for mobile robotics},
          author = {Tomas Krajnik and Matias Nitsche and Jan Faigl and Marta Mejail and Libor Preucil and Tom Duckett},
       publisher = {IEEE},
            year = {2013},
         journal = {International Conference on Advanced Robotics, ICAR 2013 (Proceedings)},
        keywords = {ARRAY(0x55a24bb3f1a8)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/12670/},
        abstract = {We present a fast and precise vision-based software intended for multiple robot localization. The core component of
the proposed localization system is an efficient method for black and white circular pattern detection. The method is robust to variable lighting conditions, achieves sub-pixel precision, and its computational complexity is independent of the processed image size. With off-the-shelf computational equipment and low-cost camera, its core algorithm is able to process hundreds of images per second while tracking hundreds of objects with millimeter precision. We propose a mathematical model of the method that allows to calculate its precision, area of coverage, and processing speed from the camera?s intrinsic parameters and hardware?s processing capacity. The correctness of the presented model and
performance of the algorithm in real-world conditions are verified in several experiments. Apart from the method description, we also publish its source code; so, it can be used as an enabling technology for various mobile robotics problems.}
}

@inproceedings{lincoln13757,
           month = {November},
          author = {Gabriel Zahi and Shigang Yue},
       booktitle = {Modelling Symposium (EMS), 2013 European},
           title = {Automatic detection of low light images in a video sequence Shot under different light conditions},
       publisher = {IEEE},
             doi = {10.1109/EMS.2013.47},
           pages = {271--276},
            year = {2013},
        keywords = {ARRAY(0x55a24b84d318)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/13757/},
        abstract = {Nocturnal insects have the ability to neurally sum visual signals in space and time to be able to see under very low light conditions. This ability shown by nocturnal insects has inspired many researchers to develop a night vision algorithm, that is capable of significantly improving the quality and reliability of digital images captured under very low light conditions. This algorithm however when applied to day time images rather degrades their quality. It is therefore not suitable to apply the night vision algorithms equally to an image stream with different light conditions. This paper introduces a quick method of automatically determining when to apply the nocturnal vision algorithm by analysing the cumulative intensity histogram of each image in the stream. The effectiveness of this method is demonstrated with relevant experiments in a good and acceptable way.}
}

@inproceedings{lincoln11637,
       booktitle = {International Conference on Social Robotics (ICSR)},
           month = {October},
           title = {Qualitative design and implementation of human-robot spatial interactions},
          author = {Nicola Bellotto and Marc Hanheide and Nico Van de Weghe},
       publisher = {Springer},
            year = {2013},
             doi = {10.1007/978-3-319-02675-6\_33},
        keywords = {ARRAY(0x55a24bd001e0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/11637/},
        abstract = {Despite the large number of navigation algorithms available for mobile robots, in many social contexts they often exhibit inopportune motion behaviours in proximity of people, often with very "unnatural" movements due to the execution of segmented trajectories or the sudden activation of safety mechanisms (e.g., for obstacle avoidance). We argue that the reason of the problem is not only the difficulty of modelling human behaviours and generating opportune robot control policies, but also the way human-robot spatial interactions are represented and implemented.
In this paper we propose a new methodology based on a qualitative representation of spatial interactions, which is both flexible and compact, adopting the well-defined and coherent formalization of Qualitative Trajectory Calculus (QTC). We show the potential of a QTC-based approach to abstract and design complex robot behaviours, where the desired robot's behaviour is represented together with its actual performance in one coherent approach, focusing on spatial interactions rather than pure navigation problems.}
}

@inproceedings{lincoln25693,
          volume = {2015-F},
          number = {Februa},
           month = {October},
          author = {A. Paraschos and G. Neumann and J. Peters},
       booktitle = {13th IEEE-RAS International Conference on  Humanoid Robots (Humanoids)},
           title = {A probabilistic approach to robot trajectory generation},
       publisher = {IEEE},
            year = {2013},
             doi = {10.1109/HUMANOIDS.2013.7030017},
           pages = {477--483},
        keywords = {ARRAY(0x55a24bcca810)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/25693/},
        abstract = {Motor Primitives (MPs) are a promising approach for the data-driven acquisition as well as for the modular and re-usable generation of movements. However, a modular control architecture with MPs is only effective if the MPs support co-activation as well as continuously blending the activation from one MP to the next. In addition, we need efficient mechanisms to adapt a MP to the current situation. Common approaches to movement primitives lack such capabilities or their implementation is based on heuristics. We present a probabilistic movement primitive approach that overcomes the limitations of existing approaches. We encode a primitive as a probability distribution over trajectories. The representation as distribution has several beneficial properties. It allows encoding a time-varying variance profile. Most importantly, it allows performing new operations - a product of distributions for the co-activation of MPs conditioning for generalizing the MP to different desired targets. We derive a feedback controller that reproduces a given trajectory distribution in closed form. We compare our approach to the existing state-of-the art and present real robot results for learning from demonstration.}
}

@inproceedings{lincoln11636,
       booktitle = {IEEE SMC Int. Workshop on Human-Machine Systems, Cyborgs and Enhancing Devices (HUMASCEND)},
           month = {October},
           title = {A multimodal smartphone interface for active perception by visually impaired},
          author = {Nicola Bellotto},
       publisher = {IEEE},
            year = {2013},
        keywords = {ARRAY(0x55a24b8a62b0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/11636/},
        abstract = {The diffuse availability of mobile devices, such as smartphones and tablets, has the potential to bring substantial benefits to the people with sensory impairments. The solution proposed in this paper is part of an ongoing effort to create an accurate obstacle and hazard detector for the visually impaired, which is embedded in a hand-held device. In particular, it presents a proof of concept for a multimodal interface to control the orientation of a smartphone's camera, while being held by a person, using a combination of vocal messages, 3D sounds and vibrations. The solution, which is to be evaluated experimentally by users, will enable further research in the area of active vision with human-in-the-loop, with potential application to mobile assistive devices for indoor navigation of visually impaired people.}
}

@article{lincoln23076,
          volume = {6},
           month = {October},
          author = {Paul E. Baxter and Joachim de Greeff and Tony Belpaeme},
           title = {Cognitive architecture for human?robot interaction: towards behavioural alignment},
       publisher = {Elsevier B.V.},
            year = {2013},
         journal = {Biologically Inspired Cognitive Architectures},
             doi = {10.1016/j.bica.2013.07.002},
           pages = {30--39},
        keywords = {ARRAY(0x55a24b8b00e0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/23076/},
        abstract = {Abstract With increasingly competent robotic systems desired and required for social human?robot interaction comes the necessity for more complex means of control. Cognitive architectures (specifically the perspective where principles of structure and function are sought to account for multiple cognitive competencies) have only relatively recently been considered for applica- tion to this domain. In this paper, we describe one such set of architectural principles ? acti- vation dynamics over a developmental distributed associative substrate ? and show how this enables an account of a fundamental competence for social cognition: multi-modal behavioural alignment. Data from real human?robot interactions is modelled using a computational system based on this set of principles to demonstrate how this competence can therefore be consid- ered as embedded in wider cognitive processing. It is shown that the proposed system can model the behavioural characteristics of human subjects. While this study is a simulation using real interaction data, the results obtained validate the application of the proposed approach to this issue.}
}

@article{lincoln12768,
          volume = {61},
          number = {10},
           month = {October},
          author = {Tom Duckett and Achim Lilienthal},
            note = {Selected Papers from the 5th European Conference on Mobile Robots (ECMR 2011)},
           title = {Editorial},
       publisher = {Elsevier for North-Holland / Intelligent Autonomous Systems (IAS) Society},
            year = {2013},
         journal = {Robotics and Autonomous Systems},
             doi = {10.1016/j.robot.2013.01.005},
           pages = {1049--1050},
        keywords = {ARRAY(0x55a24bcd6ff0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/12768/},
        abstract = {.}
}

@article{lincoln13793,
          volume = {27},
          number = {06},
           month = {September},
          author = {Jiawei Xu and Shigang Yue and Yuchao Tang},
           title = {A motion attention model based on rarity weighting and motion cues in dynamic scenes},
       publisher = {World Scientific Publishing},
            year = {2013},
         journal = {International Journal of Pattern Recognition and Artificial Intelligence},
             doi = {10.1142/S0218001413550094},
           pages = {1355009},
        keywords = {ARRAY(0x55a24bccfb58)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/13793/},
        abstract = {Nowadays, motion attention model is a controversial topic in the biological computer vision area. The computational attention model can be decomposed into a set of features via predefined channels. Here we designed a bio-inspired vision attention model, and added the rarity measurement onto it. The priority of rarity is emphasized under the assumption of weighting effect upon the features logic fusion. At this stage, a final saliency map at each frame is adjusted by the spatiotemporal and rarity values. By doing this, the process of mimicking human vision attention becomes more realistic and logical to the real circumstance. The experiments are conducted on the benchmark dataset of static images and video sequences. We simulated the attention shift based on several dataset. Most importantly, our dynamic scenes are mostly selected from the objects moving on the highway and dynamic scenes. The former one can be developed on the detection of car collision and will be a useful tool for further application in robotics. We also conduct experiment on the other video clips to prove the rationality of rarity factor and feature cues fusion methods. Finally, the evaluation results indicate our visual attention model outperforms several state-of-the-art motion attention models.


Read More: http://www.worldscientific.com/doi/abs/10.1142/S0218001413550094}
}

@article{lincoln28029,
          volume = {2},
          number = {1-2},
           month = {August},
          author = {M. P. Deisenroth and G. Neumann and J. Peters},
           title = {A survey on policy search for robotics},
       publisher = {Now Publishers},
            year = {2013},
         journal = {Foundations and Trends in Robotics},
             doi = {10.1561/2300000021},
           pages = {388--403},
        keywords = {ARRAY(0x55a24b84d228)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/28029/},
        abstract = {Policy search is a subfield in reinforcement learning which focuses on
finding good parameters for a given policy parametrization. It is well
suited for robotics as it can cope with high-dimensional state and action
spaces, one of the main challenges in robot learning. We review recent
successes of both model-free and model-based policy search in robot
learning.
Model-free policy search is a general approach to learn policies
based on sampled trajectories. We classify model-free methods based on
their policy evaluation strategy, policy update strategy, and exploration
strategy and present a unified view on existing algorithms. Learning a
policy is often easier than learning an accurate forward model, and,
hence, model-free methods are more frequently used in practice. However,
for each sampled trajectory, it is necessary to interact with the
* Both authors contributed equally.
robot, which can be time consuming and challenging in practice. Modelbased
policy search addresses this problem by first learning a simulator
of the robot?s dynamics from data. Subsequently, the simulator generates
trajectories that are used for policy learning. For both modelfree
and model-based policy search methods, we review their respective
properties and their applicability to robotic systems.}
}

@inproceedings{lincoln11330,
       booktitle = {Towards Autonomous Robotic Systems},
           month = {August},
           title = {Evaluation of laser range-finder mapping for agricultural spraying vehicles},
          author = {Francisco-Angel Moreno and Grzegorz Cielniak and Tom Duckett},
            year = {2013},
           pages = {210--221},
             doi = {10.1007/978-3-662-43645-5\_22},
        keywords = {ARRAY(0x55a24b8ad048)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/11330/},
        abstract = {In this paper, we present a new application of laser range-finder sensing to agricultural spraying vehicles. The current generation of spraying vehicles use automatic controllers to maintain the height of the sprayer booms above the crop.
However, these control systems are typically based on ultrasonic sensors mounted on the booms, which limits the accuracy of the measurements and the response of the controller to changes in the terrain, resulting in a sub-optimal spraying process. To overcome these limitations, we propose to use a laser scanner, attached to the front of the sprayer's cabin, to scan the ground surface in front of the vehicle and to build a scrolling 3d map of the terrain. We evaluate the proposed solution in a series of field tests, demonstrating that the approach provides a more detailed and accurate representation of the environment than the current sonar-based solution, and which can lead to the development of more efficient boom control systems.}
}

@misc{lincoln53189,
       booktitle = {Living machines: An exhibition of biomimetic and biohybrid technologies and artworks},
           month = {August},
           title = {Living machines: An exhibition of biomimetic and biohybrid technologies and artworks},
          author = {Tony Prescott and Paul Verschure and Charles Fox and Anna Mura and Stuart Wilson and Gill Ryder},
            year = {2013},
        keywords = {ARRAY(0x55a24bb32868)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/53189/},
        abstract = {Living Machines is an international conference series concerned with the development of future real-world technologies that harness the principles underlying living systems and the flow of communication signals between living and artificial systems.  The conference highlights the most exciting contemporary research in biomimetics{--}the development of ovel technologies through the distillation of principles from the study of biological systems, and biohybrids{--}formed by combining a biological component{--}an existing living system{--}with an artificial, newly-engineered component. The concept of ?Living Machine? captures the insight that useful artificial entities can be designed by copying life, and, at the same time, that we can understand biological organisms, including ourselves, as living machines ?designed? by nature. Some of the most interesting new developments in biomimetic and biohybrid technologies, grouped under five themes, together with some striking examples of contemporary biomimetic or biohybrid art, have been selected for  presentation at the Living Machines Exhibition, a one-day event at the Science Museum in London.  Highlights of the 2013 Living Machines exhibition include:
? A musical performance featuring the iCub humanoid robot
? Mammal-like robots with whiskered touch systems
? A robot model of fossilised animal behaviour from the dawn of life
? Biomimetic medical devices including a wasp-like needle for minimally-invasive
surgery
? A robot that powers itself by digesting human waste
? Micro-flying robots, worm, octopus, fish and mammal-like robots
? Biohybrid clothing made with living cells and robots controlled by slime mould
? Live visual art generated by the Artificial Intelligence AARON, created by Harold Cohen
? A string quartet performing music generated by the Artificial Intelligence EMI, created
by David Cope.}
}

@article{lincoln25777,
           month = {July},
           title = {Data-efficient generalization of robot skills with contextual policy search},
          author = {A. G. Kupcsik and M. P. Deisenroth and J. Peters and Gerhard Neumann},
            year = {2013},
           pages = {1401--1407},
            note = {27th AAAI Conference on Artificial Intelligence, AAAI 2013; Bellevue, WA; United States; 14 - 18 July 2013},
         journal = {Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013},
        keywords = {ARRAY(0x55a24bd099d0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/25777/},
        abstract = {In robotics, controllers make the robot solve a task within a specific context. The context can describe the objectives of
the robot or physical properties of the environment and is always specified before task execution. To generalize the controller to multiple contexts, we follow a hierarchical approach for policy learning: A lower-level policy controls the robot for a given context and an upper-level policy generalizes among contexts. Current approaches for learning such upper-level policies are based on model-free policy search, which require an excessive number of interactions of the robot with its environment.
More data-efficient policy search approaches are model based but, thus far, without the capability of learning
hierarchical policies. We propose a new model-based policy search approach that can also learn contextual upper-level
policies. Our approach is based on learning probabilistic forward models for long-term predictions. Using these  redictions, we use information-theoretic insights to improve the upper-level policy. Our method achieves a substantial improvement in learning speed compared to existing methods on simulated and real robotic tasks.}
}

@inproceedings{lincoln51742,
       booktitle = {Conference on Biomimetic and Biohybrid Systems},
           month = {June},
           title = {Where wall-following works: case study of simple
heuristics vs. optimal exploratory behaviour},
          author = {Charles Fox},
            year = {2013},
        keywords = {ARRAY(0x55a24b8b5e90)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/51742/},
        abstract = {Where wall-following works: case study of simple heuristics vs. optimal exploratory behaviour}
}

@phdthesis{lincoln29374,
           month = {June},
           title = {Haptic role allocation and intention negotiation in human-robot collaboration},
          school = {Koc University},
          author = {Ayse Kucukyilmaz},
            year = {2013},
        keywords = {ARRAY(0x55a24bc6f478)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/29374/},
        abstract = {This dissertation aims to present a perspective to build more natural shared control systems for physical human-robot cooperation. As the tasks become more complex and more dynamic, many shared control schemes fail to meet the expectation of an effortless interaction that resembles human-human sensory communication. Since such systems are mainly built to improve task performance, the richness of sensory communication is of secondary concern. We suggest that effective cooperation can be achieved when the human?s and the robot?s roles within the task are dynamically updated during the execution of the task. These roles define states for the system, in which the robot?s control leads or follows the human?s actions. In such a system, a state transition can occur at certain times if the robot can determine the user?s intention for gaining/relinquishing control. Specifically, with these state transitions we assign certain roles to the human and the robot. We believe that only by employing the robot with tools to change its behavior during collaboration, we can improve the collaboration experience. 

We explore how human-robot cooperation in virtual and physical worlds can be improved using a force-based role-exchange mechanism. Our findings indicate that the proposed role exchange framework is beneficial in a sense that it can improve task performance and the efficiency of the partners during the task, and decrease the energy requirement of the human. Moreover, the results imply that the subjective acceptability of the proposed model is attained only when role exchanges are performed in a smooth and transparent fashion. Finally, we illustrate that adding extra sensory cues on top of a role exchange scheme is useful for improving the sense of interaction during the task, as well as making the system more comfortable and easier to use, and the task more enjoyable.}
}

@article{lincoln9307,
          volume = {5},
          number = {2},
           month = {June},
          author = {Shigang Yue and F. Claire Rind},
           title = {Redundant neural vision systems: competing for collision recognition roles},
       publisher = {IEEE / Institute of Electrical and Electronics Engineers Incorporated},
            year = {2013},
         journal = {IEEE Transactions on Autonomous Mental Development},
             doi = {10.1109/TAMD.2013.2255050},
           pages = {173--186},
        keywords = {ARRAY(0x55a24bcfbd60)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/9307/},
        abstract = {Ability to detect collisions is vital for future robots that interact with humans in complex visual environments. Lobula giant movement detectors (LGMD) and directional selective neurons (DSNs) are two types of identified neurons found in the visual pathways of insects such as locusts. Recent modelling studies showed that the LGMD or grouped DSNs could each be tuned for collision recognition. In both biological and artificial vision systems, however, which one should play the collision recognition role and the way the two types of specialized visual neurons could be functioning together are not clear. In this modeling study, we compared the competence of the LGMD and the DSNs, and also investigate the cooperation of the two neural vision systems for collision recognition via artificial evolution. We implemented three types of collision recognition neural subsystems ? the LGMD, the DSNs and a hybrid system which combines the LGMD and the DSNs subsystems together, in each individual agent. A switch gene determines which of the three redundant neural subsystems plays the collision recognition role. We found that, in both robotics and driving environments, the LGMD was able to build up its ability for collision recognition quickly and robustly therefore reducing the chance of other types of neural networks to play the same role. The results suggest that the LGMD neural network could be the ideal model to be realized in hardware for collision recognition.}
}

@manual{lincoln22903,
           month = {May},
            type = {Documentation},
           title = {FUBUTEC-ECEC'2013},
          author = {Cristina Cherino and Grzegorz Cielniak and Patrick Dickinson and Philippe Geril},
       publisher = {EUROSIS-ETI BVBA},
            year = {2013},
            note = {FUBUTEC'2013, Future Business Technology Conference, June 10-12, 2013, University of Lincoln, Lincoln, UK},
        keywords = {ARRAY(0x55a24bcde618)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/22903/},
        abstract = {This edition covers Risk Management, Management Techniques, Production Design Optimization and Video Applications}
}

@article{lincoln29366,
          volume = {6},
          number = {1},
           month = {May},
          author = {Ayse Kucukyilmaz and Tevfik Metin Sezgin and Cagatay Basdogan},
           title = {Intention recognition for dynamic role exchange in haptic collaboration},
       publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
            year = {2013},
         journal = {IEEE Transactions on Haptics},
             doi = {10.1109/TOH.2012.21},
           pages = {58--68},
        keywords = {ARRAY(0x55a24b83f8c0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/29366/},
        abstract = {In human-computer collaboration involving haptics, a key issue that remains to be solved is to establish an intuitive communication between the partners. Even though computers are widely used to aid human operators in teleoperation, guidance, and training, since they lack the adaptability, versatility, and  awareness of a human, their ability to improve efficiency and effectiveness in dynamic tasks is limited. We suggest that the communication between a human and a computer can be improved if it involves a decision making process in which the computer is programmed to infer the intentions of the human operator and dynamically adjust the control levels of the interacting parties to facilitate a more intuitive interaction setup. In this paper, we investigate the utility of such a dynamic role exchange mechanism where partners negotiate through the haptic channel to trade their control levels on a collaborative task. We examine the energy consumption, the work done on the manipulated object, and the joint efficiency in addition to the task performance. We show that when compared to an equal control condition, a role exchange mechanism improves task performance and the joint efficiency of the partners. We also show that augmenting the system with additional informative visual and vibrotactile cues, which are used to display the state of interaction, allows the users to become aware of the underlying role exchange mechanism and utilize it in favor of the task. These cues also improve the user's sense of interaction and reinforce his/her belief that the computer aids with the execution of the task.}
}

@inproceedings{lincoln25781,
           month = {May},
          author = {C. Daniel and G. Neumann and O. Kroemer and J. Peters},
            note = {cited By 3},
       booktitle = {IEEE International Conference on Robotics and Automation},
           title = {Learning sequential motor tasks},
            year = {2013},
         journal = {Proceedings - IEEE International Conference on Robotics and Automation},
             doi = {10.1109/ICRA.2013.6630937},
           pages = {2626--2632},
        keywords = {ARRAY(0x55a24b8eda20)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/25781/},
        abstract = {Many real robot applications require the sequential use of multiple distinct motor primitives. This requirement implies the need to learn the individual primitives as well as a strategy to select the primitives sequentially. Such hierarchical learning problems are commonly either treated as one complex monolithic problem which is hard to learn, or as separate tasks learned in isolation. However, there exists a strong link between the robots strategy and its motor primitives. Consequently, a consistent framework is needed that can learn jointly on the level of the individual primitives and the robots strategy. We present a hierarchical learning method which improves individual motor primitives and, simultaneously, learns how to combine these motor primitives sequentially to solve complex motor tasks. We evaluate our method on the game of robot hockey, which is both difficult to learn in terms of the required motor primitives as well as its strategic elements.}
}

@inproceedings{lincoln13775,
           month = {May},
          author = {C. Lang and S. Wachsmuth and M. Hanheide and H. Wersing},
            note = {Conference Code:100673},
       booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
           title = {Facial communicative signal interpretation in human-robot interaction by discriminative video subsequence selection},
         address = {Karlsruhe},
       publisher = {IEEE},
            year = {2013},
             doi = {10.1109/ICRA.2013.6630572},
           pages = {170--177},
        keywords = {ARRAY(0x55a24b84ccb8)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/13775/},
        abstract = {Facial communicative signals (FCSs) such as head gestures, eye gaze, and facial expressions can provide useful feedback in conversations between people and also in human-robot interaction. This paper presents a pattern recognition approach for the interpretation of FCSs in terms of valence, based on the selection of discriminative subsequences in video data. These subsequences capture important temporal dynamics and are used as prototypical reference subsequences in a classification procedure based on dynamic time warping and feature extraction with active appearance models. Using this valence classification, the robot can discriminate positive from negative interaction situations and react accordingly. The approach is evaluated on a database containing videos of people interacting with a robot by teaching the names of several objects to it. The verbal answer of the robot is expected to elicit the display of spontaneous FCSs by the human tutor, which were classified in this work. The achieved classification accuracies are comparable to the average human recognition performance and outperformed our previous results on this task. {\^A}{\copyright} 2013 IEEE.}
}

@inproceedings{lincoln7880,
           month = {May},
          author = {Christian Lang and Sven Wachsmuth and Marc Hanheide and Heiko Wersing},
            note = {Facial communicative signals (FCSs) such as head gestures, eye gaze, and facial expressions can provide useful feedback in conversations between people and also in humanrobot interaction. This paper presents a pattern recognition approach for the interpretation of FCSs in terms of valence, based on the selection of discriminative subsequences in video data. These subsequences capture important temporal dynamics and are used as prototypical reference subsequences in a classi?cation procedure based on dynamic time warping and feature extraction with active appearance models. Using this valence classi?cation, the robot can discriminate positive from negative interaction situations and react accordingly. The approach is evaluated on a database containing videos of people interacting with a robot by teaching the names of several objects to it. The verbal answer of the robot is expected to elicit the display of spontaneous FCSs by the human tutor, which were classi?ed in this work. The achieved classi?cation accuracies are comparable to the average human recognition performance and outperformed our previous results on this task.},
       booktitle = {International Conference on Robotics and Automation (ICRA)},
           title = {Facial communicative signal interpretation in human-robot interaction by discriminative video subsequence selection},
       publisher = {IEEE},
            year = {2013},
             doi = {10.1109/ICRA.2013.6630572},
           pages = {170--177},
        keywords = {ARRAY(0x55a24b84cbc8)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/7880/},
        abstract = {Facial communicative signals (FCSs) such as head gestures, eye gaze, and facial expressions can provide useful feedback in conversations between people and also in humanrobot interaction. This paper presents a pattern recognition approach for the interpretation of FCSs in terms of valence, based on the selection of discriminative subsequences in video data. These subsequences capture important temporal dynamics and are used as prototypical reference subsequences in a classi?cation procedure based on dynamic time warping and feature extraction with active appearance models. Using this valence classi?cation, the robot can discriminate positive from negative interaction situations and react accordingly. The approach is evaluated on a database containing videos of people interacting with a robot by teaching the names of several objects to it. The verbal answer of the robot is expected to elicit the display of spontaneous FCSs by the human tutor, which were classi?ed in this work. The achieved classi?cation accuracies are comparable to the average human recognition performance and outperformed our previous results on this task.}
}

@inproceedings{lincoln39644,
       booktitle = {12th Symposium on Advanced Space Technologies in Robotics and Automation},
           month = {May},
           title = {Multi-Level Soil Sensing Systems to Identify Safe Trafficability Areas for Extra-Planetary Rovers},
          author = {W.A. Lewinger and F. Comin and S. Ransom and L. Richter and S. Al-Milli and C. Spiteri and M. Gao and M. Matthews and C. Saaj},
            year = {2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/39644/}
}

@inproceedings{lincoln39647,
       booktitle = {12th Symposium on Advanced Space Technologies in Robotics and Automation, ESA/ESTEC},
           month = {May},
           title = {Safe Long-Range Travel for Planetary Rovers through Forward Sensing},
          author = {Y. Nevatia and F. Bulens and J. Gancet and Y. Gao and S. Al-Mili and R.U. Sonsalla and T.P. Kaupisch and M. Fritsche and T. Vogele and E. Allouis and K. Skocki and S. Ransom and C. Saaj and M. Matthews and B. Yeomans and L. Richter},
            year = {2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/39647/}
}

@inproceedings{lincoln39643,
       booktitle = {IEEE International Conference on Robotics and Automation Planetary Rovers Workshop},
           month = {May},
           title = {Improved Traversal for Planetary Rovers through Forward Acquisition of Terrain Trafficability},
          author = {Y.H. Nevatia and J. Gancet and F. Bulens and T. Voegele and U. Sonsalla and C.M. Saaj and W.A. Lewinger and M. Matthews and F.J.C. Cabrera and Y. Gao and E. Allouis and B. Imhof and S. Ransom and L. Richter and K. Skocki},
            year = {2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/39643/}
}

@inproceedings{lincoln39646,
       booktitle = {12th Symposium on Advanced Space Technologies in Robotics and Automation, ESA/ESTEC},
           month = {May},
           title = {Modelling Leg / Terrain Interaction for a Legged Planetary Micro-Rover},
          author = {B. Yeomans and C. Saaj and M. van Winnedael},
            year = {2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/39646/}
}

@incollection{lincoln29369,
           month = {April},
          author = {Ayse Kucukyilmaz and Tevfik Metin Sezgin and Cagatay Basdogan},
       booktitle = {2013 21st Signal Processing and Communications Applications Conference (SIU)},
           title = {Role allocation through haptics in physical human-robot interaction},
       publisher = {IEEE},
             doi = {10.1109/SIU.2013.6531558},
           pages = {1--5},
            year = {2013},
        keywords = {ARRAY(0x55a24bca5b40)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/29369/},
        abstract = {This paper presents a summary of our efforts to enable dynamic role allocation between humans and robots in physical collaboration tasks. A major goal in physical human-robot interaction research is to develop tacit and natural communication between partners. In previous work, we suggested that the communication between a human and a robot would benefit from a decision making process in which the robot can dynamically adjust its control level during the task based on the intentions of the human. In order to do this, we define leader and follower roles for the partners, and using a role exchange mechanism, we enable the partners to negotiate solely through force information to exchange roles. We show that when compared to an ?equal control? condition, the role exchange mechanism improves task performance and the joint efficiency of the partners.}
}

@article{lincoln37414,
          volume = {50},
          number = {2},
           month = {April},
          author = {B. Yeomans and C. Saaj and M. Van Winnendael},
            note = {cited By 10},
           title = {Walking planetary rovers-Experimental analysis and modelling of leg thrust in loose granular soils},
       publisher = {Elsevier},
            year = {2013},
         journal = {Journal of Terramechanics},
             doi = {10.1016/j.jterra.2013.01.006},
           pages = {107--120},
        keywords = {ARRAY(0x55a24bca6900)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/37414/},
        abstract = {One of the principal differences between locomotion in granular soils using legs when compared with wheels is that the drag between the leg assembly and the regolith material provides additional thrust. Experimental work is presented which demonstrates that this additional force is substantial, and can significantly augment legged vehicle Drawbar Pull. The paper also demonstrates that the drag force depends in a highly non-linear manner on sinkage depth, and linearly on leg cross section yet is only weakly dependent on leg cross-sectional shape, leg material frictional properties, or leg velocity. Comparison with modelled forces using established wedge theory techniques demonstrates poor correlation between predicted and actual results; in contrast, a modelling approach based on an analysis of the dynamics of granular materials produces an excellent correlation with experimental results and enables the drag force to be accurately characterised by deriving a constant coefficient which is characteristic of the soil material. Future work will investigate the relationship between this characteristic coefficient and the physical properties of the soil material to develop a robust method of predicting the coefficient for any soil.}
}

@article{lincoln23077,
          volume = {3},
          number = {4},
           month = {April},
          author = {Paul Baxter and Joachim De Greeff and Rachel Wood and Tony Belpaeme},
            note = {Issue cover date: December 2012},
           title = {Modelling concept prototype competencies using a developmental memory model},
       publisher = {De Gruyter/Springer},
            year = {2013},
         journal = {Paladyn, Journal of Behavioral Robotics},
             doi = {10.2478/s13230-013-0105-9},
           pages = {200--208},
        keywords = {ARRAY(0x55a24bca6930)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/23077/},
        abstract = {The use of concepts is fundamental to human-level cognition, but there remain a number of open questions as to the structures supporting this competence. Specifically, it has been shown that humans use concept prototypes, a flexible means of representing concepts such that it can be used both for categorisation and for similarity judgements. In the context of autonomous robotic agents, the processes by which such concept functionality could be acquired would be particularly useful, enabling flexible knowledge representation and application. This paper seeks to explore this issue of autonomous concept acquisition. By applying a set of structural and operational principles, that support a wide range of cognitive competencies, within a developmental framework, the intention is to explicitly embed the development of concepts into a wider framework of cognitive processing. Comparison with a benchmark concept modelling system shows that the proposed approach can account for a number of features, namely concept-based classification, and its extension to prototype-like functionality.}
}

@inproceedings{lincoln14893,
       booktitle = {International IEEE/EPSRC Workshop on Autonomous Cognitive Robotics},
           month = {March},
           title = {Spatio-temporal representation for cognitive control in long-term scenarios},
          author = {Tom Duckett and Marc Hanheide and Tomas Krajnik and Jaime Pulido Fentanes and Christian Dondrup},
            year = {2013},
             doi = {10.13140/2.1.2678.7205},
        keywords = {ARRAY(0x55a24bca6960)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/14893/},
        abstract = {The FP-7 Integrated Project STRANDS [1] is aimed at producing intelligent mobile robots that are able to operate robustly for months in dynamic human environments. To achieve long-term autonomy, the robots would need to understand the environment and how it changes over time. For that, we will have to develop novel approaches to extract 3D shapes, objects, people, and models of activity from sensor data gathered during months of autonomous operation.
So far, the environment models used in mobile robotics have been tailored to capture static scenes and environment variations are largely treated as noise. Therefore, utilization of the static models in ever-changing, real world environments is difficult. We propose to represent the environment?s spatio-temporal dynamics by its frequency spectrum.}
}

@inproceedings{lincoln8365,
           month = {March},
          author = {Michael Zillich and Kai Zhou and Danijel Skocaj and Matej Kristan and Alen Vrecko and Marko Mahnic and Miroslav Janicek and Geert-Jan M. Kruijff and Thomas Keller and Marc Hanheide and Nick Hawes},
       booktitle = {Proceedings of the 8th ACM/IEEE international conference on Human-robot interaction},
           title = {Robot George: interactive continuous learning of visual concepts},
       publisher = {IEEE Press},
             doi = {10.1109/HRI.2013.6483629},
           pages = {425},
            year = {2013},
        keywords = {ARRAY(0x55a24bca5b88)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/8365/},
        abstract = {The video presents the robot George learning visual concepts in dialogue with a tutor}
}

@article{lincoln9308,
          volume = {103},
           month = {March},
          author = {Shigang Yue and F. Claire Rind},
           title = {Postsynaptic organizations of directional selective visual neural networks for collision detection},
       publisher = {Elsevier Science Limited},
            year = {2013},
         journal = {Neurocomputing},
             doi = {10.1016/j.neucom.2012.08.027},
           pages = {50--62},
        keywords = {ARRAY(0x55a24b83f920)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/9308/},
        abstract = {In this paper, we studied the postsynaptic organizations of directional selective visual neurons for collision detection. Directional selective neurons can extract different directional visual motion cues fast and reliably by allowing inhibition spreads to further layers in specific directions with one or several time steps delay. Whether these directional selective neurons can be easily organised for other specific visual tasks is not known. Taking collision detection as the primary visual task, we investigated the postsynaptic organizations of these directional selective neurons through evolutionary processes. The evolved postsynaptic organizations demonstrated robust properties in detecting imminent collisions in complex visual environments with many of which achieved 94\% success rate after evolution suggesting active roles in collision detection directional selective neurons and its postsynaptic organizations can play.}
}

@article{lincoln33061,
          volume = {61},
          number = {5},
           month = {February},
          author = {Abdullah Almeshal and Khaled Goher and Osman Tokhi},
            note = {The final published version of this article is available online at https://www.sciencedirect.com/science/article/pii/S0921889013000195?via\%3Dihub},
           title = {Dynamic Modelling and Stabilization of a New Configuration of Two-Wheeled Machines},
       publisher = {Elsevier},
            year = {2013},
         journal = {Robotics and Autonomous Systems},
             doi = {10.1016/j.robot.2013.01.006},
           pages = {443--472},
        keywords = {ARRAY(0x55a24bc5bcb8)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/33061/},
        abstract = {This paper presents a novel design of two-wheeled vehicles and an associated stabilization approach. The proposed design provides the vehicle with more flexibility in terms of increased degrees of freedom which enable the vehicle to enlarge its working space. The additional translational degree of freedom (DOF), offered by the linear actuator, assists an attached payload to reach different levels of height as and when required. The model of the system mimics the scenario of the double inverted pendulum on a moving base, with the added DOF. Lagrangian dynamic formulation is used to derive the system dynamics. Joints frictions based on the Coulomb friction model are considered so as to retain nonlinear characteristics of the system. A PD-PID robust control approach is derived for the stabilization of the system. An investigation of the impact of damping associated with joints on the stability of the system is carried out. Simulation results validating the model and the control approach are presented and discussed.}
}

@article{lincoln6031,
          volume = {56},
          number = {1},
           month = {February},
          author = {Grzegorz Cielniak and Nicola Bellotto and Tom Duckett},
           title = {Integrating mobile robotics and vision with undergraduate computer science},
       publisher = {The IEEE Education Society},
            year = {2013},
         journal = {IEEE Transactions on Education},
             doi = {10.1109/TE.2012.2213822},
           pages = {48--53},
        keywords = {ARRAY(0x55a24b84d210)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/6031/},
        abstract = {This paper describes the integration of robotics education into an undergraduate Computer Science curriculum. The proposed approach delivers mobile robotics as well as covering the closely related field of Computer Vision, and is directly linked to the research conducted at the authors? institution. The paper describes the most relevant details of the module content and assessment strategy, paying particular attention to the practical sessions using Rovio mobile robots. The specific choices are discussed that were made with regard to the mobile platform, software libraries and lab environment. The paper also presents a detailed qualitative and quantitative analysis of student results, including the correlation between student engagement and performance, and discusses the outcomes of this experience.}
}

@article{lincoln37415,
          volume = {25},
          number = {1},
           month = {January},
          author = {B.J. Smith and C. Saaj and E. Allouis},
            note = {cited By 1},
           title = {ANUBIS: artificial neuromodulation using a Bayesian inference system},
       publisher = {MIT Press},
            year = {2013},
         journal = {Neural computation},
             doi = {10.1162/NECO\_a\_00376},
           pages = {221--258},
        keywords = {ARRAY(0x55a24bcea498)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/37415/},
        abstract = {Gain tuning is a crucial part of controller design and depends not only on an accurate understanding of the system in question, but also on the designer's ability to predict what disturbances and other perturbations the system will encounter throughout its operation. This letter presents ANUBIS (artificial neuromodulation using a Bayesian inference system), a novel biologically inspired technique for automatically tuning controller parameters in real time. ANUBIS is based on the Bayesian brain concept and modifies it by incorporating a model of the neuromodulatory system comprising four artificial neuromodulators. It has been applied to the controller of EchinoBot, a prototype walking rover for Martian exploration. ANUBIS has been implemented at three levels of the controller; gait generation, foot trajectory planning using B{\'e}zier curves, and foot trajectory tracking using a terminal sliding mode controller. We compare the results to a similar system that has been tuned using a multilayer perceptron. The use of Bayesian inference means that the system retains mathematical interpretability, unlike other intelligent tuning techniques, which use neural networks, fuzzy logic, or evolutionary algorithms. The simulation results show that ANUBIS provides significant improvements in efficiency and adaptability of the three controller components; it allows the robot to react to obstacles and uncertainties faster than the system tuned with the MLP, while maintaining stability and accuracy. As well as advancing rover autonomy, ANUBIS could also be applied to other situations where operating conditions are likely to change or cannot be accurately modeled in advance, such as process control. In addition, it demonstrates one way in which neuromodulation could fit into the Bayesian brain framework.}
}

@article{lincoln25789,
          volume = {6},
           month = {January},
          author = {Elmar A. Rueckert and Gerhard Neumann and Marc Toussaint and Wolfgang Maass},
           title = {Learned graphical models for probabilistic planning provide a new class of movement primitives},
       publisher = {Frontiers Media},
         journal = {Frontiers in Computational Neuroscience},
             doi = {10.3389/fncom.2012.00097},
            year = {2013},
        keywords = {ARRAY(0x55a24bcea4e0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/25789/},
        abstract = {Biological movement generation combines three interesting aspects: its modular organization in movement primitives (MPs), its characteristics of stochastic optimality under perturbations, and its efficiency in terms of learning. A common approach to motor skill learning is to endow the primitives with dynamical systems. Here, the parameters of the primitive indirectly define the shape of a reference trajectory. We propose an alternative MP representation based on probabilistic inference in learned graphical models with new and interesting properties that complies with salient features of biological movement control. Instead of endowing the primitives with dynamical systems, we propose to endow MPs with an intrinsic probabilistic planning system, integrating the power of stochastic optimal control (SOC) methods within a MP. The parameterization of the primitive is a graphical model that represents the dynamics and intrinsic cost function such that inference in this graphical model yields the control policy. We parameterize the intrinsic cost function using task-relevant features, such as the importance of passing through certain via-points. The system dynamics as well as intrinsic cost function parameters are learned in a reinforcement learning (RL) setting. We evaluate our approach on a complex 4-link balancing task. Our experiments show that our movement representation facilitates learning significantly and leads to better generalization to new task settings without re-learning.}
}

@inproceedings{lincoln38439,
          volume = {2},
           title = {An argumentation-based dialogue system for human-robot collaboration},
          author = {M.Q. Azhar and Simon Parsons and Elizabeth Sklar},
            year = {2013},
           pages = {1353--1354},
            note = {cited By 3},
         journal = {12th International Conference on Autonomous Agents and Multiagent Systems 2013, AAMAS 2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38439/}
}

@inproceedings{lincoln13462,
          author = {C. Lang and S. Wachsmuth and M. Hanheide and H. Wersing},
            note = {Conference Code:100673},
       booktitle = {IEEE International Conference on Robotics and Automation, ICRA 2013},
         address = {Karlsruhe},
           title = {Facial communicative signal interpretation in human-robot interaction by discriminative video subsequence selection},
       publisher = {IEEE},
            year = {2013},
             doi = {10.1109/ICRA.2013.6630572},
           pages = {170--177},
        keywords = {ARRAY(0x55a24bd12c78)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/13462/},
        abstract = {Facial communicative signals (FCSs) such as head gestures, eye gaze, and facial expressions can provide useful feedback in conversations between people and also in human-robot interaction. This paper presents a pattern recognition approach for the interpretation of FCSs in terms of valence, based on the selection of discriminative subsequences in video data. These subsequences capture important temporal dynamics and are used as prototypical reference subsequences in a classification procedure based on dynamic time warping and feature extraction with active appearance models. Using this valence classification, the robot can discriminate positive from negative interaction situations and react accordingly. The approach is evaluated on a database containing videos of people interacting with a robot by teaching the names of several objects to it. The verbal answer of the robot is expected to elicit the display of spontaneous FCSs by the human tutor, which were classified in this work. The achieved classification accuracies are comparable to the average human recognition performance and outperformed our previous results on this task. {\^A}{\copyright} 2013 IEEE.}
}

@inproceedings{lincoln38438,
          volume = {2},
           title = {Maximizing matching in double-sided auctions},
          author = {J. Niu and Simon Parsons},
            year = {2013},
           pages = {1283--1284},
            note = {cited By 2},
         journal = {12th International Conference on Autonomous Agents and Multiagent Systems 2013, AAMAS 2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38438/}
}

@inproceedings{lincoln38441,
          volume = {2},
           title = {ArgTrust: Decision making with information from sources of varying trustworthiness},
          author = {Simon Parsons and Elizabeth Sklar and J. Salvit and H. Wall and Z. Li},
            year = {2013},
           pages = {1395--1396},
            note = {cited By 7},
         journal = {12th International Conference on Autonomous Agents and Multiagent Systems 2013, AAMAS 2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38441/}
}

@inproceedings{lincoln38437,
          volume = {SS-13-},
           title = {An argumentation-based approach to handling trust in distributed decision making},
          author = {Simon Parsons and Elizabeth Sklar and M. Singh and K. Levitt and J. Rowe},
            year = {2013},
           pages = {66--71},
            note = {cited By 4},
         journal = {AAAI Spring Symposium - Technical Report},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38437/}
}

@inproceedings{lincoln38440,
          volume = {2},
           title = {Enabling human-robot collaboration via argumentation},
          author = {Elizabeth Sklar and M.Q. Azhar and T. Flyr and Simon Parsons},
            year = {2013},
           pages = {1251--1252},
            note = {cited By 1},
         journal = {12th International Conference on Autonomous Agents and Multiagent Systems 2013, AAMAS 2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38440/}
}

@inproceedings{lincoln38442,
          volume = {2},
           title = {HRTeam: A framework to support research on human/multi-robot interaction},
          author = {Elizabeth Sklar and Simon Parsons and A.T. Ozgelen and E. Schneider and M. Costantino and S.L. Epstein},
            year = {2013},
           pages = {1409--1410},
            note = {cited By 3},
         journal = {12th International Conference on Autonomous Agents and Multiagent Systems 2013, AAMAS 2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38442/}
}

@inproceedings{lincoln38436,
          volume = {SS-13-},
           title = {Unsupervised modeling of patient-level disease dynamics},
          author = {S. Tamang and Simon Parsons},
            year = {2013},
           pages = {78--80},
            note = {cited By 0},
         journal = {AAAI Spring Symposium - Technical Report},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38436/}
}

