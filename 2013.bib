@article{lincoln12817,
          volume = {2013},
           month = {December},
          author = {Pin Shen Teh and Andrew Beng Jin Teoh and Shigang Yue},
           title = {A survey of keystroke dynamics biometrics},
       publisher = {Hindawi Publishing Corporation / Scientific World},
            year = {2013},
         journal = {The Scientific World Journal},
             doi = {10.1155/2013/408280},
           pages = {408280},
        keywords = {ARRAY(0x555ddbfa7940)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/12817/},
        abstract = {Research on keystroke dynamics biometrics has been increasing, especially in the last decade. The main motivation behind this effort is due to the fact that keystroke dynamics biometrics is economical and can be easily integrated into the existing computer security systems with minimal alteration and user intervention. Numerous studies have been conducted in terms of data acquisition devices, feature representations, classification methods, experimental protocols, and evaluations. However, an up-to-date extensive survey and evaluation is not yet available. The objective of this paper is to provide an insightful survey and comparison on keystroke dynamics biometrics research performed throughout the last three decades, as well as offering suggestions and possible future research directions.}
}

@inproceedings{lincoln12670,
       booktitle = {16th International Conference on Advanced Robotics (ICAR 2013)},
           month = {November},
           title = {External localization system for mobile robotics},
          author = {Tomas Krajnik and Matias Nitsche and Jan Faigl and Marta Mejail and Libor Preucil and Tom Duckett},
       publisher = {IEEE},
            year = {2013},
         journal = {International Conference on Advanced Robotics, ICAR 2013 (Proceedings)},
        keywords = {ARRAY(0x555ddbf8c550)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/12670/},
        abstract = {We present a fast and precise vision-based software intended for multiple robot localization. The core component of
the proposed localization system is an efficient method for black and white circular pattern detection. The method is robust to variable lighting conditions, achieves sub-pixel precision, and its computational complexity is independent of the processed image size. With off-the-shelf computational equipment and low-cost camera, its core algorithm is able to process hundreds of images per second while tracking hundreds of objects with millimeter precision. We propose a mathematical model of the method that allows to calculate its precision, area of coverage, and processing speed from the camera?s intrinsic parameters and hardware?s processing capacity. The correctness of the presented model and
performance of the algorithm in real-world conditions are verified in several experiments. Apart from the method description, we also publish its source code; so, it can be used as an enabling technology for various mobile robotics problems.}
}

@inproceedings{lincoln13757,
           month = {November},
          author = {Gabriel Zahi and Shigang Yue},
       booktitle = {Modelling Symposium (EMS), 2013 European},
           title = {Automatic detection of low light images in a video sequence Shot under different light conditions},
       publisher = {IEEE},
             doi = {10.1109/EMS.2013.47},
           pages = {271--276},
            year = {2013},
        keywords = {ARRAY(0x555ddbfe6c08)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/13757/},
        abstract = {Nocturnal insects have the ability to neurally sum visual signals in space and time to be able to see under very low light conditions. This ability shown by nocturnal insects has inspired many researchers to develop a night vision algorithm, that is capable of significantly improving the quality and reliability of digital images captured under very low light conditions. This algorithm however when applied to day time images rather degrades their quality. It is therefore not suitable to apply the night vision algorithms equally to an image stream with different light conditions. This paper introduces a quick method of automatically determining when to apply the nocturnal vision algorithm by analysing the cumulative intensity histogram of each image in the stream. The effectiveness of this method is demonstrated with relevant experiments in a good and acceptable way.}
}

@inproceedings{lincoln11637,
       booktitle = {International Conference on Social Robotics (ICSR)},
           month = {October},
           title = {Qualitative design and implementation of human-robot spatial interactions},
          author = {Nicola Bellotto and Marc Hanheide and Nico Van de Weghe},
       publisher = {Springer},
            year = {2013},
             doi = {10.1007/978-3-319-02675-6\_33},
        keywords = {ARRAY(0x555ddbf85818)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/11637/},
        abstract = {Despite the large number of navigation algorithms available for mobile robots, in many social contexts they often exhibit inopportune motion behaviours in proximity of people, often with very "unnatural" movements due to the execution of segmented trajectories or the sudden activation of safety mechanisms (e.g., for obstacle avoidance). We argue that the reason of the problem is not only the difficulty of modelling human behaviours and generating opportune robot control policies, but also the way human-robot spatial interactions are represented and implemented.
In this paper we propose a new methodology based on a qualitative representation of spatial interactions, which is both flexible and compact, adopting the well-defined and coherent formalization of Qualitative Trajectory Calculus (QTC). We show the potential of a QTC-based approach to abstract and design complex robot behaviours, where the desired robot's behaviour is represented together with its actual performance in one coherent approach, focusing on spatial interactions rather than pure navigation problems.}
}

@inproceedings{lincoln11636,
       booktitle = {IEEE SMC Int. Workshop on Human-Machine Systems, Cyborgs and Enhancing Devices (HUMASCEND)},
           month = {October},
           title = {A multimodal smartphone interface for active perception by visually impaired},
          author = {Nicola Bellotto},
       publisher = {IEEE},
            year = {2013},
        keywords = {ARRAY(0x555ddbcec2f0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/11636/},
        abstract = {The diffuse availability of mobile devices, such as smartphones and tablets, has the potential to bring substantial benefits to the people with sensory impairments. The solution proposed in this paper is part of an ongoing effort to create an accurate obstacle and hazard detector for the visually impaired, which is embedded in a hand-held device. In particular, it presents a proof of concept for a multimodal interface to control the orientation of a smartphone's camera, while being held by a person, using a combination of vocal messages, 3D sounds and vibrations. The solution, which is to be evaluated experimentally by users, will enable further research in the area of active vision with human-in-the-loop, with potential application to mobile assistive devices for indoor navigation of visually impaired people.}
}

@article{lincoln23076,
          volume = {6},
           month = {October},
          author = {Paul E. Baxter and Joachim de Greeff and Tony Belpaeme},
           title = {Cognitive architecture for human?robot interaction: towards behavioural alignment},
       publisher = {Elsevier B.V.},
            year = {2013},
         journal = {Biologically Inspired Cognitive Architectures},
             doi = {10.1016/j.bica.2013.07.002},
           pages = {30--39},
        keywords = {ARRAY(0x555ddbd4c430)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/23076/},
        abstract = {Abstract With increasingly competent robotic systems desired and required for social human?robot interaction comes the necessity for more complex means of control. Cognitive architectures (specifically the perspective where principles of structure and function are sought to account for multiple cognitive competencies) have only relatively recently been considered for applica- tion to this domain. In this paper, we describe one such set of architectural principles ? acti- vation dynamics over a developmental distributed associative substrate ? and show how this enables an account of a fundamental competence for social cognition: multi-modal behavioural alignment. Data from real human?robot interactions is modelled using a computational system based on this set of principles to demonstrate how this competence can therefore be consid- ered as embedded in wider cognitive processing. It is shown that the proposed system can model the behavioural characteristics of human subjects. While this study is a simulation using real interaction data, the results obtained validate the application of the proposed approach to this issue.}
}

@article{lincoln12768,
          volume = {61},
          number = {10},
           month = {October},
          author = {Tom Duckett and Achim Lilienthal},
            note = {Selected Papers from the 5th European Conference on Mobile Robots (ECMR 2011)},
           title = {Editorial},
       publisher = {Elsevier for North-Holland / Intelligent Autonomous Systems (IAS) Society},
            year = {2013},
         journal = {Robotics and Autonomous Systems},
             doi = {10.1016/j.robot.2013.01.005},
           pages = {1049--1050},
        keywords = {ARRAY(0x555ddbd4c3d0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/12768/},
        abstract = {.}
}

@article{lincoln13793,
          volume = {27},
          number = {06},
           month = {September},
          author = {Jiawei Xu and Shigang Yue and Yuchao Tang},
           title = {A motion attention model based on rarity weighting and motion cues in dynamic scenes},
       publisher = {World Scientific Publishing},
            year = {2013},
         journal = {International Journal of Pattern Recognition and Artificial Intelligence},
             doi = {10.1142/S0218001413550094},
           pages = {1355009},
        keywords = {ARRAY(0x555ddbe96be8)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/13793/},
        abstract = {Nowadays, motion attention model is a controversial topic in the biological computer vision area. The computational attention model can be decomposed into a set of features via predefined channels. Here we designed a bio-inspired vision attention model, and added the rarity measurement onto it. The priority of rarity is emphasized under the assumption of weighting effect upon the features logic fusion. At this stage, a final saliency map at each frame is adjusted by the spatiotemporal and rarity values. By doing this, the process of mimicking human vision attention becomes more realistic and logical to the real circumstance. The experiments are conducted on the benchmark dataset of static images and video sequences. We simulated the attention shift based on several dataset. Most importantly, our dynamic scenes are mostly selected from the objects moving on the highway and dynamic scenes. The former one can be developed on the detection of car collision and will be a useful tool for further application in robotics. We also conduct experiment on the other video clips to prove the rationality of rarity factor and feature cues fusion methods. Finally, the evaluation results indicate our visual attention model outperforms several state-of-the-art motion attention models.


Read More: http://www.worldscientific.com/doi/abs/10.1142/S0218001413550094}
}

@inproceedings{lincoln11330,
       booktitle = {Towards Autonomous Robotic Systems},
           month = {August},
           title = {Evaluation of laser range-finder mapping for agricultural spraying vehicles},
          author = {Francisco-Angel Moreno and Grzegorz Cielniak and Tom Duckett},
            year = {2013},
           pages = {210--221},
             doi = {10.1007/978-3-662-43645-5\_22},
        keywords = {ARRAY(0x555ddbfb91e0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/11330/},
        abstract = {In this paper, we present a new application of laser range-finder sensing to agricultural spraying vehicles. The current generation of spraying vehicles use automatic controllers to maintain the height of the sprayer booms above the crop.
However, these control systems are typically based on ultrasonic sensors mounted on the booms, which limits the accuracy of the measurements and the response of the controller to changes in the terrain, resulting in a sub-optimal spraying process. To overcome these limitations, we propose to use a laser scanner, attached to the front of the sprayer's cabin, to scan the ground surface in front of the vehicle and to build a scrolling 3d map of the terrain. We evaluate the proposed solution in a series of field tests, demonstrating that the approach provides a more detailed and accurate representation of the environment than the current sonar-based solution, and which can lead to the development of more efficient boom control systems.}
}

@misc{lincoln53189,
       booktitle = {Living machines: An exhibition of biomimetic and biohybrid technologies and artworks},
           month = {August},
           title = {Living machines: An exhibition of biomimetic and biohybrid technologies and artworks},
          author = {Tony Prescott and Paul Verschure and Charles Fox and Anna Mura and Stuart Wilson and Gill Ryder},
            year = {2013},
        keywords = {ARRAY(0x555ddbd0e520)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/53189/},
        abstract = {Living Machines is an international conference series concerned with the development of future real-world technologies that harness the principles underlying living systems and the flow of communication signals between living and artificial systems.  The conference highlights the most exciting contemporary research in biomimetics{--}the development of ovel technologies through the distillation of principles from the study of biological systems, and biohybrids{--}formed by combining a biological component{--}an existing living system{--}with an artificial, newly-engineered component. The concept of ?Living Machine? captures the insight that useful artificial entities can be designed by copying life, and, at the same time, that we can understand biological organisms, including ourselves, as living machines ?designed? by nature. Some of the most interesting new developments in biomimetic and biohybrid technologies, grouped under five themes, together with some striking examples of contemporary biomimetic or biohybrid art, have been selected for  presentation at the Living Machines Exhibition, a one-day event at the Science Museum in London.  Highlights of the 2013 Living Machines exhibition include:
? A musical performance featuring the iCub humanoid robot
? Mammal-like robots with whiskered touch systems
? A robot model of fossilised animal behaviour from the dawn of life
? Biomimetic medical devices including a wasp-like needle for minimally-invasive
surgery
? A robot that powers itself by digesting human waste
? Micro-flying robots, worm, octopus, fish and mammal-like robots
? Biohybrid clothing made with living cells and robots controlled by slime mould
? Live visual art generated by the Artificial Intelligence AARON, created by Harold Cohen
? A string quartet performing music generated by the Artificial Intelligence EMI, created
by David Cope.}
}

@inproceedings{lincoln51742,
       booktitle = {Conference on Biomimetic and Biohybrid Systems},
           month = {June},
           title = {Where wall-following works: case study of simple
heuristics vs. optimal exploratory behaviour},
          author = {Charles Fox},
            year = {2013},
        keywords = {ARRAY(0x555ddbcfc168)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/51742/},
        abstract = {Where wall-following works: case study of simple heuristics vs. optimal exploratory behaviour}
}

@article{lincoln9307,
          volume = {5},
          number = {2},
           month = {June},
          author = {Shigang Yue and F. Claire Rind},
           title = {Redundant neural vision systems: competing for collision recognition roles},
       publisher = {IEEE / Institute of Electrical and Electronics Engineers Incorporated},
            year = {2013},
         journal = {IEEE Transactions on Autonomous Mental Development},
             doi = {10.1109/TAMD.2013.2255050},
           pages = {173--186},
        keywords = {ARRAY(0x555ddbd80690)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/9307/},
        abstract = {Ability to detect collisions is vital for future robots that interact with humans in complex visual environments. Lobula giant movement detectors (LGMD) and directional selective neurons (DSNs) are two types of identified neurons found in the visual pathways of insects such as locusts. Recent modelling studies showed that the LGMD or grouped DSNs could each be tuned for collision recognition. In both biological and artificial vision systems, however, which one should play the collision recognition role and the way the two types of specialized visual neurons could be functioning together are not clear. In this modeling study, we compared the competence of the LGMD and the DSNs, and also investigate the cooperation of the two neural vision systems for collision recognition via artificial evolution. We implemented three types of collision recognition neural subsystems ? the LGMD, the DSNs and a hybrid system which combines the LGMD and the DSNs subsystems together, in each individual agent. A switch gene determines which of the three redundant neural subsystems plays the collision recognition role. We found that, in both robotics and driving environments, the LGMD was able to build up its ability for collision recognition quickly and robustly therefore reducing the chance of other types of neural networks to play the same role. The results suggest that the LGMD neural network could be the ideal model to be realized in hardware for collision recognition.}
}

@manual{lincoln22903,
           month = {May},
            type = {Documentation},
           title = {FUBUTEC-ECEC'2013},
          author = {Cristina Cherino and Grzegorz Cielniak and Patrick Dickinson and Philippe Geril},
       publisher = {EUROSIS-ETI BVBA},
            year = {2013},
            note = {FUBUTEC'2013, Future Business Technology Conference, June 10-12, 2013, University of Lincoln, Lincoln, UK},
        keywords = {ARRAY(0x555ddbca3a08)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/22903/},
        abstract = {This edition covers Risk Management, Management Techniques, Production Design Optimization and Video Applications}
}

@inproceedings{lincoln13775,
           month = {May},
          author = {C. Lang and S. Wachsmuth and M. Hanheide and H. Wersing},
            note = {Conference Code:100673},
       booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
           title = {Facial communicative signal interpretation in human-robot interaction by discriminative video subsequence selection},
         address = {Karlsruhe},
       publisher = {IEEE},
            year = {2013},
             doi = {10.1109/ICRA.2013.6630572},
           pages = {170--177},
        keywords = {ARRAY(0x555ddbcd5fc0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/13775/},
        abstract = {Facial communicative signals (FCSs) such as head gestures, eye gaze, and facial expressions can provide useful feedback in conversations between people and also in human-robot interaction. This paper presents a pattern recognition approach for the interpretation of FCSs in terms of valence, based on the selection of discriminative subsequences in video data. These subsequences capture important temporal dynamics and are used as prototypical reference subsequences in a classification procedure based on dynamic time warping and feature extraction with active appearance models. Using this valence classification, the robot can discriminate positive from negative interaction situations and react accordingly. The approach is evaluated on a database containing videos of people interacting with a robot by teaching the names of several objects to it. The verbal answer of the robot is expected to elicit the display of spontaneous FCSs by the human tutor, which were classified in this work. The achieved classification accuracies are comparable to the average human recognition performance and outperformed our previous results on this task. {\^A}{\copyright} 2013 IEEE.}
}

@inproceedings{lincoln7880,
           month = {May},
          author = {Christian Lang and Sven Wachsmuth and Marc Hanheide and Heiko Wersing},
            note = {Facial communicative signals (FCSs) such as head gestures, eye gaze, and facial expressions can provide useful feedback in conversations between people and also in humanrobot interaction. This paper presents a pattern recognition approach for the interpretation of FCSs in terms of valence, based on the selection of discriminative subsequences in video data. These subsequences capture important temporal dynamics and are used as prototypical reference subsequences in a classi?cation procedure based on dynamic time warping and feature extraction with active appearance models. Using this valence classi?cation, the robot can discriminate positive from negative interaction situations and react accordingly. The approach is evaluated on a database containing videos of people interacting with a robot by teaching the names of several objects to it. The verbal answer of the robot is expected to elicit the display of spontaneous FCSs by the human tutor, which were classi?ed in this work. The achieved classi?cation accuracies are comparable to the average human recognition performance and outperformed our previous results on this task.},
       booktitle = {International Conference on Robotics and Automation (ICRA)},
           title = {Facial communicative signal interpretation in human-robot interaction by discriminative video subsequence selection},
       publisher = {IEEE},
            year = {2013},
             doi = {10.1109/ICRA.2013.6630572},
           pages = {170--177},
        keywords = {ARRAY(0x555ddbd0d3d0)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/7880/},
        abstract = {Facial communicative signals (FCSs) such as head gestures, eye gaze, and facial expressions can provide useful feedback in conversations between people and also in humanrobot interaction. This paper presents a pattern recognition approach for the interpretation of FCSs in terms of valence, based on the selection of discriminative subsequences in video data. These subsequences capture important temporal dynamics and are used as prototypical reference subsequences in a classi?cation procedure based on dynamic time warping and feature extraction with active appearance models. Using this valence classi?cation, the robot can discriminate positive from negative interaction situations and react accordingly. The approach is evaluated on a database containing videos of people interacting with a robot by teaching the names of several objects to it. The verbal answer of the robot is expected to elicit the display of spontaneous FCSs by the human tutor, which were classi?ed in this work. The achieved classi?cation accuracies are comparable to the average human recognition performance and outperformed our previous results on this task.}
}

@inproceedings{lincoln39643,
       booktitle = {IEEE International Conference on Robotics and Automation Planetary Rovers Workshop},
           month = {May},
           title = {Improved Traversal for Planetary Rovers through Forward Acquisition of Terrain Trafficability},
          author = {Y.H. Nevatia and J. Gancet and F. Bulens and T. Voegele and U. Sonsalla and C.M. Saaj and W.A. Lewinger and M. Matthews and F.J.C. Cabrera and Y. Gao and E. Allouis and B. Imhof and S. Ransom and L. Richter and K. Skocki},
            year = {2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/39643/}
}

@article{lincoln23077,
          volume = {3},
          number = {4},
           month = {April},
          author = {Paul Baxter and Joachim De Greeff and Rachel Wood and Tony Belpaeme},
            note = {Issue cover date: December 2012},
           title = {Modelling concept prototype competencies using a developmental memory model},
       publisher = {De Gruyter/Springer},
            year = {2013},
         journal = {Paladyn, Journal of Behavioral Robotics},
             doi = {10.2478/s13230-013-0105-9},
           pages = {200--208},
        keywords = {ARRAY(0x555ddbf6e808)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/23077/},
        abstract = {The use of concepts is fundamental to human-level cognition, but there remain a number of open questions as to the structures supporting this competence. Specifically, it has been shown that humans use concept prototypes, a flexible means of representing concepts such that it can be used both for categorisation and for similarity judgements. In the context of autonomous robotic agents, the processes by which such concept functionality could be acquired would be particularly useful, enabling flexible knowledge representation and application. This paper seeks to explore this issue of autonomous concept acquisition. By applying a set of structural and operational principles, that support a wide range of cognitive competencies, within a developmental framework, the intention is to explicitly embed the development of concepts into a wider framework of cognitive processing. Comparison with a benchmark concept modelling system shows that the proposed approach can account for a number of features, namely concept-based classification, and its extension to prototype-like functionality.}
}

@inproceedings{lincoln14893,
       booktitle = {International IEEE/EPSRC Workshop on Autonomous Cognitive Robotics},
           month = {March},
           title = {Spatio-temporal representation for cognitive control in long-term scenarios},
          author = {Tom Duckett and Marc Hanheide and Tomas Krajnik and Jaime Pulido Fentanes and Christian Dondrup},
            year = {2013},
             doi = {10.13140/2.1.2678.7205},
        keywords = {ARRAY(0x555ddbb9f7d8)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/14893/},
        abstract = {The FP-7 Integrated Project STRANDS [1] is aimed at producing intelligent mobile robots that are able to operate robustly for months in dynamic human environments. To achieve long-term autonomy, the robots would need to understand the environment and how it changes over time. For that, we will have to develop novel approaches to extract 3D shapes, objects, people, and models of activity from sensor data gathered during months of autonomous operation.
So far, the environment models used in mobile robotics have been tailored to capture static scenes and environment variations are largely treated as noise. Therefore, utilization of the static models in ever-changing, real world environments is difficult. We propose to represent the environment?s spatio-temporal dynamics by its frequency spectrum.}
}

@inproceedings{lincoln8365,
           month = {March},
          author = {Michael Zillich and Kai Zhou and Danijel Skocaj and Matej Kristan and Alen Vrecko and Marko Mahnic and Miroslav Janicek and Geert-Jan M. Kruijff and Thomas Keller and Marc Hanheide and Nick Hawes},
       booktitle = {Proceedings of the 8th ACM/IEEE international conference on Human-robot interaction},
           title = {Robot George: interactive continuous learning of visual concepts},
       publisher = {IEEE Press},
             doi = {10.1109/HRI.2013.6483629},
           pages = {425},
            year = {2013},
        keywords = {ARRAY(0x555ddbb9f850)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/8365/},
        abstract = {The video presents the robot George learning visual concepts in dialogue with a tutor}
}

@article{lincoln9308,
          volume = {103},
           month = {March},
          author = {Shigang Yue and F. Claire Rind},
           title = {Postsynaptic organizations of directional selective visual neural networks for collision detection},
       publisher = {Elsevier Science Limited},
            year = {2013},
         journal = {Neurocomputing},
             doi = {10.1016/j.neucom.2012.08.027},
           pages = {50--62},
        keywords = {ARRAY(0x555ddbbbea58)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/9308/},
        abstract = {In this paper, we studied the postsynaptic organizations of directional selective visual neurons for collision detection. Directional selective neurons can extract different directional visual motion cues fast and reliably by allowing inhibition spreads to further layers in specific directions with one or several time steps delay. Whether these directional selective neurons can be easily organised for other specific visual tasks is not known. Taking collision detection as the primary visual task, we investigated the postsynaptic organizations of these directional selective neurons through evolutionary processes. The evolved postsynaptic organizations demonstrated robust properties in detecting imminent collisions in complex visual environments with many of which achieved 94\% success rate after evolution suggesting active roles in collision detection directional selective neurons and its postsynaptic organizations can play.}
}

@article{lincoln6031,
          volume = {56},
          number = {1},
           month = {February},
          author = {Grzegorz Cielniak and Nicola Bellotto and Tom Duckett},
           title = {Integrating mobile robotics and vision with undergraduate computer science},
       publisher = {The IEEE Education Society},
            year = {2013},
         journal = {IEEE Transactions on Education},
             doi = {10.1109/TE.2012.2213822},
           pages = {48--53},
        keywords = {ARRAY(0x555ddbb99b10)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/6031/},
        abstract = {This paper describes the integration of robotics education into an undergraduate Computer Science curriculum. The proposed approach delivers mobile robotics as well as covering the closely related field of Computer Vision, and is directly linked to the research conducted at the authors? institution. The paper describes the most relevant details of the module content and assessment strategy, paying particular attention to the practical sessions using Rovio mobile robots. The specific choices are discussed that were made with regard to the mobile platform, software libraries and lab environment. The paper also presents a detailed qualitative and quantitative analysis of student results, including the correlation between student engagement and performance, and discusses the outcomes of this experience.}
}

@inproceedings{lincoln38439,
          volume = {2},
           title = {An argumentation-based dialogue system for human-robot collaboration},
          author = {M.Q. Azhar and Simon Parsons and Elizabeth Sklar},
            year = {2013},
           pages = {1353--1354},
            note = {cited By 3},
         journal = {12th International Conference on Autonomous Agents and Multiagent Systems 2013, AAMAS 2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38439/}
}

@inproceedings{lincoln13462,
          author = {C. Lang and S. Wachsmuth and M. Hanheide and H. Wersing},
            note = {Conference Code:100673},
       booktitle = {IEEE International Conference on Robotics and Automation, ICRA 2013},
         address = {Karlsruhe},
           title = {Facial communicative signal interpretation in human-robot interaction by discriminative video subsequence selection},
       publisher = {IEEE},
            year = {2013},
             doi = {10.1109/ICRA.2013.6630572},
           pages = {170--177},
        keywords = {ARRAY(0x555ddbbd8f98)},
             url = {https://eprints.lincoln.ac.uk/id/eprint/13462/},
        abstract = {Facial communicative signals (FCSs) such as head gestures, eye gaze, and facial expressions can provide useful feedback in conversations between people and also in human-robot interaction. This paper presents a pattern recognition approach for the interpretation of FCSs in terms of valence, based on the selection of discriminative subsequences in video data. These subsequences capture important temporal dynamics and are used as prototypical reference subsequences in a classification procedure based on dynamic time warping and feature extraction with active appearance models. Using this valence classification, the robot can discriminate positive from negative interaction situations and react accordingly. The approach is evaluated on a database containing videos of people interacting with a robot by teaching the names of several objects to it. The verbal answer of the robot is expected to elicit the display of spontaneous FCSs by the human tutor, which were classified in this work. The achieved classification accuracies are comparable to the average human recognition performance and outperformed our previous results on this task. {\^A}{\copyright} 2013 IEEE.}
}

@inproceedings{lincoln38438,
          volume = {2},
           title = {Maximizing matching in double-sided auctions},
          author = {J. Niu and Simon Parsons},
            year = {2013},
           pages = {1283--1284},
            note = {cited By 2},
         journal = {12th International Conference on Autonomous Agents and Multiagent Systems 2013, AAMAS 2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38438/}
}

@inproceedings{lincoln38441,
          volume = {2},
           title = {ArgTrust: Decision making with information from sources of varying trustworthiness},
          author = {Simon Parsons and Elizabeth Sklar and J. Salvit and H. Wall and Z. Li},
            year = {2013},
           pages = {1395--1396},
            note = {cited By 7},
         journal = {12th International Conference on Autonomous Agents and Multiagent Systems 2013, AAMAS 2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38441/}
}

@inproceedings{lincoln38437,
          volume = {SS-13-},
           title = {An argumentation-based approach to handling trust in distributed decision making},
          author = {Simon Parsons and Elizabeth Sklar and M. Singh and K. Levitt and J. Rowe},
            year = {2013},
           pages = {66--71},
            note = {cited By 4},
         journal = {AAAI Spring Symposium - Technical Report},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38437/}
}

@inproceedings{lincoln38440,
          volume = {2},
           title = {Enabling human-robot collaboration via argumentation},
          author = {Elizabeth Sklar and M.Q. Azhar and T. Flyr and Simon Parsons},
            year = {2013},
           pages = {1251--1252},
            note = {cited By 1},
         journal = {12th International Conference on Autonomous Agents and Multiagent Systems 2013, AAMAS 2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38440/}
}

@inproceedings{lincoln38442,
          volume = {2},
           title = {HRTeam: A framework to support research on human/multi-robot interaction},
          author = {Elizabeth Sklar and Simon Parsons and A.T. Ozgelen and E. Schneider and M. Costantino and S.L. Epstein},
            year = {2013},
           pages = {1409--1410},
            note = {cited By 3},
         journal = {12th International Conference on Autonomous Agents and Multiagent Systems 2013, AAMAS 2013},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38442/}
}

@inproceedings{lincoln38436,
          volume = {SS-13-},
           title = {Unsupervised modeling of patient-level disease dynamics},
          author = {S. Tamang and Simon Parsons},
            year = {2013},
           pages = {78--80},
            note = {cited By 0},
         journal = {AAAI Spring Symposium - Technical Report},
             url = {https://eprints.lincoln.ac.uk/id/eprint/38436/}
}

